# Comparing `tmp/zut-0.6.4-py3-none-any.whl.zip` & `tmp/zut-0.6.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,62 +1,67 @@
-Zip file size: 72279 bytes, number of entries: 60
--rw-rw-rw-  2.0 fat     2172 b- defN 23-Jun-15 12:07 zut/__init__.py
--rw-rw-rw-  2.0 fat      164 b- defN 23-Jun-15 16:31 zut/_version.py
--rw-rw-rw-  2.0 fat      276 b- defN 23-Jun-12 06:54 zut/apps.py
--rw-rw-rw-  2.0 fat      949 b- defN 23-Jun-12 06:54 zut/colors.py
--rw-rw-rw-  2.0 fat     6914 b- defN 23-Jun-14 14:15 zut/commands.py
--rw-rw-rw-  2.0 fat     1979 b- defN 23-Jun-14 14:31 zut/config.py
--rw-rw-rw-  2.0 fat     9861 b- defN 23-Jun-14 14:15 zut/credentials.py
--rw-rw-rw-  2.0 fat     1220 b- defN 23-Jun-15 16:27 zut/csv.py
--rw-rw-rw-  2.0 fat     1662 b- defN 23-Jun-15 12:10 zut/datetime.py
--rw-rw-rw-  2.0 fat    19433 b- defN 23-Jun-15 12:10 zut/excel.py
--rw-rw-rw-  2.0 fat    12996 b- defN 23-Jun-15 10:42 zut/filesh.py
--rw-rw-rw-  2.0 fat     2590 b- defN 23-Jun-12 06:54 zut/git.py
--rw-rw-rw-  2.0 fat     1537 b- defN 23-Jun-14 14:15 zut/gpg.py
--rw-rw-rw-  2.0 fat     4943 b- defN 23-Jun-12 06:54 zut/json.py
--rw-rw-rw-  2.0 fat      429 b- defN 23-Jun-15 12:07 zut/lang.py
--rw-rw-rw-  2.0 fat     4578 b- defN 23-Jun-14 14:15 zut/logging.py
--rw-rw-rw-  2.0 fat     3252 b- defN 23-Jun-15 12:12 zut/numeric.py
--rw-rw-rw-  2.0 fat     2710 b- defN 23-Jun-14 14:15 zut/process.py
--rw-rw-rw-  2.0 fat     2054 b- defN 23-Jun-12 06:54 zut/tabular.py
--rw-rw-rw-  2.0 fat     2019 b- defN 23-Jun-12 06:54 zut/text.py
--rw-rw-rw-  2.0 fat      309 b- defN 23-Jun-15 12:06 zut/typing.py
--rw-rw-rw-  2.0 fat      632 b- defN 23-Jun-14 14:15 zut/venv.py
--rw-rw-rw-  2.0 fat     1665 b- defN 23-Jun-12 11:26 zut/db/__init__.py
--rw-rw-rw-  2.0 fat    14970 b- defN 23-Jun-15 12:14 zut/db/commons.py
--rw-rw-rw-  2.0 fat     2650 b- defN 23-Jun-12 11:26 zut/db/mssql.py
--rw-rw-rw-  2.0 fat     8672 b- defN 23-Jun-12 11:26 zut/db/pg.py
--rw-rw-rw-  2.0 fat       42 b- defN 23-Jun-12 06:54 zut/db/sql_pg/010_extensions.sql
--rw-rw-rw-  2.0 fat      479 b- defN 23-Jun-12 06:54 zut/db/sql_pg/020_slugify.sql
--rw-rw-rw-  2.0 fat     1157 b- defN 23-Jun-12 11:26 zut/django/migration_tools.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-12 06:54 zut/django/management/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-12 06:54 zut/django/management/commands/__init__.py
--rw-rw-rw-  2.0 fat     8265 b- defN 23-Jun-14 14:15 zut/django/management/commands/reinit.py
--rw-rw-rw-  2.0 fat     3271 b- defN 23-Jun-12 06:54 zut/django/middleware/__init__.py
--rw-rw-rw-  2.0 fat       71 b- defN 23-Jun-12 06:54 zut/django/templatetags/__init__.py
--rw-rw-rw-  2.0 fat     2808 b- defN 23-Jun-12 06:54 zut/django/templatetags/static_lib.py
--rw-rw-rw-  2.0 fat     9036 b- defN 23-Jun-12 11:26 zut/django/tracking/__init__.py
--rw-rw-rw-  2.0 fat     4666 b- defN 23-Jun-12 11:26 zut/django/tracking/models.py
--rw-rw-rw-  2.0 fat      621 b- defN 23-Jun-12 11:26 zut/django/tracking/sql_pg/tracking_history_view.sql
--rw-rw-rw-  2.0 fat     1652 b- defN 23-Jun-12 06:54 zut/django/views/MarkdownTemplateView.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Jun-12 06:54 zut/django/views/__init__.py
--rw-rw-rw-  2.0 fat     1767 b- defN 23-Jun-14 14:15 zut/inout/InCsv.py
--rw-rw-rw-  2.0 fat     2124 b- defN 23-Jun-15 16:27 zut/inout/InDb.py
--rw-rw-rw-  2.0 fat     1403 b- defN 23-Jun-14 14:15 zut/inout/InExcel.py
--rw-rw-rw-  2.0 fat     4082 b- defN 23-Jun-15 12:15 zut/inout/InTable.py
--rw-rw-rw-  2.0 fat     4626 b- defN 23-Jun-15 16:27 zut/inout/OutCsv.py
--rw-rw-rw-  2.0 fat     2876 b- defN 23-Jun-15 16:27 zut/inout/OutDb.py
--rw-rw-rw-  2.0 fat     3340 b- defN 23-Jun-15 16:27 zut/inout/OutExcel.py
--rw-rw-rw-  2.0 fat     3151 b- defN 23-Jun-15 16:27 zut/inout/OutFile.py
--rw-rw-rw-  2.0 fat    10204 b- defN 23-Jun-15 16:27 zut/inout/OutTable.py
--rw-rw-rw-  2.0 fat     1163 b- defN 23-Jun-14 14:15 zut/inout/OutTabulate.py
--rw-rw-rw-  2.0 fat     7261 b- defN 23-Jun-15 12:15 zut/inout/__init__.py
--rw-rw-rw-  2.0 fat     2786 b- defN 23-Jun-15 12:17 zut/inout/utils.py
--rw-rw-rw-  2.0 fat     9147 b- defN 23-Jun-12 06:54 zut/network/__init__.py
--rw-rw-rw-  2.0 fat     3850 b- defN 23-Jun-12 06:54 zut/network/commons.py
--rw-rw-rw-  2.0 fat     2962 b- defN 23-Jun-12 06:54 zut/network/winhttp.py
--rw-rw-rw-  2.0 fat     1066 b- defN 23-Jun-15 16:31 zut-0.6.4.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     4962 b- defN 23-Jun-15 16:31 zut-0.6.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-15 16:31 zut-0.6.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        4 b- defN 23-Jun-15 16:31 zut-0.6.4.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     4627 b- defN 23-Jun-15 16:31 zut-0.6.4.dist-info/RECORD
-60 files, 214253 bytes uncompressed, 65133 bytes compressed:  69.6%
+Zip file size: 74691 bytes, number of entries: 65
+-rw-r--r--  2.0 unx     2064 b- defN 23-Jun-16 12:44 zut/__init__.py
+-rw-r--r--  2.0 unx      160 b- defN 23-Jun-16 13:00 zut/_version.py
+-rw-r--r--  2.0 unx      281 b- defN 23-Jun-16 12:44 zut/apps.py
+-rw-r--r--  2.0 unx      915 b- defN 23-Jun-10 13:46 zut/colors.py
+-rw-r--r--  2.0 unx     6876 b- defN 23-Jun-15 21:04 zut/commands.py
+-rw-r--r--  2.0 unx     1915 b- defN 23-Jun-15 21:04 zut/config.py
+-rw-r--r--  2.0 unx     9602 b- defN 23-Jun-15 21:04 zut/credentials.py
+-rw-r--r--  2.0 unx     1176 b- defN 23-Jun-15 21:04 zut/csv.py
+-rw-r--r--  2.0 unx     1605 b- defN 23-Jun-15 21:04 zut/datetime.py
+-rw-r--r--  2.0 unx    18969 b- defN 23-Jun-15 21:04 zut/excel.py
+-rw-r--r--  2.0 unx    12996 b- defN 23-Jun-15 21:04 zut/filesh.py
+-rw-r--r--  2.0 unx     2508 b- defN 23-Jun-10 13:46 zut/git.py
+-rw-r--r--  2.0 unx     1497 b- defN 23-Jun-15 21:04 zut/gpg.py
+-rw-r--r--  2.0 unx     4770 b- defN 23-Jun-10 13:46 zut/json.py
+-rw-r--r--  2.0 unx      412 b- defN 23-Jun-15 21:04 zut/lang.py
+-rw-r--r--  2.0 unx     4448 b- defN 23-Jun-15 21:04 zut/logging.py
+-rw-r--r--  2.0 unx     3157 b- defN 23-Jun-15 21:04 zut/numeric.py
+-rw-r--r--  2.0 unx     2626 b- defN 23-Jun-15 21:04 zut/process.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-Jun-10 13:46 zut/tabular.py
+-rw-r--r--  2.0 unx     1949 b- defN 23-Jun-10 13:46 zut/text.py
+-rw-r--r--  2.0 unx      296 b- defN 23-Jun-15 21:04 zut/typing.py
+-rw-r--r--  2.0 unx      803 b- defN 23-Jun-16 12:44 zut/unittest.py
+-rw-r--r--  2.0 unx      610 b- defN 23-Jun-15 21:04 zut/venv.py
+-rw-r--r--  2.0 unx     1637 b- defN 23-Jun-16 12:44 zut/db/__init__.py
+-rw-r--r--  2.0 unx    15330 b- defN 23-Jun-16 12:44 zut/db/commons.py
+-rw-r--r--  2.0 unx     2576 b- defN 23-Jun-15 21:04 zut/db/mssql.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-Jun-15 21:04 zut/db/pg.py
+-rw-r--r--  2.0 unx       41 b- defN 23-Jun-10 13:46 zut/db/sql_pg/010_extensions.sql
+-rw-r--r--  2.0 unx      464 b- defN 23-Jun-10 13:46 zut/db/sql_pg/020_slugify.sql
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-16 12:44 zut/django/__init__.py
+-rw-r--r--  2.0 unx      886 b- defN 23-Jun-16 12:44 zut/django/mixins.py
+-rw-r--r--  2.0 unx       84 b- defN 23-Jun-16 12:44 zut/django/models.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-10 13:46 zut/django/management/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-10 13:46 zut/django/management/commands/__init__.py
+-rw-r--r--  2.0 unx     4750 b- defN 23-Jun-16 12:44 zut/django/management/commands/reinit.py
+-rw-r--r--  2.0 unx     4964 b- defN 23-Jun-16 12:44 zut/django/management/commands/remakemigrations.py
+-rw-r--r--  2.0 unx     3959 b- defN 23-Jun-16 12:44 zut/django/middleware/__init__.py
+-rw-r--r--  2.0 unx     3345 b- defN 23-Jun-16 12:44 zut/django/migrations/0001_initial.py
+-rw-r--r--  2.0 unx     1515 b- defN 23-Jun-16 12:44 zut/django/migrations/0002_manual.py
+-rw-r--r--  2.0 unx      763 b- defN 23-Jun-16 12:44 zut/django/migrations/__init__.py
+-rw-r--r--  2.0 unx      611 b- defN 23-Jun-16 12:44 zut/django/migrations/0002_manual/sql_pg/tracking_history_view.sql
+-rw-r--r--  2.0 unx       70 b- defN 23-Jun-10 13:46 zut/django/templatetags/__init__.py
+-rw-r--r--  2.0 unx     2718 b- defN 23-Jun-10 13:46 zut/django/templatetags/static_lib.py
+-rw-r--r--  2.0 unx     8165 b- defN 23-Jun-16 12:44 zut/django/tracking/__init__.py
+-rw-r--r--  2.0 unx     4421 b- defN 23-Jun-16 12:44 zut/django/tracking/models.py
+-rw-r--r--  2.0 unx     1710 b- defN 23-Jun-15 21:04 zut/inout/InCsv.py
+-rw-r--r--  2.0 unx     2062 b- defN 23-Jun-15 21:04 zut/inout/InDb.py
+-rw-r--r--  2.0 unx     1357 b- defN 23-Jun-15 21:04 zut/inout/InExcel.py
+-rw-r--r--  2.0 unx     3951 b- defN 23-Jun-15 21:04 zut/inout/InTable.py
+-rw-r--r--  2.0 unx     4507 b- defN 23-Jun-15 21:04 zut/inout/OutCsv.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Jun-15 21:04 zut/inout/OutDb.py
+-rw-r--r--  2.0 unx     3238 b- defN 23-Jun-15 21:04 zut/inout/OutExcel.py
+-rw-r--r--  2.0 unx     3047 b- defN 23-Jun-15 21:04 zut/inout/OutFile.py
+-rw-r--r--  2.0 unx     9916 b- defN 23-Jun-15 21:04 zut/inout/OutTable.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-Jun-15 21:04 zut/inout/OutTabulate.py
+-rw-r--r--  2.0 unx     7113 b- defN 23-Jun-15 21:04 zut/inout/__init__.py
+-rw-r--r--  2.0 unx     2698 b- defN 23-Jun-15 21:04 zut/inout/utils.py
+-rw-r--r--  2.0 unx     8906 b- defN 23-Jun-10 13:46 zut/network/__init__.py
+-rw-r--r--  2.0 unx     3729 b- defN 23-Jun-10 13:46 zut/network/commons.py
+-rw-r--r--  2.0 unx     2887 b- defN 23-Jun-10 13:46 zut/network/winhttp.py
+-rw-r--r--  2.0 unx     1054 b- defN 23-Jun-16 13:00 zut-0.6.5.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     4621 b- defN 23-Jun-16 13:00 zut-0.6.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-16 13:00 zut-0.6.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Jun-16 13:00 zut-0.6.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5055 b- defN 23-Jun-16 13:00 zut-0.6.5.dist-info/RECORD
+65 files, 216204 bytes uncompressed, 66865 bytes compressed:  69.1%
```

## zipnote {}

```diff
@@ -57,14 +57,17 @@
 
 Filename: zut/text.py
 Comment: 
 
 Filename: zut/typing.py
 Comment: 
 
+Filename: zut/unittest.py
+Comment: 
+
 Filename: zut/venv.py
 Comment: 
 
 Filename: zut/db/__init__.py
 Comment: 
 
 Filename: zut/db/commons.py
@@ -78,48 +81,60 @@
 
 Filename: zut/db/sql_pg/010_extensions.sql
 Comment: 
 
 Filename: zut/db/sql_pg/020_slugify.sql
 Comment: 
 
-Filename: zut/django/migration_tools.py
+Filename: zut/django/__init__.py
+Comment: 
+
+Filename: zut/django/mixins.py
+Comment: 
+
+Filename: zut/django/models.py
 Comment: 
 
 Filename: zut/django/management/__init__.py
 Comment: 
 
 Filename: zut/django/management/commands/__init__.py
 Comment: 
 
 Filename: zut/django/management/commands/reinit.py
 Comment: 
 
+Filename: zut/django/management/commands/remakemigrations.py
+Comment: 
+
 Filename: zut/django/middleware/__init__.py
 Comment: 
 
-Filename: zut/django/templatetags/__init__.py
+Filename: zut/django/migrations/0001_initial.py
 Comment: 
 
-Filename: zut/django/templatetags/static_lib.py
+Filename: zut/django/migrations/0002_manual.py
 Comment: 
 
-Filename: zut/django/tracking/__init__.py
+Filename: zut/django/migrations/__init__.py
 Comment: 
 
-Filename: zut/django/tracking/models.py
+Filename: zut/django/migrations/0002_manual/sql_pg/tracking_history_view.sql
 Comment: 
 
-Filename: zut/django/tracking/sql_pg/tracking_history_view.sql
+Filename: zut/django/templatetags/__init__.py
 Comment: 
 
-Filename: zut/django/views/MarkdownTemplateView.py
+Filename: zut/django/templatetags/static_lib.py
 Comment: 
 
-Filename: zut/django/views/__init__.py
+Filename: zut/django/tracking/__init__.py
+Comment: 
+
+Filename: zut/django/tracking/models.py
 Comment: 
 
 Filename: zut/inout/InCsv.py
 Comment: 
 
 Filename: zut/inout/InDb.py
 Comment: 
@@ -159,23 +174,23 @@
 
 Filename: zut/network/commons.py
 Comment: 
 
 Filename: zut/network/winhttp.py
 Comment: 
 
-Filename: zut-0.6.4.dist-info/LICENSE.txt
+Filename: zut-0.6.5.dist-info/LICENSE.txt
 Comment: 
 
-Filename: zut-0.6.4.dist-info/METADATA
+Filename: zut-0.6.5.dist-info/METADATA
 Comment: 
 
-Filename: zut-0.6.4.dist-info/WHEEL
+Filename: zut-0.6.5.dist-info/WHEEL
 Comment: 
 
-Filename: zut-0.6.4.dist-info/top_level.txt
+Filename: zut-0.6.5.dist-info/top_level.txt
 Comment: 
 
-Filename: zut-0.6.4.dist-info/RECORD
+Filename: zut-0.6.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zut/__init__.py

```diff
@@ -1,36 +1,37 @@
-from __future__ import annotations
-
-try:
-    # Version generated by setuptools_scm during build
-    from ._version import __version__
-except ImportError:
-    __version__ = None
-
-# Top-level API (available directly from "zut" namespace). Does not require any dependency.
-from .colors import Colors
-from .commands import create_command_parser, add_func_command, add_module_command, add_package_commands, get_command_func, run_command_func, exec_command_func, run_command_parser, exec_command_parser, ArgumentParser
-from .config import get_config, ExtendedConfigParser
-from .csv import get_default_csv_delimiter
-from .datetime import now_aware, make_aware, is_aware
-from .filesh import configure_smb_credentials  # other functions should be called directly with the module, e.g. `from zut import filesh`, then `filesh.open_file(...)`
-from .git import get_git_tags, get_git_hash, git_has_changes, check_git_version_tag
-from .gpg import download_gpg_key, verify_gpg_signature
-from .inout import out_file, out_table, in_table, transfer_table, OutFile, OutTable, OutTabulate, OutCsv, OutExcel, OutDb, InTable, InCsv, InExcel, InDb, normalize_inout
-from .json import ExtendedJSONDecoder, ExtendedJSONEncoder
-from .lang import ZUT_ROOT, is_list_or_tuple_of
-from .logging import configure_logging, DEFAULT_LOGGING_DICTCONFIG
-from .network import configure_proxy, check_connectivity, check_socket
-from .numeric import human_bytes, parse_decimal
-from .process import get_exit_code, check_completed_subprocess
-from .text import slugify, slugify_snake, remove_whitespaces, remove_consecutive_whitespaces, reconfigure_encoding
-from .venv import get_venv, is_in_venv
-
-
-# NOTE: Inner API: the following features require extra dependencies.
-# zut.credentials       -> [credentials]
-# zut.db                -> [pg] or [mssql]
-# zut.django            -> [django]: django, djangorestframework (for zut.django.middleware), cmarkgfm (for zut.django.views.MarkdownTemplateView)
-# zut.excel             -> [excel]
-# zut.filesh            -> [smb] (for operations on Samba/Windows shares)
-# zut.network.winhttp   -> [winhttp]
-# zut.typing            -> [typing]
+from __future__ import annotations
+
+try:
+    # Version generated by setuptools_scm during build
+    from ._version import __version__
+except ImportError:
+    __version__ = None
+
+# Top-level API (available directly from "zut" namespace). Does not require any dependency.
+from .colors import Colors
+from .commands import create_command_parser, add_func_command, add_module_command, add_package_commands, get_command_func, run_command_func, exec_command_func, run_command_parser, exec_command_parser, ArgumentParser
+from .config import get_config, ExtendedConfigParser
+from .csv import get_default_csv_delimiter
+from .datetime import now_aware, make_aware, is_aware
+from .filesh import configure_smb_credentials  # other functions should be called directly with the module, e.g. `from zut import filesh`, then `filesh.open_file(...)`
+from .git import get_git_tags, get_git_hash, git_has_changes, check_git_version_tag
+from .gpg import download_gpg_key, verify_gpg_signature
+from .inout import out_file, out_table, in_table, transfer_table, OutFile, OutTable, OutTabulate, OutCsv, OutExcel, OutDb, InTable, InCsv, InExcel, InDb, normalize_inout
+from .json import ExtendedJSONDecoder, ExtendedJSONEncoder
+from .lang import ZUT_ROOT, is_list_or_tuple_of
+from .logging import configure_logging, DEFAULT_LOGGING_DICTCONFIG
+from .network import configure_proxy, check_connectivity, check_socket
+from .numeric import human_bytes, parse_decimal
+from .process import get_exit_code, check_completed_subprocess
+from .text import slugify, slugify_snake, remove_whitespaces, remove_consecutive_whitespaces, reconfigure_encoding
+from .venv import get_venv, is_in_venv
+from .unittest import StartTestProgram
+
+
+# NOTE: Inner API: the following features rely on extra dependencies.
+# zut.credentials       -> [credentials]
+# zut.db                -> [pg] or [mssql]
+# zut.django            -> [django]
+# zut.excel             -> [excel]
+# zut.filesh            -> [smb] (for operations on Samba/Windows shares)
+# zut.network.winhttp   -> [winhttp]
+# zut.typing            -> [typing]
```

## zut/_version.py

```diff
@@ -1,4 +1,4 @@
-# file generated by setuptools_scm
-# don't change, don't track in version control
-__version__ = version = '0.6.4'
-__version_tuple__ = version_tuple = (0, 6, 4)
+# file generated by setuptools_scm
+# don't change, don't track in version control
+__version__ = version = '0.6.5'
+__version_tuple__ = version_tuple = (0, 6, 5)
```

## zut/apps.py

```diff
@@ -1,13 +1,14 @@
-"""
-Indicate Django application directory. Allows zut to be added in INSTALLED_APP with simple name:
-
-    INSTALLED_APP = [
-        ...
-        'zut',
-        ...
-    ]
-"""
-from django.apps import AppConfig
-
-class ZutAppConfig(AppConfig):
-    name = "zut.django"
+"""
+Indicate Django application directory. Allows zut to be added in INSTALLED_APP with simple name:
+
+    INSTALLED_APP = [
+        ...
+        'zut',
+        ...
+    ]
+"""
+from django.apps import AppConfig
+
+class ZutAppConfig(AppConfig):
+    name = "zut.django"
+    label = "zut"
```

## zut/colors.py

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-from __future__ import annotations
-import ctypes
-import sys
-
-
-class Colors:
-    """ ANSI color codes """
-    RESET = "\033[0m"
-
-    BLACK = "\033[0;30m"
-    RED = "\033[0;31m"
-    GREEN = "\033[0;32m"
-    YELLOW = "\033[0;33m"
-    BLUE = "\033[0;34m"
-    PURPLE = "\033[0;35m"
-    CYAN = "\033[0;36m"
-    WHITE = "\033[0;37m"
-    GRAY = "\033[0;90m"
-    BOLD_RED = '\033[0;1;31m'
-
-    # Platform/terminal specifics
-    DISABLED = False
-    if not (sys.stdout.isatty() and sys.stderr.isatty()):
-        # Disable colors if we don't write to a terminal
-        DISABLED = True
-        for _ in dir():
-            if isinstance(_, str) and _[0] != "_" and _ not in ['DISABLED']:
-                locals()[_] = ""
-    
-    elif sys.platform == 'win32':
-        # Set Windows console in VT mode
-        kernel32 = ctypes.windll.kernel32
-        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
-        del kernel32
+from __future__ import annotations
+import ctypes
+import sys
+
+
+class Colors:
+    """ ANSI color codes """
+    RESET = "\033[0m"
+
+    BLACK = "\033[0;30m"
+    RED = "\033[0;31m"
+    GREEN = "\033[0;32m"
+    YELLOW = "\033[0;33m"
+    BLUE = "\033[0;34m"
+    PURPLE = "\033[0;35m"
+    CYAN = "\033[0;36m"
+    WHITE = "\033[0;37m"
+    GRAY = "\033[0;90m"
+    BOLD_RED = '\033[0;1;31m'
+
+    # Platform/terminal specifics
+    DISABLED = False
+    if not (sys.stdout.isatty() and sys.stderr.isatty()):
+        # Disable colors if we don't write to a terminal
+        DISABLED = True
+        for _ in dir():
+            if isinstance(_, str) and _[0] != "_" and _ not in ['DISABLED']:
+                locals()[_] = ""
+    
+    elif sys.platform == 'win32':
+        # Set Windows console in VT mode
+        kernel32 = ctypes.windll.kernel32
+        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
+        del kernel32
```

## zut/commands.py

```diff
@@ -1,203 +1,209 @@
-"""
-Add and execute commands easily, based on argparse.
-Usefull for non-Django applications.
-For Django applications, use including command management instead.
-"""
-from __future__ import annotations
-import logging
-import sys
-from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
-from types import FunctionType, ModuleType
-from pathlib import Path
-from importlib import import_module
-from importlib.util import find_spec
-from typing import Any
-from .process import get_exit_code
-
-logger = logging.getLogger(__name__)
-
-
-def create_command_parser(prog: str = None, version: str = None, description: str = None):
-    parser = ArgumentParser(prog=prog, description=description, formatter_class=RawTextHelpFormatter)
-    parser.add_argument('--version', action='version', version=f"%(prog)s {version if version else ''}")
-    return parser
-
-
-def add_func_command(parser: ArgumentParser|_SubParsersAction, func: FunctionType, add_arguments: FunctionType = None, name: str = None, doc: str = None):
-    """
-    Add the given function as a subcommand of the parser.
-    """
-    if name is None:
-        name = func.__name__
-    if doc is None:
-        doc = func.__doc__
-
-    subparsers = get_commands_subparsers(parser)
-    cmdparser: ArgumentParser = subparsers.add_parser(name, help=get_help_text(doc), description=get_description_text(doc), formatter_class=RawTextHelpFormatter)
-    cmdparser.set_defaults(func=func)
-
-    if add_arguments:
-        add_arguments(cmdparser)
-
-    return cmdparser
-
-
-def add_module_command(parser: ArgumentParser|_SubParsersAction, module: str|ModuleType, name: str = None, doc: str = None):
-    """
-    Add the given module as a subcommand of the parser.
-    
-    The command function must be named `handler` and the arguments definition function, if any, must be named `add_arguments`.
-    """
-    if not isinstance(module, ModuleType):
-        module = import_module(module)
-
-    func = getattr(module, 'handle')
-
-    if name is None:
-        name = module.__name__.split(".")[-1]
-        if name.endswith('cmd') and len(name) > len('cmd'):
-            name = name[0:-len('cmd')]
-    
-    add_arguments = getattr(module, 'add_arguments', None)
-    add_func_command(parser, func, add_arguments=add_arguments, name=name, doc=doc)
-
-
-def add_package_commands(parser: ArgumentParser|_SubParsersAction, package: str):
-    """
-    Add all modules in the given package as subcommands of the parser.
-    """
-    package_spec = find_spec(package)
-    if not package_spec:
-        raise KeyError(f"package not found: {package}")
-    if not package_spec.origin:
-        raise KeyError(f"not a package: {package} (did you forget __init__.py ?)")
-    package_path = Path(package_spec.origin).parent
-    
-    for module_path in package_path.iterdir():
-        if module_path.is_dir() or module_path.name.startswith("_") or not module_path.name.endswith(".py"):
-            continue
-
-        module = module_path.stem
-        add_module_command(parser, f"{package}.{module}")
-
-
-def get_commands_subparsers(parser: ArgumentParser) -> _SubParsersAction:
-    """
-    Get or create the subparsers object associated with the given parser.
-    """
-    if isinstance(parser, _SubParsersAction):
-        return parser
-    elif parser._subparsers is not None:
-        return next(filter(lambda action: isinstance(action, _SubParsersAction), parser._subparsers._actions))
-    else:
-        return parser.add_subparsers(title='commands')
-
-
-def get_help_text(docstring: str):
-    if docstring is None:
-        return None
-    
-    docstring = docstring.strip()
-    try:
-        return docstring[0:docstring.index('\n')].strip()
-    except:
-        return docstring
-
-
-def get_description_text(docstring: str):
-    if docstring is None:
-        return None
-    
-    description = None
-    indent_size = 0
-    
-    for line in docstring.splitlines(keepends=False):
-        if description:
-            description += '\n' + line[indent_size:]
-        else:
-            indent_size = 0
-            for char in line:
-                if char not in [' ', '\t']:
-                    description = line[indent_size:]
-                    break
-                else:
-                    indent_size += 1
-
-    return description
-
-
-def _get_default_name(parser: ArgumentParser, default: FunctionType):
-    subparsers = get_commands_subparsers(parser)
-    for name, subparser in subparsers.choices.items():
-        if not isinstance(subparser, ArgumentParser):
-            continue
-        if subparser.get_default('func') == default:
-            return name
-
-    # not found: we add it    
-    add_func_command(subparsers, default)
-    return default.__name__
-
-
-def get_command_func(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None) -> tuple[FunctionType|int,dict[str,Any]]:
-    """
-    Run the command-line application, returning command result.
-    """
-    try:
-        namespace, unknown = parser.parse_known_args(args)
-    except SystemExit as e: # catch exit in case of --help or --version
-        return e.code, {}
-
-    kwargs = vars(namespace)
-    func = kwargs.pop('func', None)
-
-    if func:
-        if unknown:
-            logger.error(f"{parser.prog}: unrecognized arguments: {' '.join(unknown)}")
-            return 2, {}
-
-    elif default:
-        # call get_command_func again, adding default at start of unknown arguments
-        if not isinstance(default, str):
-            default = _get_default_name(parser, default)
-
-        if args is None:
-            args = sys.argv[1:]
-        new_args = args[:len(args)-len(unknown)] + [default] + unknown
-        return get_command_func(parser=parser, args=new_args)
-
-    else:
-        logger.error(f"{parser.prog}: missing command name")
-        return 2, {}
-    
-    return func, kwargs
-
-
-def run_command_func(func: FunctionType|int, **kwargs):
-    if isinstance(func, int):
-        return func
-    
-    try:
-        return func(**kwargs)
-    except KeyboardInterrupt:
-        logger.error("interrupted")
-        return 1
-
-
-def exec_command_func(func: FunctionType|int, **kwargs):
-    r = run_command_func(func, **kwargs)
-    r = get_exit_code(r)
-    exit(r)
-
-
-def run_command_parser(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None):
-    func, kwargs = get_command_func(parser=parser, default=default, args=args)
-    return run_command_func(func, **kwargs)
-
-
-def exec_command_parser(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None):
-    """
-    Run the command-line application and exit with appropriate return code.
-    """
-    func, kwargs = get_command_func(parser=parser, default=default, args=args)
-    exec_command_func(func, **kwargs)
+"""
+Add and execute commands easily, based on argparse.
+Usefull for non-Django applications.
+For Django applications, use including command management instead.
+"""
+from __future__ import annotations
+import logging
+import sys
+from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
+from types import FunctionType, ModuleType
+from pathlib import Path
+from importlib import import_module
+from importlib.util import find_spec
+from typing import Any
+from .process import get_exit_code
+
+logger = logging.getLogger(__name__)
+
+
+def create_command_parser(prog: str = None, version: str = None, description: str = None):
+    parser = ArgumentParser(prog=prog, description=description, formatter_class=RawTextHelpFormatter)
+    parser.add_argument('--version', action='version', version=f"%(prog)s {version if version else ''}")
+    return parser
+
+
+def add_func_command(parser: ArgumentParser|_SubParsersAction, func: FunctionType, add_arguments: FunctionType = None, name: str = None, doc: str = None):
+    """
+    Add the given function as a subcommand of the parser.
+    """
+    if name is None:
+        name = func.__name__
+    if doc is None:
+        doc = func.__doc__
+
+    subparsers = get_commands_subparsers(parser)
+    cmdparser: ArgumentParser = subparsers.add_parser(name, help=get_help_text(doc), description=get_description_text(doc), formatter_class=RawTextHelpFormatter)
+    cmdparser.set_defaults(func=func)
+
+    if add_arguments:
+        add_arguments(cmdparser)
+
+    return cmdparser
+
+
+def add_module_command(parser: ArgumentParser|_SubParsersAction, module: str|ModuleType, name: str = None, doc: str = None):
+    """
+    Add the given module as a subcommand of the parser.
+    
+    The command function must be named `handler` and the arguments definition function, if any, must be named `add_arguments`.
+    """
+    if not isinstance(module, ModuleType):
+        module = import_module(module)
+
+    func = getattr(module, 'handle')
+
+    if name is None:
+        name = module.__name__.split(".")[-1]
+        if name.endswith('cmd') and len(name) > len('cmd'):
+            name = name[0:-len('cmd')]
+    
+    add_arguments = getattr(module, 'add_arguments', None)
+    add_func_command(parser, func, add_arguments=add_arguments, name=name, doc=doc)
+
+
+def add_package_commands(parser: ArgumentParser|_SubParsersAction, package: str):
+    """
+    Add all modules in the given package as subcommands of the parser.
+    """
+    package_spec = find_spec(package)
+    if not package_spec:
+        raise KeyError(f"package not found: {package}")
+    if not package_spec.origin:
+        raise KeyError(f"not a package: {package} (did you forget __init__.py ?)")
+    package_path = Path(package_spec.origin).parent
+    
+    for module_path in package_path.iterdir():
+        if module_path.is_dir() or module_path.name.startswith("_") or not module_path.name.endswith(".py"):
+            continue
+
+        module = module_path.stem
+        add_module_command(parser, f"{package}.{module}")
+
+
+def get_commands_subparsers(parser: ArgumentParser) -> _SubParsersAction:
+    """
+    Get or create the subparsers object associated with the given parser.
+    """
+    if isinstance(parser, _SubParsersAction):
+        return parser
+    elif parser._subparsers is not None:
+        return next(filter(lambda action: isinstance(action, _SubParsersAction), parser._subparsers._actions))
+    else:
+        return parser.add_subparsers(title='commands')
+
+
+def get_help_text(docstring: str):
+    if docstring is None:
+        return None
+    
+    docstring = docstring.strip()
+    try:
+        return docstring[0:docstring.index('\n')].strip()
+    except:
+        return docstring
+
+
+def get_description_text(docstring: str):
+    if docstring is None:
+        return None
+    
+    description = None
+    indent_size = 0
+    
+    for line in docstring.splitlines(keepends=False):
+        if description:
+            description += '\n' + line[indent_size:]
+        else:
+            indent_size = 0
+            for char in line:
+                if char not in [' ', '\t']:
+                    description = line[indent_size:]
+                    break
+                else:
+                    indent_size += 1
+
+    return description
+
+
+def _get_default_name(parser: ArgumentParser, default: str|FunctionType|ModuleType) -> str:
+    if isinstance(default, str):
+        return default
+    
+    elif isinstance(default, ModuleType):
+        default = getattr(default, 'handle')
+
+    # try to find the default function
+    subparsers = get_commands_subparsers(parser)
+    for name, subparser in subparsers.choices.items():
+        if not isinstance(subparser, ArgumentParser):
+            continue
+        if subparser.get_default('func') == default:
+            return name
+
+    # not found: we add it    
+    add_func_command(subparsers, default)
+    return default.__name__
+
+
+def get_command_func(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None) -> tuple[FunctionType|int,dict[str,Any]]:
+    """
+    Run the command-line application, returning command result.
+    """
+    try:
+        namespace, unknown = parser.parse_known_args(args)
+    except SystemExit as e: # catch exit in case of --help or --version
+        return e.code, {}
+
+    kwargs = vars(namespace)
+    func = kwargs.pop('func', None)
+
+    if func:
+        if unknown:
+            logger.error(f"{parser.prog}: unrecognized arguments: {' '.join(unknown)}")
+            return 2, {}
+
+    elif default:
+        # call get_command_func again, adding default at start of unknown arguments
+        default = _get_default_name(parser, default)
+
+        if args is None:
+            args = sys.argv[1:]
+        new_args = args[:len(args)-len(unknown)] + [default] + unknown
+        return get_command_func(parser=parser, args=new_args)
+
+    else:
+        logger.error(f"{parser.prog}: missing command name")
+        return 2, {}
+    
+    return func, kwargs
+
+
+def run_command_func(func: FunctionType|int, **kwargs):
+    if isinstance(func, int):
+        return func
+    
+    try:
+        return func(**kwargs)
+    except KeyboardInterrupt:
+        logger.error("interrupted")
+        return 1
+
+
+def exec_command_func(func: FunctionType|int, **kwargs):
+    r = run_command_func(func, **kwargs)
+    r = get_exit_code(r)
+    exit(r)
+
+
+def run_command_parser(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None):
+    func, kwargs = get_command_func(parser=parser, default=default, args=args)
+    return run_command_func(func, **kwargs)
+
+
+def exec_command_parser(parser: ArgumentParser, default: str|FunctionType = None, args: list[str] = None):
+    """
+    Run the command-line application and exit with appropriate return code.
+    """
+    func, kwargs = get_command_func(parser=parser, default=default, args=args)
+    exec_command_func(func, **kwargs)
```

## zut/config.py

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-from __future__ import annotations
-import sys
-from configparser import ConfigParser, _UNSET
-from pathlib import Path
-
-
-def get_config(local: str|Path|list[str|Path] = 'local.conf', *, name: str = None, directory: str = None, default: str|Path|list[str|Path] = None) -> ExtendedConfigParser:
-    files = []
-
-    # Add default file(s)
-    if default:
-        if not isinstance(default, (list,tuple)):
-            default = [default]
-
-        for path in default:
-            if not isinstance(path, Path):
-                path = Path(path)
-            files.append(path.expanduser())
-
-    # Add system and user files
-    if name:
-        pathend = f'{name}.conf'
-        if directory:
-            pathend = f'{directory}/{pathend}'
-
-        # Add system file
-        files.append(Path(f'C:/ProgramData/{pathend}' if sys.platform == 'win32' else f'/etc/{pathend}').expanduser())
-        
-        # Add user file
-        files.append(Path(f'~/.config/{pathend}').expanduser())
-
-    # Add local file(s)
-    if local:
-        if not isinstance(local, (list,tuple)):
-            local = [local]
-
-        for path in local:
-            if not isinstance(path, Path):
-                path = Path(path)
-            files.append(path.expanduser())
-
-    # Build config parser
-    parser = ExtendedConfigParser()
-    parser.read(files, encoding='utf-8')
-    return parser
-
-
-class ExtendedConfigParser(ConfigParser):
-    def getlist(self, section: str, option: str, *, fallback: list[str]|None = _UNSET, separator: str = ',') -> list[str]:
-        try:            
-            values_str = self.get(section, option)
-        except:
-            if fallback != _UNSET:
-                return fallback
-            raise
-
-        values = []
-        for value in values_str.split(separator):
-            value = value.strip()
-            if not value:
-                continue
-            values.append(value)
-
-        return values
+from __future__ import annotations
+import sys
+from configparser import ConfigParser, _UNSET
+from pathlib import Path
+
+
+def get_config(local: str|Path|list[str|Path] = 'local.conf', *, name: str = None, directory: str = None, default: str|Path|list[str|Path] = None) -> ExtendedConfigParser:
+    files = []
+
+    # Add default file(s)
+    if default:
+        if not isinstance(default, (list,tuple)):
+            default = [default]
+
+        for path in default:
+            if not isinstance(path, Path):
+                path = Path(path)
+            files.append(path.expanduser())
+
+    # Add system and user files
+    if name:
+        pathend = f'{name}.conf'
+        if directory:
+            pathend = f'{directory}/{pathend}'
+
+        # Add system file
+        files.append(Path(f'C:/ProgramData/{pathend}' if sys.platform == 'win32' else f'/etc/{pathend}').expanduser())
+        
+        # Add user file
+        files.append(Path(f'~/.config/{pathend}').expanduser())
+
+    # Add local file(s)
+    if local:
+        if not isinstance(local, (list,tuple)):
+            local = [local]
+
+        for path in local:
+            if not isinstance(path, Path):
+                path = Path(path)
+            files.append(path.expanduser())
+
+    # Build config parser
+    parser = ExtendedConfigParser()
+    parser.read(files, encoding='utf-8')
+    return parser
+
+
+class ExtendedConfigParser(ConfigParser):
+    def getlist(self, section: str, option: str, *, fallback: list[str]|None = _UNSET, separator: str = ',') -> list[str]:
+        try:            
+            values_str = self.get(section, option)
+        except:
+            if fallback != _UNSET:
+                return fallback
+            raise
+
+        values = []
+        for value in values_str.split(separator):
+            value = value.strip()
+            if not value:
+                continue
+            values.append(value)
+
+        return values
```

## zut/credentials.py

 * *Ordering differences only*

```diff
@@ -1,259 +1,259 @@
-from __future__ import annotations
-
-import os
-import subprocess
-import sys
-from argparse import ArgumentParser
-from datetime import datetime
-from getpass import getpass
-
-
-if sys.platform == "win32":
-    import win32cred
-    from keyring.backends.Windows import WinVaultKeyring
-
-    # See: https://docs.microsoft.com/en-us/windows/win32/api/wincred/ns-wincred-credentiala
-    CRED_TYPE_GENERIC = 1 # The credential is a generic credential. The credential will not be used by any particular authentication package. The credential will be stored securely but has no other significant characteristics.
-    CRED_TYPE_DOMAIN_PASSWORD = 2 # The credential is a password credential and is specific to Microsoft's authentication packages. The NTLM, Kerberos, and Negotiate authentication packages will automatically use this credential when connecting to the named target.
-    CRED_TYPE_DOMAIN_CERTIFICATE = 3 # The credential is a certificate credential and is specific to Microsoft's authentication packages. The Kerberos, Negotiate, and Schannel authentication packages automatically use this credential when connecting to the named target.
-
-    enumerate_additional_props = ["LastWritten", "TargetAlias", "Comment", "Type", "Flags", "CredentialBlob"]
-    ENUMERATE_HEADERS = ["Service", "Username", *enumerate_additional_props]
-
-    def enumerate_credentials(service=None, username=None):
-        if service is not None:
-            service = service.lower()
-        if username is not None:
-            username = username.lower()
-        
-        for cred in win32cred.CredEnumerate():
-            cred_service = cred["TargetName"]
-            cred_username = cred["UserName"]
-
-            # Filter
-            if service is not None:
-                if not service in cred_service.lower():
-                    continue
-            if username is not None:
-                if cred_username is None or not username in cred_username.lower():
-                    continue
-
-            # Additional properties
-            props = []
-            for prop in enumerate_additional_props:
-                if prop == "CredentialBlob":
-                    props.append("X" if cred[prop] else "")
-                elif prop == "Type":
-                    indication = ""
-                    if cred["Type"] == CRED_TYPE_GENERIC:
-                        indication = "generic"
-                    elif cred["Type"] == CRED_TYPE_DOMAIN_PASSWORD:
-                        indication = "domain password"
-                    elif cred["Type"] == CRED_TYPE_DOMAIN_CERTIFICATE:
-                        indication = "domain certificate"
-                    props.append(str(cred[prop]) + (" (%s)" % indication if indication else ""))
-                else:
-                    props.append(cred[prop])
-
-            yield [cred_service, cred_username, *props]
-    
-
-    def get_username(service):
-        found = None
-
-        for cred in win32cred.CredEnumerate():
-            if cred["Type"] == CRED_TYPE_GENERIC and cred["TargetName"] == service and cred["UserName"]:
-                if found:
-                    raise ValueError(f"several usernames found for service {service}")
-                found = cred["UserName"]
-
-        if not found:
-            return None
-        
-        return found
-
-
-    _keyring = None
-
-    def _get_keyring():
-        global _keyring
-        if _keyring is None:
-            _keyring = WinVaultKeyring()
-        return _keyring
-
-    def get_password(service, username = None):
-        return _get_keyring().get_password(service, username if username else '')
-
-
-    def set_password(service, username, password):
-        return _get_keyring().set_password(service, username, password)
-
-
-    def delete_password(service, username):
-        return _get_keyring().delete_password(service, username)
-
-
-else: # sys.platform != "win32"
-    import re
-    from glob import glob
-
-    _several_slashes_re = re.compile(r"/{2,}")
-
-    def _sanitize_part(part):
-        """ Replace several slashes with only one """
-        if not part:
-            return part
-        part = part.replace("://", ":")
-        return _several_slashes_re.sub("/", part)
-        
-    def _get_key(service, username):
-        if not service:
-            raise ValueError(f'service name must be provided')
-        
-        parts = [_sanitize_part(service)]
-        if username:
-            parts.append(_sanitize_part(username))
-
-        return os.path.join(*parts).rstrip("/")
-    
-    ENUMERATE_HEADERS = ["Service", "Username", "LastWritten", "Path"]
-
-    def enumerate_credentials(service=None, username=None):
-        if service is not None:
-            service = _sanitize_part(service.lower())
-        if username is not None:
-            username = _sanitize_part(username.lower())
-
-        prefix = os.path.expanduser("~/.password-store/")
-        for path in glob(f"{prefix}**/*.gpg", recursive=True):
-            name_without_extension = path[:-4]
-            name_parts = name_without_extension[len(prefix):].rsplit("/", 1)
-            if len(name_parts) == 0:             
-                cred_service = ""
-                cred_username = ""
-            elif len(name_parts) == 1:                
-                cred_service = name_parts[0]
-                cred_username = ""
-            else:
-                cred_service = "/".join(name_parts[:-1])
-                cred_username = name_parts[-1]
-
-            # Filter
-            if service is not None:
-                if not service in cred_service.lower():
-                    continue
-            if username is not None:
-                if cred_username or not username in cred_username.lower():
-                    continue
-
-            last_written = datetime.fromtimestamp(os.path.getmtime(path))
-            yield [cred_service, cred_username, last_written, path]
-
-        
-    def get_username(service):
-        found = None
-
-        service = _sanitize_part(service)
-        for cred in enumerate_credentials():
-            if cred[0] == service and cred[1]:
-                if found:
-                    raise ValueError(f"several usernames found for service {service}")
-                found = cred[1]
-
-        if not found:
-            return None
-        
-        return found
-
-    
-    _not_in_the_password_store_re = re.compile(r"^Error: .+ is not in the password store.$")
-    def get_password(service, username = None):
-        key = _get_key(service, username)
-        try:
-            cp = subprocess.run(["pass", "show", key], capture_output=True, check=True, text=True)
-        except subprocess.CalledProcessError as err:
-            if cp.returncode == 1 and _not_in_the_password_store_re.match(cp.stderr):
-                return None
-            else:
-                raise err
-        return cp.stdout.splitlines()[0]
-
-
-    def set_password(service, username, password):
-        if not service:
-            raise ValueError("service cannot be empty")
-
-        password = password.splitlines()[0]
-        inp = '%s\n' % password
-        inp *= 2
-        
-        key = _get_key(service, username)
-        subprocess.run(['pass', 'insert', '--force', key], input=inp, capture_output=True, check=True, text=True)
-
-
-    def delete_password(service, username):
-        key = _get_key(service, username)
-        subprocess.run(['pass', 'rm', '--force', key], check=True, capture_output=True)
-
-
-def add_arguments(parser: ArgumentParser):
-    parser.add_argument("action", nargs="?", choices=["list", "ls", "get", "set", "delete", "del", "rm"], default="list", help="action to perform")
-    parser.add_argument("service", nargs="?", help="service name")
-    parser.add_argument("username", nargs="?", help="user name")
-
-
-def handle(action: str = None, service: str = None, username: str = None):
-    """
-    List, get or set credentials from the credentials manager.
-    """    
-    if action == "get":
-        # Get a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        print(get_password(service, username))
-
-    elif action == "set":
-        # Set a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        set_password(service, username, getpass(f"Password for service \"{service}\" (username: \"{username}\"): "))
-
-    elif action in ["delete", "del", "rm"]:
-        # Delete a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        delete_password(service, username)
-
-    else: # list
-        from tabulate import tabulate
-
-        # Search/list available credentials
-        if sys.platform == "win32":
-            print("See: rundll32.exe keymgr.dll, KRShowKeyMgr")
-        else:
-            print("See: find ~/.password-store -type f -name '*.gpg'")
-            
-        rows = []
-        for cred in enumerate_credentials(service=service, username=username):
-            rows.append(cred)
-
-        rows.sort(key=lambda row: row[0].lower())
-
-        # Display LastWritten as simplified local time
-        try:
-            i = ENUMERATE_HEADERS.index("LastWritten")            
-            for row in rows:
-                localdt = row[i].astimezone()
-                row[i] = localdt.strftime("%H:%M" if localdt.date() == datetime.now().date() else "%Y-%m-%d")
-
-        except ValueError:
-            pass
-
-
-        print(tabulate(rows, ENUMERATE_HEADERS))
+from __future__ import annotations
+
+import os
+import subprocess
+import sys
+from argparse import ArgumentParser
+from datetime import datetime
+from getpass import getpass
+
+
+if sys.platform == "win32":
+    import win32cred
+    from keyring.backends.Windows import WinVaultKeyring
+
+    # See: https://docs.microsoft.com/en-us/windows/win32/api/wincred/ns-wincred-credentiala
+    CRED_TYPE_GENERIC = 1 # The credential is a generic credential. The credential will not be used by any particular authentication package. The credential will be stored securely but has no other significant characteristics.
+    CRED_TYPE_DOMAIN_PASSWORD = 2 # The credential is a password credential and is specific to Microsoft's authentication packages. The NTLM, Kerberos, and Negotiate authentication packages will automatically use this credential when connecting to the named target.
+    CRED_TYPE_DOMAIN_CERTIFICATE = 3 # The credential is a certificate credential and is specific to Microsoft's authentication packages. The Kerberos, Negotiate, and Schannel authentication packages automatically use this credential when connecting to the named target.
+
+    enumerate_additional_props = ["LastWritten", "TargetAlias", "Comment", "Type", "Flags", "CredentialBlob"]
+    ENUMERATE_HEADERS = ["Service", "Username", *enumerate_additional_props]
+
+    def enumerate_credentials(service=None, username=None):
+        if service is not None:
+            service = service.lower()
+        if username is not None:
+            username = username.lower()
+        
+        for cred in win32cred.CredEnumerate():
+            cred_service = cred["TargetName"]
+            cred_username = cred["UserName"]
+
+            # Filter
+            if service is not None:
+                if not service in cred_service.lower():
+                    continue
+            if username is not None:
+                if cred_username is None or not username in cred_username.lower():
+                    continue
+
+            # Additional properties
+            props = []
+            for prop in enumerate_additional_props:
+                if prop == "CredentialBlob":
+                    props.append("X" if cred[prop] else "")
+                elif prop == "Type":
+                    indication = ""
+                    if cred["Type"] == CRED_TYPE_GENERIC:
+                        indication = "generic"
+                    elif cred["Type"] == CRED_TYPE_DOMAIN_PASSWORD:
+                        indication = "domain password"
+                    elif cred["Type"] == CRED_TYPE_DOMAIN_CERTIFICATE:
+                        indication = "domain certificate"
+                    props.append(str(cred[prop]) + (" (%s)" % indication if indication else ""))
+                else:
+                    props.append(cred[prop])
+
+            yield [cred_service, cred_username, *props]
+    
+
+    def get_username(service):
+        found = None
+
+        for cred in win32cred.CredEnumerate():
+            if cred["Type"] == CRED_TYPE_GENERIC and cred["TargetName"] == service and cred["UserName"]:
+                if found:
+                    raise ValueError(f"several usernames found for service {service}")
+                found = cred["UserName"]
+
+        if not found:
+            return None
+        
+        return found
+
+
+    _keyring = None
+
+    def _get_keyring():
+        global _keyring
+        if _keyring is None:
+            _keyring = WinVaultKeyring()
+        return _keyring
+
+    def get_password(service, username = None):
+        return _get_keyring().get_password(service, username if username else '')
+
+
+    def set_password(service, username, password):
+        return _get_keyring().set_password(service, username, password)
+
+
+    def delete_password(service, username):
+        return _get_keyring().delete_password(service, username)
+
+
+else: # sys.platform != "win32"
+    import re
+    from glob import glob
+
+    _several_slashes_re = re.compile(r"/{2,}")
+
+    def _sanitize_part(part):
+        """ Replace several slashes with only one """
+        if not part:
+            return part
+        part = part.replace("://", ":")
+        return _several_slashes_re.sub("/", part)
+        
+    def _get_key(service, username):
+        if not service:
+            raise ValueError(f'service name must be provided')
+        
+        parts = [_sanitize_part(service)]
+        if username:
+            parts.append(_sanitize_part(username))
+
+        return os.path.join(*parts).rstrip("/")
+    
+    ENUMERATE_HEADERS = ["Service", "Username", "LastWritten", "Path"]
+
+    def enumerate_credentials(service=None, username=None):
+        if service is not None:
+            service = _sanitize_part(service.lower())
+        if username is not None:
+            username = _sanitize_part(username.lower())
+
+        prefix = os.path.expanduser("~/.password-store/")
+        for path in glob(f"{prefix}**/*.gpg", recursive=True):
+            name_without_extension = path[:-4]
+            name_parts = name_without_extension[len(prefix):].rsplit("/", 1)
+            if len(name_parts) == 0:             
+                cred_service = ""
+                cred_username = ""
+            elif len(name_parts) == 1:                
+                cred_service = name_parts[0]
+                cred_username = ""
+            else:
+                cred_service = "/".join(name_parts[:-1])
+                cred_username = name_parts[-1]
+
+            # Filter
+            if service is not None:
+                if not service in cred_service.lower():
+                    continue
+            if username is not None:
+                if cred_username or not username in cred_username.lower():
+                    continue
+
+            last_written = datetime.fromtimestamp(os.path.getmtime(path))
+            yield [cred_service, cred_username, last_written, path]
+
+        
+    def get_username(service):
+        found = None
+
+        service = _sanitize_part(service)
+        for cred in enumerate_credentials():
+            if cred[0] == service and cred[1]:
+                if found:
+                    raise ValueError(f"several usernames found for service {service}")
+                found = cred[1]
+
+        if not found:
+            return None
+        
+        return found
+
+    
+    _not_in_the_password_store_re = re.compile(r"^Error: .+ is not in the password store.$")
+    def get_password(service, username = None):
+        key = _get_key(service, username)
+        try:
+            cp = subprocess.run(["pass", "show", key], capture_output=True, check=True, text=True)
+        except subprocess.CalledProcessError as err:
+            if cp.returncode == 1 and _not_in_the_password_store_re.match(cp.stderr):
+                return None
+            else:
+                raise err
+        return cp.stdout.splitlines()[0]
+
+
+    def set_password(service, username, password):
+        if not service:
+            raise ValueError("service cannot be empty")
+
+        password = password.splitlines()[0]
+        inp = '%s\n' % password
+        inp *= 2
+        
+        key = _get_key(service, username)
+        subprocess.run(['pass', 'insert', '--force', key], input=inp, capture_output=True, check=True, text=True)
+
+
+    def delete_password(service, username):
+        key = _get_key(service, username)
+        subprocess.run(['pass', 'rm', '--force', key], check=True, capture_output=True)
+
+
+def add_arguments(parser: ArgumentParser):
+    parser.add_argument("action", nargs="?", choices=["list", "ls", "get", "set", "delete", "del", "rm"], default="list", help="action to perform")
+    parser.add_argument("service", nargs="?", help="service name")
+    parser.add_argument("username", nargs="?", help="user name")
+
+
+def handle(action: str = None, service: str = None, username: str = None):
+    """
+    List, get or set credentials from the credentials manager.
+    """    
+    if action == "get":
+        # Get a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        print(get_password(service, username))
+
+    elif action == "set":
+        # Set a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        set_password(service, username, getpass(f"Password for service \"{service}\" (username: \"{username}\"): "))
+
+    elif action in ["delete", "del", "rm"]:
+        # Delete a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        delete_password(service, username)
+
+    else: # list
+        from tabulate import tabulate
+
+        # Search/list available credentials
+        if sys.platform == "win32":
+            print("See: rundll32.exe keymgr.dll, KRShowKeyMgr")
+        else:
+            print("See: find ~/.password-store -type f -name '*.gpg'")
+            
+        rows = []
+        for cred in enumerate_credentials(service=service, username=username):
+            rows.append(cred)
+
+        rows.sort(key=lambda row: row[0].lower())
+
+        # Display LastWritten as simplified local time
+        try:
+            i = ENUMERATE_HEADERS.index("LastWritten")            
+            for row in rows:
+                localdt = row[i].astimezone()
+                row[i] = localdt.strftime("%H:%M" if localdt.date() == datetime.now().date() else "%Y-%m-%d")
+
+        except ValueError:
+            pass
+
+
+        print(tabulate(rows, ENUMERATE_HEADERS))
```

## zut/csv.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-from __future__ import annotations
-import csv
-from io import IOBase
-from locale import getlocale
-from pathlib import Path
-
-from .text import reconfigure_encoding
-from . import filesh
-
-def get_default_csv_delimiter():
-    locale = getlocale()
-    if locale and locale[0] and locale[0].startswith('fr'):
-        return ';'
-    else:
-        return ','
-
-
-def get_csv_headers(csv_file: str|Path|IOBase, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"'):
-    if delimiter is None:
-        delimiter = get_default_csv_delimiter()
-        
-    fp = None
-    fp_to_close = None
-
-    try:        
-        if isinstance(csv_file, (str,Path)):
-            fp = filesh.open_file(csv_file, 'r', newline='', encoding=encoding)
-            fp_to_close = fp
-            encoding = reconfigure_encoding(fp, encoding)
-        else:
-            fp = csv_file
-            fp.seek(0)
-            
-        reader = csv.reader(fp, delimiter=delimiter, quotechar=quotechar)
-        try:
-            return next(reader)
-        except StopIteration:
-            return None
-
-    finally:
-        if fp_to_close:
-            fp_to_close.close()
-        else:
-            fp.seek(0)
+from __future__ import annotations
+import csv
+from io import IOBase
+from locale import getlocale
+from pathlib import Path
+
+from .text import reconfigure_encoding
+from . import filesh
+
+def get_default_csv_delimiter():
+    locale = getlocale()
+    if locale and locale[0] and locale[0].startswith('fr'):
+        return ';'
+    else:
+        return ','
+
+
+def get_csv_headers(csv_file: str|Path|IOBase, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"'):
+    if delimiter is None:
+        delimiter = get_default_csv_delimiter()
+        
+    fp = None
+    fp_to_close = None
+
+    try:        
+        if isinstance(csv_file, (str,Path)):
+            fp = filesh.open_file(csv_file, 'r', newline='', encoding=encoding)
+            fp_to_close = fp
+            encoding = reconfigure_encoding(fp, encoding)
+        else:
+            fp = csv_file
+            fp.seek(0)
+            
+        reader = csv.reader(fp, delimiter=delimiter, quotechar=quotechar)
+        try:
+            return next(reader)
+        except StopIteration:
+            return None
+
+    finally:
+        if fp_to_close:
+            fp_to_close.close()
+        else:
+            fp.seek(0)
```

## zut/datetime.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-from __future__ import annotations
-from datetime import datetime, time, timezone, timedelta
-
-
-def is_aware(value: datetime|time):
-    if value is None:
-        return False
-    return value.utcoffset() is not None
-
-
-def make_aware(value: datetime|time, tz: timezone = 'local'):
-    """
-    Make a datetime aware in the timezone `tz` (use `tz='local'` for the local system timezone).
-    """
-    if value is None:
-        return None
-    return value.astimezone(None if tz == 'local' else tz)
-
-
-def now_aware(tz: timezone = 'local', no_microsecond = False):
-    """
-    Get the current datetime in the timezone `tz` (use `tz='local'` for the local system timezone).
-    """
-    now = datetime.now(timezone.utc).astimezone(None if tz == 'local' else tz)
-    if no_microsecond:
-        now = now.replace(microsecond=0)
-    return now
-
-
-def duration_iso_string(duration):
-    # Adapted from: django.utils.duration.duration_iso_string
-    if duration < timedelta(0):
-        sign = "-"
-        duration *= -1
-    else:
-        sign = ""
-
-    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
-    ms = ".{:06d}".format(microseconds) if microseconds else ""
-    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
-        sign, days, hours, minutes, seconds, ms
-    )
-
-
-def _get_duration_components(duration: timedelta):
-    days = duration.days
-    seconds = duration.seconds
-    microseconds = duration.microseconds
-
-    minutes = seconds // 60
-    seconds = seconds % 60
-
-    hours = minutes // 60
-    minutes = minutes % 60
-
-    return days, hours, minutes, seconds, microseconds
-
+from __future__ import annotations
+from datetime import datetime, time, timezone, timedelta
+
+
+def is_aware(value: datetime|time):
+    if value is None:
+        return False
+    return value.utcoffset() is not None
+
+
+def make_aware(value: datetime|time, tz: timezone = 'local'):
+    """
+    Make a datetime aware in the timezone `tz` (use `tz='local'` for the local system timezone).
+    """
+    if value is None:
+        return None
+    return value.astimezone(None if tz == 'local' else tz)
+
+
+def now_aware(tz: timezone = 'local', no_microsecond = False):
+    """
+    Get the current datetime in the timezone `tz` (use `tz='local'` for the local system timezone).
+    """
+    now = datetime.now(timezone.utc).astimezone(None if tz == 'local' else tz)
+    if no_microsecond:
+        now = now.replace(microsecond=0)
+    return now
+
+
+def duration_iso_string(duration):
+    # Adapted from: django.utils.duration.duration_iso_string
+    if duration < timedelta(0):
+        sign = "-"
+        duration *= -1
+    else:
+        sign = ""
+
+    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
+    ms = ".{:06d}".format(microseconds) if microseconds else ""
+    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
+        sign, days, hours, minutes, seconds, ms
+    )
+
+
+def _get_duration_components(duration: timedelta):
+    days = duration.days
+    seconds = duration.seconds
+    microseconds = duration.microseconds
+
+    minutes = seconds // 60
+    seconds = seconds % 60
+
+    hours = minutes // 60
+    minutes = minutes % 60
+
+    return days, hours, minutes, seconds, microseconds
+
```

## zut/excel.py

```diff
@@ -1,479 +1,479 @@
-from __future__ import annotations
-import logging
-from pathlib import Path
-from typing import Any
-from .tabular import Row
-from . import filesh
-from openpyxl import load_workbook, Workbook, DEFUSEDXML
-from openpyxl.worksheet.worksheet import Worksheet
-from openpyxl.worksheet.table import Table, TableColumn, TableFormula, TableStyleInfo
-from openpyxl.worksheet.formula import DataTableFormula, ArrayFormula
-from openpyxl.worksheet.filters import AutoFilter
-from openpyxl.cell.cell import Cell
-from openpyxl.styles.differential import DifferentialStyle, DifferentialStyleList
-from openpyxl.styles.fills import PatternFill
-from openpyxl.utils import range_boundaries, get_column_letter
-
-logger = logging.getLogger(__name__)
-
-
-class ExcelWorkbook:
-    _workbook_cache: dict[str,ExcelWorkbook] = {}
-    """ Workbooks per canonical path """
-    
-    _defusedxml_alert_emitted = False
-
-    def __init__(self, path: str|Path):        
-        if not DEFUSEDXML and not self.__class__._defusedxml_alert_emitted:
-            logger.warning("By default openpyxl does not guard against quadratic blowup or billion laughs xml attacks. To guard against these attacks install defusedxml.")
-            self.__class__._defusedxml_alert_emitted = True
-
-        self.path = Path(path) if not isinstance(path, Path) else path
-        
-        if filesh.exists(self.path):
-            logger.debug("load workbook %s", self.path)
-            with filesh.open_file(self.path, 'rb') as fp:
-                self._pyxl_workbook: Workbook = load_workbook(fp)
-            self._create_next_table_in_active_sheet = False
-        else:
-            logger.debug("create workbook for %s", self.path)
-            self._pyxl_workbook = Workbook()
-            self._create_next_table_in_active_sheet = True
-
-        self._tables: dict[str,ExcelTable] = {}
-        self._needs_save = False
-
-
-    @classmethod
-    def get_cached(cls, path: str|Path) -> ExcelWorkbook:
-        path = Path(path) if not isinstance(path, Path) else path
-        canonical_path = str(path.resolve())
-        return cls._workbook_cache[canonical_path]
-
-
-    @classmethod
-    def get_or_create_cached(cls, path: str|Path) -> ExcelWorkbook:
-        path = Path(path) if not isinstance(path, Path) else path
-        canonical_path = str(path.resolve())
-        if not canonical_path in cls._workbook_cache:
-            cls._workbook_cache[canonical_path] = ExcelWorkbook(path)
-        return cls._workbook_cache[canonical_path]
-
-
-    @classmethod
-    def close_all_cached(cls):
-        for workbook in cls._workbook_cache.values():
-            workbook.close()
-
-
-    def close(self):
-        if not self._needs_save:
-            return
-        
-        for table in self._tables.values():
-            table.redefine()
-        
-        logger.debug("save workbook %s", self.path)
-        with filesh.open_file(self.path, 'wb') as fp:
-            self._pyxl_workbook.save(fp)
-
-        self._needs_save = False
-        
-
-    def get_table(self, name: str, default = '__raise__') -> ExcelTable:
-        if name in self._tables:
-            return self._tables[name]
-        
-        for sheet_name in self._pyxl_workbook.sheetnames:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
-            if name in pyxl_worksheet.tables:
-                pyxl_table = pyxl_worksheet.tables[name]
-                self._tables[name] = ExcelTable(pyxl_table, pyxl_worksheet, self)
-                return self._tables[name]
-            
-        if default == '__raise__':
-            raise KeyError(f"no table found with name \"{name}\" in workbook \"{self.path}\"")
-        else:
-            return default
-        
-
-    def create_table(self, name: str, no_headers: bool = False) -> ExcelTable:
-        for sheet_name in self._pyxl_workbook.sheetnames:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
-            if name in pyxl_worksheet.tables:
-                raise ValueError(f"table {name} already exist")
-    
-        self._needs_save = True
-        
-        if self._create_next_table_in_active_sheet:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook.active
-            pyxl_worksheet.title = name
-            self._create_next_table_in_active_sheet = False
-        else:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook.create_sheet(title=name)
-
-        self._tables[name] = ExcelTable(name, pyxl_worksheet, self)
-        return self._tables[name]
-
-
-class ExcelTable:
-    def __init__(self, pyxl_table: Table|str, pyxl_worksheet: Worksheet, workbook: ExcelWorkbook, no_headers: bool = None):
-        self.workbook = workbook
-        self._pyxl_worksheet = pyxl_worksheet        
-
-        if isinstance(pyxl_table, str):
-            self._pyxl_table: Table = None
-            self.name: str = pyxl_table
-            if no_headers:
-                self.has_headers = False
-                self._min_row_index = 0
-            else:
-                self.has_headers = True
-                self._min_row_index = 1
-            self._min_col_index = 0
-            self._row_count = 0
-            self._column_names: list[str] = []
-
-        elif isinstance(pyxl_table, Table):
-            self._pyxl_table: Table = pyxl_table
-            self.name: str = pyxl_table.name
-            
-            min_col, min_row, max_col, max_row = range_boundaries(pyxl_table.ref)
-
-            # NOTE: range_boundaries returns 1-base indices
-            self._min_col_index = min_col - 1
-            if pyxl_table.headerRowCount == 0:
-                self.has_headers = False
-                self._min_row_index = min_row - 1
-            elif pyxl_table.headerRowCount == 1:
-                self.has_headers = True
-                self._min_row_index = min_row
-            else:
-                raise ValueError(f'invalid headerRowCount: {pyxl_table.headerRowCount}')
-            
-            self._row_count = max_row - self._min_row_index
-            col_count = max_col - self._min_col_index
-        
-            self._column_names: list[str] = list(self._pyxl_table.column_names)
-            # NOTE: if no headers, default names are returned, example in French: ['Colonne1', 'Colonne2']
-            if len(self._column_names) != col_count:
-                raise ValueError(f'invalid column_names length ({len(self._column_names)}, expected {col_count}): {self._column_names}')
-        else:
-            raise ValueError(f"invalid type for pyxl_table: {type(pyxl_table).__name__}")
-
-        self._column_formats: list[dict[str,Any]] = None
-
-
-    @property
-    def column_names(self) -> list[str]:
-        return self._column_names
-
-    @property
-    def min_row_index(self) -> int:
-        """ 0-base index of the first data row. """
-        return self._min_row_index
-    
-    @property
-    def min_col_index(self) -> int:
-        """ 0-base index of the first column. """
-        return self._min_col_index
-    
-    @property
-    def row_count(self) -> int:
-        return self._row_count
-    
-    @property
-    def col_count(self) -> int:
-        return len(self._column_names)
-    
-    @property
-    def ref(self) -> str:
-        if self.col_count == 0:
-            raise ValueError(f"cannot get table ref: table does not contain any column")
-        if self.row_count == 0 and not self.has_headers:
-            raise ValueError(f"cannot get table ref: table does not contain any line")
-                
-        return f"{get_column_letter(self._min_col_index + 1)}{self._min_row_index - (1 if self.has_headers else 0) + 1}:{get_column_letter(self._min_col_index + self.col_count)}{self._min_row_index + self._row_count}"
-
-
-    def get_row(self, index: int) -> Row:
-        """
-        Get row at the given 0-base index.
-        """
-        if index == -1:
-            if self.row_count == 0:
-                raise ValueError(f"cannot get last row: table does not contain any row")
-            index = self.row_count - 1
-        elif index < 0:
-            raise ValueError(f"invalid row index: {index}")
-        
-        if index >= self.row_count:
-            raise ValueError(f"cannot get row at index {index}: table contains {self.row_count} rows")
-
-        return Row(
-            get=lambda col_index: self.get_value(index, col_index),
-            set=lambda col_index, value: self.set_value(index, col_index, value),
-            headers=self.column_names
-        )
-    
-
-    def insert_row(self) -> Row:
-        self.workbook._needs_save = True
-        row_index = self._row_count
-
-        # ensure empty values
-        for col_index in range(0, self.col_count):
-            cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
-            if cell.value is not None:
-                raise ValueError(f'cannot insert row: row at index {row_index} is not empty')
-
-        self._row_count += 1
-
-        # erase old styles and apply column format
-        for col_index in range(0, self.col_count):
-            self.erase_cell(row_index, col_index)
-            
-        return self.get_row(row_index)
-    
-
-    def insert_col(self, name: str):
-        self.workbook._needs_save = True
-        if not name:
-            raise ValueError(f'name cannot be empty')
-        if name in self.column_names:
-            raise ValueError(f'column name already used: {name}')
-            
-        col_index = self.col_count
-
-        # ensure empty values
-        for row_index in range(0, self._row_count):
-            cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
-            if cell.value is not None:
-                raise ValueError(f'cannot insert column: column {col_index} is not empty')
-
-        self.column_names.append(name) # implies self.col_count += 1
-
-        if self.has_headers:
-            cell = self._pyxl_worksheet.cell(self._min_row_index, self._min_col_index + 1 + col_index)
-            #logger.debug('set column name to cell %s%s: %s', cell.column_letter, cell.row, name)
-            cell.value = name
-
-        # erase old styles and apply column format
-        for row_index in range(0, self._row_count):
-            self.erase_cell(row_index, col_index)
-            
-
-    def truncate(self):
-        self.workbook._needs_save = True
-        prev_row_count = self._row_count
-        self._row_count = 0
-
-        for row_index in range(0, prev_row_count):
-            for col_index in range(0, self.col_count):
-                self.erase_cell(row_index, col_index, allow_outside=True)
-
-
-    def redefine(self):
-        self.workbook._needs_save = True
-        new_ref = self.ref
-        if self._pyxl_table is not None and new_ref == self._pyxl_table.ref:
-            return
-        
-        logger.debug("define table %s: %s => %s", self.name, self._pyxl_table.ref if self._pyxl_table is not None else None, new_ref)
-
-        newcolumns = []
-
-        for i in range(0, self.col_count):
-            if self.has_headers:
-                name = self._pyxl_worksheet.cell(self._min_row_index, self._min_col_index + 1 + i).value
-            else:
-                name = self.column_names[i] if i < len(self.column_names) else None
-            newcolumn = TableColumn(id=i+1, name=name)
-            newcolumns.append(newcolumn)
-
-            if self._pyxl_table is not None and i < len(self._pyxl_table.tableColumns):
-                prevcolumn: TableColumn = self._pyxl_table.tableColumns[i]
-                newcolumn.dataCellStyle = prevcolumn.dataCellStyle
-                newcolumn.dataDxfId = prevcolumn.dataDxfId # refers to workbook._differential_styles
-                newcolumn.calculatedColumnFormula = prevcolumn.calculatedColumnFormula
-
-
-        newtable_kwargs = {
-            'name': self.name,
-            'displayName': self.name,
-            'ref': new_ref,
-            'tableColumns': newcolumns,
-            'headerRowCount': 1 if self.has_headers else 0,
-        }
-
-        if self._pyxl_table is not None:
-            newtable_kwargs['autoFilter'] = self._pyxl_table.autoFilter
-            newtable_kwargs['sortState'] = self._pyxl_table.sortState
-            newtable_kwargs['tableStyleInfo'] = self._pyxl_table.tableStyleInfo
-        else:
-            newtable_kwargs['autoFilter'] = AutoFilter()
-            newtable_kwargs['tableStyleInfo'] = TableStyleInfo(name="TableStyleMedium2", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False)
-
-        newtable = Table(**newtable_kwargs)
-
-        self._pyxl_table = newtable
-        
-        if self.name in self._pyxl_worksheet.tables:
-            del self._pyxl_worksheet.tables[self.name]
-        self._pyxl_worksheet.add_table(newtable)
-
-
-    def get_value(self, row_index: int, col_index: int) -> Any:
-        self._check_indexes(row_index, col_index)
-        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
-        value = cell.value
-        # TODO: formatting
-        return value
-    
-
-    def set_value(self, row_index: int, col_index: int, value: Any):
-        """
-        Set the value of the cell located at the given 0-base indices, and apply the default formatting and formulas of the corresponding table column.
-
-        Any value (including `None`) overrides default column formulas. If you want to use the default column formula, use `erase_cell` method instead.
-        """
-        self._check_indexes(row_index, col_index)
-        self.workbook._needs_save = True
-        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
-
-        self._apply_column_format(cell)
-
-        try:
-            #logger.debug('set value to cell %s%s: %s', cell.column_letter, cell.row, value)
-            cell.value = value
-        except ValueError as err:
-            if str(err).startswith('Cannot convert'):
-                cell.value = str(value)
-            else:
-                raise
-        
-
-    def _check_indexes(self, row_index: int, col_index: int):
-        if row_index == -1:
-            row_index = self.row_count - 1
-        if col_index == -1:
-            col_index = self.col_count - 1
-
-        if row_index < 0 or row_index >= self.row_count:
-            raise ValueError(f"invalid row index: {row_index} (row count: {self.row_count})")
-        if col_index < 0 or col_index >= self.col_count:
-            raise ValueError(f"invalid row index: {col_index} (row count: {self.col_count})")
-
-
-    def erase_cell(self, row_index: int, col_index: int, allow_outside: bool = False):
-        """
-        Erase the value of the cell located at the given 0-base indices, and apply the default formatting and formulas of the corresponding table column.
-        
-        If `allow_outside` is set, the cell may be located outside of the table. In this case, no formatting or formula is applied.
-        """
-        if not allow_outside:
-            self._check_indexes(row_index, col_index)
-
-        self.workbook._needs_save = True
-        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
-        cell.style = 'Normal'
-        #logger.debug('erase cell %s%s', cell.column_letter, cell.row)
-        cell.value = None
-
-        if not allow_outside or (row_index < self._row_count and col_index < self.col_count):
-            self._apply_column_format(cell)
-    
-
-    def _apply_column_format(self, cell: Cell):
-        if self._column_formats is None:
-            self._column_formats = self._build_column_formats()
-        
-        index = (cell.col_idx - 1) - self._min_col_index
-        if index >= len(self._column_formats):
-            return
-        
-        fmt = self._column_formats[index]
-
-        if 'formula' in fmt:
-            formula = fmt['formula']
-            if isinstance(formula, ArrayFormula):
-                pass # TODO: not supported yet
-            else:
-                #logger.debug('apply formula to cell %s%s: %s', cell.column_letter, cell.row, formula)
-                cell.value = formula
-
-        if 'style' in fmt:
-            cell.style = fmt['style']
-
-        for fmt_key, fmt_value in fmt.items():
-            if fmt_key in ['formula', 'style']:
-                continue
-            setattr(cell, fmt_key, fmt_value)
-
-
-    def _build_column_formats(self) -> list[dict[str,Any]]:
-        if not self._pyxl_table:
-            return []
-                
-        column: TableColumn
-        fmt_list = []
-        for index, column in enumerate(self._pyxl_table.tableColumns):
-            fmt: dict[str,Any] = {}
-            fmt_list.append(fmt)
-
-            # Read dataCellStyle
-            if column.dataCellStyle:
-                fmt['style'] = column.dataCellStyle
-            
-            # Read dxf
-            if column.dataDxfId is not None:
-                dxf: DifferentialStyle = self.workbook._pyxl_workbook._differential_styles[column.dataDxfId]
-
-                if dxf.numFmt:
-                    fmt['number_format'] = dxf.numFmt.formatCode
-                else:
-                    if not 'style' in fmt:
-                        fmt['number_format'] = self._DEFAULT_NUMBER_FORMAT
-
-                fmt['alignment'] = dxf.alignment if dxf.alignment else self._DEFAULT_ALIGNMENT
-                fmt['border'] = dxf.border if dxf.border else self._DEFAULT_BORDER
-                fmt['font'] = dxf.font if dxf.font else self._DEFAULT_FONT
-                fmt['protection'] = dxf.protection if dxf.protection else self._DEFAULT_PROTECTION
-                fmt['fill'] = PatternFill(fill_type=dxf.fill.fill_type, bgColor=dxf.fill.fgColor, fgColor=dxf.fill.bgColor) if dxf.fill else self._DEFAULT_FILL # NOTE: fgcolor and bgcolor are inversed in DifferentialStyle
-
-            # Read formula
-            if column.calculatedColumnFormula:
-                formula = column.calculatedColumnFormula
-                if formula.array:
-                    fmt['formula'] = ArrayFormula(formula.attr_text)
-                else:
-                    fmt['formula'] = '=' + formula.attr_text
-
-                #logger.debug('column %s (%s) formula: %s', get_column_letter(index + 1), column.name, fmt['formula'])
-            
-        return fmt_list
-
-
-    _DEFAULT_NUMBER_FORMAT = 'General'
-    _DEFAULT_FILL = PatternFill(fill_type=None)
-    _DEFAULT_ALIGNMENT = None # openpyxl.styles.alignment.Alignment
-    _DEFAULT_BORDER = None # openpyxl.styles.alignment.Border
-    _DEFAULT_FONT = None # openpyxl.styles.fonts.Font
-    _DEFAULT_PROTECTION = None # openpyxl.styles.protection.Protection
-
-
-    def __iter__(self):
-        return self.Iterator(self)
-
-    class Iterator:
-        def __init__(self, table: ExcelTable):
-            self.next_index = 0
-            self.table = table
-
-        def __next__(self):
-            if self.next_index >= self.table.row_count:
-                raise StopIteration()
-            
-            row = self.table.get_row(self.next_index)
-            self.next_index += 1
-            return row
+from __future__ import annotations
+import logging
+from pathlib import Path
+from typing import Any
+from .tabular import Row
+from . import filesh
+from openpyxl import load_workbook, Workbook, DEFUSEDXML
+from openpyxl.worksheet.worksheet import Worksheet
+from openpyxl.worksheet.table import Table, TableColumn, TableFormula, TableStyleInfo
+from openpyxl.worksheet.formula import DataTableFormula, ArrayFormula
+from openpyxl.worksheet.filters import AutoFilter
+from openpyxl.cell.cell import Cell
+from openpyxl.styles.differential import DifferentialStyle, DifferentialStyleList
+from openpyxl.styles.fills import PatternFill
+from openpyxl.utils import range_boundaries, get_column_letter
+
+logger = logging.getLogger(__name__)
+
+
+class ExcelWorkbook:
+    _workbook_cache: dict[str,ExcelWorkbook] = {}
+    """ Workbooks per canonical path """
+    
+    _defusedxml_alert_emitted = False
+
+    def __init__(self, path: str|Path):        
+        if not DEFUSEDXML and not self.__class__._defusedxml_alert_emitted:
+            logger.warning("By default openpyxl does not guard against quadratic blowup or billion laughs xml attacks. To guard against these attacks install defusedxml.")
+            self.__class__._defusedxml_alert_emitted = True
+
+        self.path = Path(path) if not isinstance(path, Path) else path
+        
+        if filesh.exists(self.path):
+            logger.info("load excel workbook %s", self.path)
+            with filesh.open_file(self.path, 'rb') as fp:
+                self._pyxl_workbook: Workbook = load_workbook(fp)
+            self._create_next_table_in_active_sheet = False
+        else:
+            logger.info("create excel workbook for %s", self.path)
+            self._pyxl_workbook = Workbook()
+            self._create_next_table_in_active_sheet = True
+
+        self._tables: dict[str,ExcelTable] = {}
+        self._needs_save = False
+
+
+    @classmethod
+    def get_cached(cls, path: str|Path) -> ExcelWorkbook:
+        path = Path(path) if not isinstance(path, Path) else path
+        canonical_path = str(path.resolve())
+        return cls._workbook_cache[canonical_path]
+
+
+    @classmethod
+    def get_or_create_cached(cls, path: str|Path) -> ExcelWorkbook:
+        path = Path(path) if not isinstance(path, Path) else path
+        canonical_path = str(path.resolve())
+        if not canonical_path in cls._workbook_cache:
+            cls._workbook_cache[canonical_path] = ExcelWorkbook(path)
+        return cls._workbook_cache[canonical_path]
+
+
+    @classmethod
+    def close_all_cached(cls):
+        for workbook in cls._workbook_cache.values():
+            workbook.close()
+
+
+    def close(self):
+        if not self._needs_save:
+            return
+        
+        for table in self._tables.values():
+            table.redefine()
+        
+        logger.info("save excel workbook %s", self.path)
+        with filesh.open_file(self.path, 'wb') as fp:
+            self._pyxl_workbook.save(fp)
+
+        self._needs_save = False
+        
+
+    def get_table(self, name: str, default = '__raise__') -> ExcelTable:
+        if name in self._tables:
+            return self._tables[name]
+        
+        for sheet_name in self._pyxl_workbook.sheetnames:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
+            if name in pyxl_worksheet.tables:
+                pyxl_table = pyxl_worksheet.tables[name]
+                self._tables[name] = ExcelTable(pyxl_table, pyxl_worksheet, self)
+                return self._tables[name]
+            
+        if default == '__raise__':
+            raise KeyError(f"no table found with name \"{name}\" in workbook \"{self.path}\"")
+        else:
+            return default
+        
+
+    def create_table(self, name: str, no_headers: bool = False) -> ExcelTable:
+        for sheet_name in self._pyxl_workbook.sheetnames:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
+            if name in pyxl_worksheet.tables:
+                raise ValueError(f"table {name} already exist")
+    
+        self._needs_save = True
+        
+        if self._create_next_table_in_active_sheet:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook.active
+            pyxl_worksheet.title = name
+            self._create_next_table_in_active_sheet = False
+        else:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook.create_sheet(title=name)
+
+        self._tables[name] = ExcelTable(name, pyxl_worksheet, self)
+        return self._tables[name]
+
+
+class ExcelTable:
+    def __init__(self, pyxl_table: Table|str, pyxl_worksheet: Worksheet, workbook: ExcelWorkbook, no_headers: bool = None):
+        self.workbook = workbook
+        self._pyxl_worksheet = pyxl_worksheet        
+
+        if isinstance(pyxl_table, str):
+            self._pyxl_table: Table = None
+            self.name: str = pyxl_table
+            if no_headers:
+                self.has_headers = False
+                self._min_row_index = 0
+            else:
+                self.has_headers = True
+                self._min_row_index = 1
+            self._min_col_index = 0
+            self._row_count = 0
+            self._column_names: list[str] = []
+
+        elif isinstance(pyxl_table, Table):
+            self._pyxl_table: Table = pyxl_table
+            self.name: str = pyxl_table.name
+            
+            min_col, min_row, max_col, max_row = range_boundaries(pyxl_table.ref)
+
+            # NOTE: range_boundaries returns 1-base indices
+            self._min_col_index = min_col - 1
+            if pyxl_table.headerRowCount == 0:
+                self.has_headers = False
+                self._min_row_index = min_row - 1
+            elif pyxl_table.headerRowCount == 1:
+                self.has_headers = True
+                self._min_row_index = min_row
+            else:
+                raise ValueError(f'invalid headerRowCount: {pyxl_table.headerRowCount}')
+            
+            self._row_count = max_row - self._min_row_index
+            col_count = max_col - self._min_col_index
+        
+            self._column_names: list[str] = list(self._pyxl_table.column_names)
+            # NOTE: if no headers, default names are returned, example in French: ['Colonne1', 'Colonne2']
+            if len(self._column_names) != col_count:
+                raise ValueError(f'invalid column_names length ({len(self._column_names)}, expected {col_count}): {self._column_names}')
+        else:
+            raise ValueError(f"invalid type for pyxl_table: {type(pyxl_table).__name__}")
+
+        self._column_formats: list[dict[str,Any]] = None
+
+
+    @property
+    def column_names(self) -> list[str]:
+        return self._column_names
+
+    @property
+    def min_row_index(self) -> int:
+        """ 0-base index of the first data row. """
+        return self._min_row_index
+    
+    @property
+    def min_col_index(self) -> int:
+        """ 0-base index of the first column. """
+        return self._min_col_index
+    
+    @property
+    def row_count(self) -> int:
+        return self._row_count
+    
+    @property
+    def col_count(self) -> int:
+        return len(self._column_names)
+    
+    @property
+    def ref(self) -> str:
+        if self.col_count == 0:
+            raise ValueError(f"cannot get table ref: table does not contain any column")
+        if self.row_count == 0 and not self.has_headers:
+            raise ValueError(f"cannot get table ref: table does not contain any line")
+                
+        return f"{get_column_letter(self._min_col_index + 1)}{self._min_row_index - (1 if self.has_headers else 0) + 1}:{get_column_letter(self._min_col_index + self.col_count)}{self._min_row_index + self._row_count}"
+
+
+    def get_row(self, index: int) -> Row:
+        """
+        Get row at the given 0-base index.
+        """
+        if index == -1:
+            if self.row_count == 0:
+                raise ValueError(f"cannot get last row: table does not contain any row")
+            index = self.row_count - 1
+        elif index < 0:
+            raise ValueError(f"invalid row index: {index}")
+        
+        if index >= self.row_count:
+            raise ValueError(f"cannot get row at index {index}: table contains {self.row_count} rows")
+
+        return Row(
+            get=lambda col_index: self.get_value(index, col_index),
+            set=lambda col_index, value: self.set_value(index, col_index, value),
+            headers=self.column_names
+        )
+    
+
+    def insert_row(self) -> Row:
+        self.workbook._needs_save = True
+        row_index = self._row_count
+
+        # ensure empty values
+        for col_index in range(0, self.col_count):
+            cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
+            if cell.value is not None:
+                raise ValueError(f'cannot insert row: row at index {row_index} is not empty')
+
+        self._row_count += 1
+
+        # erase old styles and apply column format
+        for col_index in range(0, self.col_count):
+            self.erase_cell(row_index, col_index)
+            
+        return self.get_row(row_index)
+    
+
+    def insert_col(self, name: str):
+        self.workbook._needs_save = True
+        if not name:
+            raise ValueError(f'name cannot be empty')
+        if name in self.column_names:
+            raise ValueError(f'column name already used: {name}')
+            
+        col_index = self.col_count
+
+        # ensure empty values
+        for row_index in range(0, self._row_count):
+            cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
+            if cell.value is not None:
+                raise ValueError(f'cannot insert column: column {col_index} is not empty')
+
+        self.column_names.append(name) # implies self.col_count += 1
+
+        if self.has_headers:
+            cell = self._pyxl_worksheet.cell(self._min_row_index, self._min_col_index + 1 + col_index)
+            #logger.debug('set column name to cell %s%s: %s', cell.column_letter, cell.row, name)
+            cell.value = name
+
+        # erase old styles and apply column format
+        for row_index in range(0, self._row_count):
+            self.erase_cell(row_index, col_index)
+            
+
+    def truncate(self):
+        self.workbook._needs_save = True
+        prev_row_count = self._row_count
+        self._row_count = 0
+
+        for row_index in range(0, prev_row_count):
+            for col_index in range(0, self.col_count):
+                self.erase_cell(row_index, col_index, allow_outside=True)
+
+
+    def redefine(self):
+        self.workbook._needs_save = True
+        new_ref = self.ref
+        if self._pyxl_table is not None and new_ref == self._pyxl_table.ref:
+            return
+        
+        logger.debug("define table %s: %s => %s", self.name, self._pyxl_table.ref if self._pyxl_table is not None else None, new_ref)
+
+        newcolumns = []
+
+        for i in range(0, self.col_count):
+            if self.has_headers:
+                name = self._pyxl_worksheet.cell(self._min_row_index, self._min_col_index + 1 + i).value
+            else:
+                name = self.column_names[i] if i < len(self.column_names) else None
+            newcolumn = TableColumn(id=i+1, name=name)
+            newcolumns.append(newcolumn)
+
+            if self._pyxl_table is not None and i < len(self._pyxl_table.tableColumns):
+                prevcolumn: TableColumn = self._pyxl_table.tableColumns[i]
+                newcolumn.dataCellStyle = prevcolumn.dataCellStyle
+                newcolumn.dataDxfId = prevcolumn.dataDxfId # refers to workbook._differential_styles
+                newcolumn.calculatedColumnFormula = prevcolumn.calculatedColumnFormula
+
+
+        newtable_kwargs = {
+            'name': self.name,
+            'displayName': self.name,
+            'ref': new_ref,
+            'tableColumns': newcolumns,
+            'headerRowCount': 1 if self.has_headers else 0,
+        }
+
+        if self._pyxl_table is not None:
+            newtable_kwargs['autoFilter'] = self._pyxl_table.autoFilter
+            newtable_kwargs['sortState'] = self._pyxl_table.sortState
+            newtable_kwargs['tableStyleInfo'] = self._pyxl_table.tableStyleInfo
+        else:
+            newtable_kwargs['autoFilter'] = AutoFilter()
+            newtable_kwargs['tableStyleInfo'] = TableStyleInfo(name="TableStyleMedium2", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False)
+
+        newtable = Table(**newtable_kwargs)
+
+        self._pyxl_table = newtable
+        
+        if self.name in self._pyxl_worksheet.tables:
+            del self._pyxl_worksheet.tables[self.name]
+        self._pyxl_worksheet.add_table(newtable)
+
+
+    def get_value(self, row_index: int, col_index: int) -> Any:
+        self._check_indexes(row_index, col_index)
+        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
+        value = cell.value
+        # TODO: formatting
+        return value
+    
+
+    def set_value(self, row_index: int, col_index: int, value: Any):
+        """
+        Set the value of the cell located at the given 0-base indices, and apply the default formatting and formulas of the corresponding table column.
+
+        Any value (including `None`) overrides default column formulas. If you want to use the default column formula, use `erase_cell` method instead.
+        """
+        self._check_indexes(row_index, col_index)
+        self.workbook._needs_save = True
+        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
+
+        self._apply_column_format(cell)
+
+        try:
+            #logger.debug('set value to cell %s%s: %s', cell.column_letter, cell.row, value)
+            cell.value = value
+        except ValueError as err:
+            if str(err).startswith('Cannot convert'):
+                cell.value = str(value)
+            else:
+                raise
+        
+
+    def _check_indexes(self, row_index: int, col_index: int):
+        if row_index == -1:
+            row_index = self.row_count - 1
+        if col_index == -1:
+            col_index = self.col_count - 1
+
+        if row_index < 0 or row_index >= self.row_count:
+            raise ValueError(f"invalid row index: {row_index} (row count: {self.row_count})")
+        if col_index < 0 or col_index >= self.col_count:
+            raise ValueError(f"invalid row index: {col_index} (row count: {self.col_count})")
+
+
+    def erase_cell(self, row_index: int, col_index: int, allow_outside: bool = False):
+        """
+        Erase the value of the cell located at the given 0-base indices, and apply the default formatting and formulas of the corresponding table column.
+        
+        If `allow_outside` is set, the cell may be located outside of the table. In this case, no formatting or formula is applied.
+        """
+        if not allow_outside:
+            self._check_indexes(row_index, col_index)
+
+        self.workbook._needs_save = True
+        cell = self._pyxl_worksheet.cell(self._min_row_index + 1 + row_index, self._min_col_index + 1 + col_index)
+        cell.style = 'Normal'
+        #logger.debug('erase cell %s%s', cell.column_letter, cell.row)
+        cell.value = None
+
+        if not allow_outside or (row_index < self._row_count and col_index < self.col_count):
+            self._apply_column_format(cell)
+    
+
+    def _apply_column_format(self, cell: Cell):
+        if self._column_formats is None:
+            self._column_formats = self._build_column_formats()
+        
+        index = (cell.col_idx - 1) - self._min_col_index
+        if index >= len(self._column_formats):
+            return
+        
+        fmt = self._column_formats[index]
+
+        if 'formula' in fmt:
+            formula = fmt['formula']
+            if isinstance(formula, ArrayFormula):
+                pass # TODO: not supported yet
+            else:
+                #logger.debug('apply formula to cell %s%s: %s', cell.column_letter, cell.row, formula)
+                cell.value = formula
+
+        if 'style' in fmt:
+            cell.style = fmt['style']
+
+        for fmt_key, fmt_value in fmt.items():
+            if fmt_key in ['formula', 'style']:
+                continue
+            setattr(cell, fmt_key, fmt_value)
+
+
+    def _build_column_formats(self) -> list[dict[str,Any]]:
+        if not self._pyxl_table:
+            return []
+                
+        column: TableColumn
+        fmt_list = []
+        for index, column in enumerate(self._pyxl_table.tableColumns):
+            fmt: dict[str,Any] = {}
+            fmt_list.append(fmt)
+
+            # Read dataCellStyle
+            if column.dataCellStyle:
+                fmt['style'] = column.dataCellStyle
+            
+            # Read dxf
+            if column.dataDxfId is not None:
+                dxf: DifferentialStyle = self.workbook._pyxl_workbook._differential_styles[column.dataDxfId]
+
+                if dxf.numFmt:
+                    fmt['number_format'] = dxf.numFmt.formatCode
+                else:
+                    if not 'style' in fmt:
+                        fmt['number_format'] = self._DEFAULT_NUMBER_FORMAT
+
+                fmt['alignment'] = dxf.alignment if dxf.alignment else self._DEFAULT_ALIGNMENT
+                fmt['border'] = dxf.border if dxf.border else self._DEFAULT_BORDER
+                fmt['font'] = dxf.font if dxf.font else self._DEFAULT_FONT
+                fmt['protection'] = dxf.protection if dxf.protection else self._DEFAULT_PROTECTION
+                fmt['fill'] = PatternFill(fill_type=dxf.fill.fill_type, bgColor=dxf.fill.fgColor, fgColor=dxf.fill.bgColor) if dxf.fill else self._DEFAULT_FILL # NOTE: fgcolor and bgcolor are inversed in DifferentialStyle
+
+            # Read formula
+            if column.calculatedColumnFormula:
+                formula = column.calculatedColumnFormula
+                if formula.array:
+                    fmt['formula'] = ArrayFormula(formula.attr_text)
+                else:
+                    fmt['formula'] = '=' + formula.attr_text
+
+                #logger.debug('column %s (%s) formula: %s', get_column_letter(index + 1), column.name, fmt['formula'])
+            
+        return fmt_list
+
+
+    _DEFAULT_NUMBER_FORMAT = 'General'
+    _DEFAULT_FILL = PatternFill(fill_type=None)
+    _DEFAULT_ALIGNMENT = None # openpyxl.styles.alignment.Alignment
+    _DEFAULT_BORDER = None # openpyxl.styles.alignment.Border
+    _DEFAULT_FONT = None # openpyxl.styles.fonts.Font
+    _DEFAULT_PROTECTION = None # openpyxl.styles.protection.Protection
+
+
+    def __iter__(self):
+        return self.Iterator(self)
+
+    class Iterator:
+        def __init__(self, table: ExcelTable):
+            self.next_index = 0
+            self.table = table
+
+        def __next__(self):
+            if self.next_index >= self.table.row_count:
+                raise StopIteration()
+            
+            row = self.table.get_row(self.next_index)
+            self.next_index += 1
+            return row
```

## zut/git.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from __future__ import annotations
-import subprocess, re, logging
-from .colors import Colors
-
-logger = logging.getLogger(__name__)
-
-
-def get_git_tags(points_at: str = None, pattern: str = None) -> list[str]:
-    """
-    Get list of defined tags.
-    Use points_at="HEAD" for tags pointing on last commit.
-    """
-    cmd = ['git', 'tag', '--list']
-    if points_at:
-        cmd.append("--points-at")
-        cmd.append(points_at)
-    if pattern:
-        cmd.append(pattern)
-
-    cp = subprocess.run(cmd, check=True, text=True, capture_output=True)
-    tags = cp.stdout.strip()
-    if not tags:
-        return []
-    
-    return tags.splitlines()
-
-
-def get_git_hash(ref: str = "HEAD") -> None|str:
-    """
-    Get commit hash of a reference (branch, tag, etc).
-    Use ref="HEAD" for last commit and ref=None for last commit only if there is no changes.
-    """
-    try:
-        cp = subprocess.run(['git', 'rev-list', '-n', '1', ref], check=True, text=True, capture_output=True)
-    except subprocess.CalledProcessError:
-        if cp.returncode == 128:
-            return None
-
-    return cp.stdout.strip()
-
-
-def git_has_changes():
-    """
-    Indicate whether working directory has changes since last commit.
-    """
-    cp = subprocess.run(['git', 'status', '--porcelain'], check=True, text=True, capture_output=True)
-    return cp.stdout != ""
-
-
-def check_git_version_tag(version: str) -> bool:
-    if not isinstance(version, str):
-        raise ValueError(f"invalid type for argument \"version\": {type(version)}")
-    
-    ok = True
-
-    # Check version
-    if not re.match(r"^\d+\.\d+\.\d+(?:\-[a-z0-9\-]+)?$", version):
-        logger.error(f"version \"{version}\" does not match required regex")
-        ok = False
-
-    # Compare version with git tags
-    tags = get_git_tags(pattern="v*")
-    if not tags or not f"v{version}" in tags:
-        logger.warning(f"tag v{version} not found")
-        ok = False
-    
-    else:
-        # Ensure corresponding version tag matches current hash
-        tag_hash = get_git_hash(f"v{version}")
-        
-        if tag_hash:
-            head_hash = get_git_hash()
-            
-            if tag_hash != head_hash:
-                logger.error(f"tag v{version} points at {Colors.BLUE}{tag_hash}{Colors.RESET}, head points at {Colors.BLUE}{head_hash}{Colors.RESET}")
-                ok = False
-            
-            elif git_has_changes():
-                logger.error(f"git has changes since HEAD (tag v{version})")
-                ok = False
-
-    return ok
+from __future__ import annotations
+import subprocess, re, logging
+from .colors import Colors
+
+logger = logging.getLogger(__name__)
+
+
+def get_git_tags(points_at: str = None, pattern: str = None) -> list[str]:
+    """
+    Get list of defined tags.
+    Use points_at="HEAD" for tags pointing on last commit.
+    """
+    cmd = ['git', 'tag', '--list']
+    if points_at:
+        cmd.append("--points-at")
+        cmd.append(points_at)
+    if pattern:
+        cmd.append(pattern)
+
+    cp = subprocess.run(cmd, check=True, text=True, capture_output=True)
+    tags = cp.stdout.strip()
+    if not tags:
+        return []
+    
+    return tags.splitlines()
+
+
+def get_git_hash(ref: str = "HEAD") -> None|str:
+    """
+    Get commit hash of a reference (branch, tag, etc).
+    Use ref="HEAD" for last commit and ref=None for last commit only if there is no changes.
+    """
+    try:
+        cp = subprocess.run(['git', 'rev-list', '-n', '1', ref], check=True, text=True, capture_output=True)
+    except subprocess.CalledProcessError:
+        if cp.returncode == 128:
+            return None
+
+    return cp.stdout.strip()
+
+
+def git_has_changes():
+    """
+    Indicate whether working directory has changes since last commit.
+    """
+    cp = subprocess.run(['git', 'status', '--porcelain'], check=True, text=True, capture_output=True)
+    return cp.stdout != ""
+
+
+def check_git_version_tag(version: str) -> bool:
+    if not isinstance(version, str):
+        raise ValueError(f"invalid type for argument \"version\": {type(version)}")
+    
+    ok = True
+
+    # Check version
+    if not re.match(r"^\d+\.\d+\.\d+(?:\-[a-z0-9\-]+)?$", version):
+        logger.error(f"version \"{version}\" does not match required regex")
+        ok = False
+
+    # Compare version with git tags
+    tags = get_git_tags(pattern="v*")
+    if not tags or not f"v{version}" in tags:
+        logger.warning(f"tag v{version} not found")
+        ok = False
+    
+    else:
+        # Ensure corresponding version tag matches current hash
+        tag_hash = get_git_hash(f"v{version}")
+        
+        if tag_hash:
+            head_hash = get_git_hash()
+            
+            if tag_hash != head_hash:
+                logger.error(f"tag v{version} points at {Colors.BLUE}{tag_hash}{Colors.RESET}, head points at {Colors.BLUE}{head_hash}{Colors.RESET}")
+                ok = False
+            
+            elif git_has_changes():
+                logger.error(f"git has changes since HEAD (tag v{version})")
+                ok = False
+
+    return ok
```

## zut/gpg.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-from __future__ import annotations
-import subprocess, logging
-from tempfile import TemporaryDirectory
-from pathlib import Path
-from .process import check_completed_subprocess
-from .network import get_configured_proxy_url
-
-logger = logging.getLogger(__name__)
-
-
-def download_gpg_key(keyid: str, target_path: Path, keyserver: str = None, include_proxy_password: bool = False):
-    with TemporaryDirectory() as tmpdir:
-        # Retrieve the key
-        cmd = ["gpg", "--homedir", tmpdir]
-        
-        if keyserver:
-            cmd += ["--keyserver", keyserver]
-            
-            if keyserver.startswith("hkp://"):
-                proxy_url = get_configured_proxy_url(keyserver, include_password=include_proxy_password)
-                if proxy_url:
-                    cmd += ["--keyserver-options", f"http-proxy={proxy_url}"]
-
-        cmd += ["--recv-keys", keyid]
-        subprocess.run(cmd, capture_output=True, text=True, check=True)
-
-        # Export the key
-        cmd = ["gpg", "--homedir", tmpdir, "--output", target_path, "--export", keyid]
-        subprocess.run(cmd, capture_output=True, text=True, check=True)
-
-
-def verify_gpg_signature(sign_path: Path, public_key_path: Path):
-    cmd = ["gpg", "--no-default-keyring", "--keyring", public_key_path, "--verify", sign_path]
-    cp = subprocess.run(cmd, capture_output=True, text=True)
-    
-    if cp.returncode != 0:
-        check_completed_subprocess(cp, logger, label='gpg verify')
-        return False
-    
-    return True
+from __future__ import annotations
+import subprocess, logging
+from tempfile import TemporaryDirectory
+from pathlib import Path
+from .process import check_completed_subprocess
+from .network import get_configured_proxy_url
+
+logger = logging.getLogger(__name__)
+
+
+def download_gpg_key(keyid: str, target_path: Path, keyserver: str = None, include_proxy_password: bool = False):
+    with TemporaryDirectory() as tmpdir:
+        # Retrieve the key
+        cmd = ["gpg", "--homedir", tmpdir]
+        
+        if keyserver:
+            cmd += ["--keyserver", keyserver]
+            
+            if keyserver.startswith("hkp://"):
+                proxy_url = get_configured_proxy_url(keyserver, include_password=include_proxy_password)
+                if proxy_url:
+                    cmd += ["--keyserver-options", f"http-proxy={proxy_url}"]
+
+        cmd += ["--recv-keys", keyid]
+        subprocess.run(cmd, capture_output=True, text=True, check=True)
+
+        # Export the key
+        cmd = ["gpg", "--homedir", tmpdir, "--output", target_path, "--export", keyid]
+        subprocess.run(cmd, capture_output=True, text=True, check=True)
+
+
+def verify_gpg_signature(sign_path: Path, public_key_path: Path):
+    cmd = ["gpg", "--no-default-keyring", "--keyring", public_key_path, "--verify", sign_path]
+    cp = subprocess.run(cmd, capture_output=True, text=True)
+    
+    if cp.returncode != 0:
+        check_completed_subprocess(cp, logger, label='gpg verify')
+        return False
+    
+    return True
```

## zut/json.py

 * *Ordering differences only*

```diff
@@ -1,173 +1,173 @@
-from __future__ import annotations
-import json
-import re
-from datetime import datetime, time, date, timedelta
-from decimal import Decimal
-from uuid import UUID
-from .datetime import is_aware, duration_iso_string
-
-try:
-    from django.utils.functional import Promise
-except ImportError:
-    Promise = None
-
-
-class ExtendedJSONEncoder(json.JSONEncoder):
-    """
-    Adapted from: django.core.serializers.json.DjangoJSONEncoder
-    Usage example: json.dumps(data, indent=2, cls=ExtendedJSONEncoder)
-    """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-
-    def default(self, o):
-        if isinstance(o, datetime):
-            r = o.isoformat()
-            if o.microsecond and o.microsecond % 1000 == 0:
-                r = r[:23] + r[26:]
-            if r.endswith("+00:00"):
-                r = r[:-6] + "Z"
-            return r
-        elif isinstance(o, date):
-            return o.isoformat()
-        elif isinstance(o, time):
-            if is_aware(o):
-                raise ValueError("JSON can't represent timezone-aware times.")
-            r = o.isoformat()
-            if o.microsecond and o.microsecond % 1000 == 0:
-                r = r[:12]
-            return f'T{r}'
-        elif isinstance(o, timedelta):
-            return duration_iso_string(o)
-        elif isinstance(o, (Decimal, UUID)):
-            return str(o)
-        elif Promise is not None and isinstance(o, Promise):
-            return str(o)
-        else:
-            return super().default(o)
-
-
-class ExtendedJSONDecoder(json.JSONDecoder):
-    """
-    Reverse of: ExtendedJSONEncoder.
-    Usage example: json.loads(data, cls=ExtendedJSONDecoder)
-    """
-    def __init__(self, **kwargs):
-        if not 'object_hook' in kwargs:
-            kwargs['object_hook'] = extended_json_decode_hook
-        super().__init__(**kwargs)
-
-
-def extended_json_decode_hook(obj):
-    """
-    Decode date and datetime objects with ISO format.
-    """
-    if isinstance(obj, dict):
-        for key, value in obj.items():
-            obj[key] = extended_json_decode_hook(value)
-
-    elif isinstance(obj, list):
-        for i, value in enumerate(obj):
-            obj[i] = extended_json_decode_hook(value)
-
-    elif isinstance(obj, str):
-        if len(obj) < 10:
-            return obj # shortcut
-        
-        value = _decode_date(obj)
-        if value:
-            return value
-        
-        value = _decode_datetime(obj)
-        if value:
-            return value
-        
-        value = _decode_timedelta(obj)
-        if value:
-            return value
-        
-    return obj
-
-
-def _decode_datetime(value: str) -> datetime|time|None:
-    m = re.match(r'^(\d{4}-\d{2}-\d{2})?(T\d{2}:\d{2}:\d{2})(\.\d{3,6})?(Z|[\+\-]\d{2}:\d{2})?$', value)
-    if not m:
-        return None
-    
-    datepart = m.group(1) or '' # optional
-    timepart = m.group(2) # mandatory
-    microsecondpart = m.group(3) or '' # optional
-    tz = m.group(4) or '' # optional
-
-    format_string = ''
-
-    if datepart:
-        format_string += '%Y-%m-%d'
-    
-    format_string += 'T%H:%M:%S'
-    
-    if microsecondpart:
-        format_string += '.%f'
-    
-    if tz:
-        if not datepart:
-            # invalid (timezone-aware time without date)
-            return None
-        
-        format_string += '%z'
-        # adapt timezone: replace 'Z' with +0000, or +XX:YY with +XXYY
-        if tz == 'Z':
-            tz = '+0000'
-        else:
-            tz = tz[:-3] + tz[-2:]
-    
-    try:
-        result = datetime.strptime(f"{datepart}{timepart}{microsecondpart}{tz}", format_string)
-    except ValueError: # example: invalid month, day, hour, etc
-        return None
-
-    if not datepart:
-        result = result.time()
-    return result
-
-
-def _decode_timedelta(value: str) -> timedelta|None:
-    m = re.match(r'^(\-)?P(\d+)DT(\d{2})H(\d{2})M(\d{2})(?:\.(\d{3,6}))?S$', value)
-    if not m:
-        return None
-    
-    sign = m.group(1) or ''
-    days = int(m.group(2))
-    hours = int(m.group(3))
-    minutes = int(m.group(4))
-    seconds = int(m.group(5))
-
-    if m.group(6):
-        microseconds = int(m.group(6).ljust(6, '0'))
-    else:
-        microseconds = 0
-
-    if sign == '-':
-        days *= -1
-        hours *= -1
-        minutes *= -1
-        seconds *= -1
-        microseconds *= -1
-
-    try:
-        return timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds, microseconds=microseconds)
-    except ValueError: # example: invalid month, day, hour, etc
-        return None
-
-
-def _decode_date(value: str) -> date|None:
-    m = re.match(r'^\d{4}-\d{2}-\d{2}$', value)
-    if not m:
-        return None
-
-    try:
-        result = datetime.strptime(value, "%Y-%m-%d")
-    except ValueError: # example: invalid month or day
-        return None
-    
-    return result.date()
+from __future__ import annotations
+import json
+import re
+from datetime import datetime, time, date, timedelta
+from decimal import Decimal
+from uuid import UUID
+from .datetime import is_aware, duration_iso_string
+
+try:
+    from django.utils.functional import Promise
+except ImportError:
+    Promise = None
+
+
+class ExtendedJSONEncoder(json.JSONEncoder):
+    """
+    Adapted from: django.core.serializers.json.DjangoJSONEncoder
+    Usage example: json.dumps(data, indent=2, cls=ExtendedJSONEncoder)
+    """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def default(self, o):
+        if isinstance(o, datetime):
+            r = o.isoformat()
+            if o.microsecond and o.microsecond % 1000 == 0:
+                r = r[:23] + r[26:]
+            if r.endswith("+00:00"):
+                r = r[:-6] + "Z"
+            return r
+        elif isinstance(o, date):
+            return o.isoformat()
+        elif isinstance(o, time):
+            if is_aware(o):
+                raise ValueError("JSON can't represent timezone-aware times.")
+            r = o.isoformat()
+            if o.microsecond and o.microsecond % 1000 == 0:
+                r = r[:12]
+            return f'T{r}'
+        elif isinstance(o, timedelta):
+            return duration_iso_string(o)
+        elif isinstance(o, (Decimal, UUID)):
+            return str(o)
+        elif Promise is not None and isinstance(o, Promise):
+            return str(o)
+        else:
+            return super().default(o)
+
+
+class ExtendedJSONDecoder(json.JSONDecoder):
+    """
+    Reverse of: ExtendedJSONEncoder.
+    Usage example: json.loads(data, cls=ExtendedJSONDecoder)
+    """
+    def __init__(self, **kwargs):
+        if not 'object_hook' in kwargs:
+            kwargs['object_hook'] = extended_json_decode_hook
+        super().__init__(**kwargs)
+
+
+def extended_json_decode_hook(obj):
+    """
+    Decode date and datetime objects with ISO format.
+    """
+    if isinstance(obj, dict):
+        for key, value in obj.items():
+            obj[key] = extended_json_decode_hook(value)
+
+    elif isinstance(obj, list):
+        for i, value in enumerate(obj):
+            obj[i] = extended_json_decode_hook(value)
+
+    elif isinstance(obj, str):
+        if len(obj) < 10:
+            return obj # shortcut
+        
+        value = _decode_date(obj)
+        if value:
+            return value
+        
+        value = _decode_datetime(obj)
+        if value:
+            return value
+        
+        value = _decode_timedelta(obj)
+        if value:
+            return value
+        
+    return obj
+
+
+def _decode_datetime(value: str) -> datetime|time|None:
+    m = re.match(r'^(\d{4}-\d{2}-\d{2})?(T\d{2}:\d{2}:\d{2})(\.\d{3,6})?(Z|[\+\-]\d{2}:\d{2})?$', value)
+    if not m:
+        return None
+    
+    datepart = m.group(1) or '' # optional
+    timepart = m.group(2) # mandatory
+    microsecondpart = m.group(3) or '' # optional
+    tz = m.group(4) or '' # optional
+
+    format_string = ''
+
+    if datepart:
+        format_string += '%Y-%m-%d'
+    
+    format_string += 'T%H:%M:%S'
+    
+    if microsecondpart:
+        format_string += '.%f'
+    
+    if tz:
+        if not datepart:
+            # invalid (timezone-aware time without date)
+            return None
+        
+        format_string += '%z'
+        # adapt timezone: replace 'Z' with +0000, or +XX:YY with +XXYY
+        if tz == 'Z':
+            tz = '+0000'
+        else:
+            tz = tz[:-3] + tz[-2:]
+    
+    try:
+        result = datetime.strptime(f"{datepart}{timepart}{microsecondpart}{tz}", format_string)
+    except ValueError: # example: invalid month, day, hour, etc
+        return None
+
+    if not datepart:
+        result = result.time()
+    return result
+
+
+def _decode_timedelta(value: str) -> timedelta|None:
+    m = re.match(r'^(\-)?P(\d+)DT(\d{2})H(\d{2})M(\d{2})(?:\.(\d{3,6}))?S$', value)
+    if not m:
+        return None
+    
+    sign = m.group(1) or ''
+    days = int(m.group(2))
+    hours = int(m.group(3))
+    minutes = int(m.group(4))
+    seconds = int(m.group(5))
+
+    if m.group(6):
+        microseconds = int(m.group(6).ljust(6, '0'))
+    else:
+        microseconds = 0
+
+    if sign == '-':
+        days *= -1
+        hours *= -1
+        minutes *= -1
+        seconds *= -1
+        microseconds *= -1
+
+    try:
+        return timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds, microseconds=microseconds)
+    except ValueError: # example: invalid month, day, hour, etc
+        return None
+
+
+def _decode_date(value: str) -> date|None:
+    m = re.match(r'^\d{4}-\d{2}-\d{2}$', value)
+    if not m:
+        return None
+
+    try:
+        result = datetime.strptime(value, "%Y-%m-%d")
+    except ValueError: # example: invalid month or day
+        return None
+    
+    return result.date()
```

## zut/lang.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-from __future__ import annotations
-import sys
-from pathlib import Path
-
-ZUT_ROOT = Path(__file__).parent
-""" Root directory of `zut` library """
-
-
-def is_list_or_tuple_of(instance, element_type: type|tuple[type]):
-    if not isinstance(instance, (list,tuple)):
-        return False
-
-    for element in instance:
-        if not isinstance(element, element_type):
-            return False
-        
-    return True
+from __future__ import annotations
+import sys
+from pathlib import Path
+
+ZUT_ROOT = Path(__file__).parent
+""" Root directory of `zut` library """
+
+
+def is_list_or_tuple_of(instance, element_type: type|tuple[type]):
+    if not isinstance(instance, (list,tuple)):
+        return False
+
+    for element in instance:
+        if not isinstance(element, element_type):
+            return False
+        
+    return True
```

## zut/logging.py

 * *Ordering differences only*

```diff
@@ -1,130 +1,130 @@
-from __future__ import annotations
-import logging, logging.config, atexit, os
-from .colors import Colors
-
-
-def configure_logging(level: int|str = None, nocount: bool = False, config: dict = None):
-    merged = dict(DEFAULT_LOGGING_DICTCONFIG)
-
-    # Merge given config with default config
-    if config:
-        for prop, data in config.items():
-            if isinstance(data, dict) and prop in merged and isinstance(merged[prop], dict):
-                for key, value in data.items():
-                    if value is None:
-                        merged[prop].pop(key, None)
-                    else:
-                        merged[prop][key] = value
-            else:
-                merged[prop] = data
-
-    # Set root level if missing or if explicitely provided     
-    if level is not None or ('root' in merged and not 'level' in merged['root']):
-        if level is None:
-            level = os.environ.get('LOG_LEVEL', '').upper() or 'INFO'
-        
-        if isinstance(level, int):
-            level = logging.getLevelName(level)
-
-        merged['root']['level'] = level
-
-    # Remove count handler
-    if nocount:
-        if 'root' in merged and 'handlers' in merged['root']:
-            merged['root']['handlers'].pop('count', None)
-
-        if 'loggers' in merged:
-            for _, loggerconfig in merged['loggers'].items():
-                if 'handlers' in loggerconfig:
-                    loggerconfig['handlers'].pop('count', None)
-    
-    logging.config.dictConfig(merged)
-
-
-class ColoredRecord:
-    LEVELCOLORS = {
-        logging.DEBUG:     Colors.GRAY,
-        logging.INFO:      '',
-        logging.WARNING:   Colors.YELLOW,
-        logging.ERROR:     Colors.RED,
-        logging.CRITICAL:  Colors.BOLD_RED,
-    }
-
-    def __init__(self, record: logging.LogRecord):
-        # The internal dict is used by Python logging library when formatting the message.
-        # (inspired from library "colorlog").
-        self.__dict__.update(record.__dict__)
-        self.__dict__.update({
-            'levelcolor': self.LEVELCOLORS.get(record.levelno, ''),
-            'red': Colors.RED,
-            'green': Colors.GREEN,
-            'yellow': Colors.YELLOW,
-            'cyan': Colors.CYAN,
-            'gray': Colors.GRAY,
-            'bold_red': Colors.BOLD_RED,
-            'reset': Colors.RESET,
-        })
-
-
-class ColoredFormatter(logging.Formatter):
-    def formatMessage(self, record: logging.LogRecord) -> str:
-        """Format a message from a record object."""
-        wrapper = ColoredRecord(record)
-        message = super().formatMessage(wrapper)
-        return message
-
-
-class CountHandler(logging.Handler):
-    def __init__(self, level=logging.WARNING):
-        self.counts: dict[int, int] = {}
-        atexit.register(self.print_counts)
-        super().__init__(level=level)
-
-    def print_counts(self):
-        msg = ""
-
-        levelnos = sorted(self.counts.keys(), reverse=True)
-        for levelno in levelnos:
-            levelname = logging.getLevelName(levelno)
-            levelcolor = ColoredRecord.LEVELCOLORS.get(levelno, '')
-            msg += (", " if msg else "") + f"{levelcolor}%s{Colors.RESET}" % levelname + ": %d" % self.counts[levelno]
-
-        if msg:
-            print("Logged " + msg)
-
-    def emit(self, record: logging.LogRecord):
-        if record.levelno >= self.level:
-            if not record.levelno in self.counts:
-                self.counts[record.levelno] = 1
-            else:
-                self.counts[record.levelno] += 1
-
-
-DEFAULT_LOGGING_DICTCONFIG = {
-    'version': 1,
-    'disable_existing_loggers': False,
-    'formatters': {
-        'color': {
-            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
-            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(levelcolor)s%(message)s%(reset)s',
-        },
-    },
-    'handlers': {
-        'console': {
-            'class': 'logging.StreamHandler',
-            'formatter': 'color',
-        },
-        'count': {
-            'class': CountHandler.__module__ + '.' + CountHandler.__qualname__,
-            'level': 'WARNING',
-        },
-    },
-    'root': {
-        'handlers': ['console', 'count'],
-        'level': os.environ.get('LOG_LEVEL', '').upper() or 'INFO',
-    },
-    'loggers': {
-        'django': { 'level': os.environ.get('DJANGO_LOG_LEVEL', '').upper() or 'INFO', 'propagate': False },
-        'smbprotocol': { 'level': 'WARNING' },
-    },
-}
+from __future__ import annotations
+import logging, logging.config, atexit, os
+from .colors import Colors
+
+
+def configure_logging(level: int|str = None, nocount: bool = False, config: dict = None):
+    merged = dict(DEFAULT_LOGGING_DICTCONFIG)
+
+    # Merge given config with default config
+    if config:
+        for prop, data in config.items():
+            if isinstance(data, dict) and prop in merged and isinstance(merged[prop], dict):
+                for key, value in data.items():
+                    if value is None:
+                        merged[prop].pop(key, None)
+                    else:
+                        merged[prop][key] = value
+            else:
+                merged[prop] = data
+
+    # Set root level if missing or if explicitely provided     
+    if level is not None or ('root' in merged and not 'level' in merged['root']):
+        if level is None:
+            level = os.environ.get('LOG_LEVEL', '').upper() or 'INFO'
+        
+        if isinstance(level, int):
+            level = logging.getLevelName(level)
+
+        merged['root']['level'] = level
+
+    # Remove count handler
+    if nocount:
+        if 'root' in merged and 'handlers' in merged['root']:
+            merged['root']['handlers'].pop('count', None)
+
+        if 'loggers' in merged:
+            for _, loggerconfig in merged['loggers'].items():
+                if 'handlers' in loggerconfig:
+                    loggerconfig['handlers'].pop('count', None)
+    
+    logging.config.dictConfig(merged)
+
+
+class ColoredRecord:
+    LEVELCOLORS = {
+        logging.DEBUG:     Colors.GRAY,
+        logging.INFO:      '',
+        logging.WARNING:   Colors.YELLOW,
+        logging.ERROR:     Colors.RED,
+        logging.CRITICAL:  Colors.BOLD_RED,
+    }
+
+    def __init__(self, record: logging.LogRecord):
+        # The internal dict is used by Python logging library when formatting the message.
+        # (inspired from library "colorlog").
+        self.__dict__.update(record.__dict__)
+        self.__dict__.update({
+            'levelcolor': self.LEVELCOLORS.get(record.levelno, ''),
+            'red': Colors.RED,
+            'green': Colors.GREEN,
+            'yellow': Colors.YELLOW,
+            'cyan': Colors.CYAN,
+            'gray': Colors.GRAY,
+            'bold_red': Colors.BOLD_RED,
+            'reset': Colors.RESET,
+        })
+
+
+class ColoredFormatter(logging.Formatter):
+    def formatMessage(self, record: logging.LogRecord) -> str:
+        """Format a message from a record object."""
+        wrapper = ColoredRecord(record)
+        message = super().formatMessage(wrapper)
+        return message
+
+
+class CountHandler(logging.Handler):
+    def __init__(self, level=logging.WARNING):
+        self.counts: dict[int, int] = {}
+        atexit.register(self.print_counts)
+        super().__init__(level=level)
+
+    def print_counts(self):
+        msg = ""
+
+        levelnos = sorted(self.counts.keys(), reverse=True)
+        for levelno in levelnos:
+            levelname = logging.getLevelName(levelno)
+            levelcolor = ColoredRecord.LEVELCOLORS.get(levelno, '')
+            msg += (", " if msg else "") + f"{levelcolor}%s{Colors.RESET}" % levelname + ": %d" % self.counts[levelno]
+
+        if msg:
+            print("Logged " + msg)
+
+    def emit(self, record: logging.LogRecord):
+        if record.levelno >= self.level:
+            if not record.levelno in self.counts:
+                self.counts[record.levelno] = 1
+            else:
+                self.counts[record.levelno] += 1
+
+
+DEFAULT_LOGGING_DICTCONFIG = {
+    'version': 1,
+    'disable_existing_loggers': False,
+    'formatters': {
+        'color': {
+            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
+            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(levelcolor)s%(message)s%(reset)s',
+        },
+    },
+    'handlers': {
+        'console': {
+            'class': 'logging.StreamHandler',
+            'formatter': 'color',
+        },
+        'count': {
+            'class': CountHandler.__module__ + '.' + CountHandler.__qualname__,
+            'level': 'WARNING',
+        },
+    },
+    'root': {
+        'handlers': ['console', 'count'],
+        'level': os.environ.get('LOG_LEVEL', '').upper() or 'INFO',
+    },
+    'loggers': {
+        'django': { 'level': os.environ.get('DJANGO_LOG_LEVEL', '').upper() or 'INFO', 'propagate': False },
+        'smbprotocol': { 'level': 'WARNING' },
+    },
+}
```

## zut/numeric.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-from __future__ import annotations
-from decimal import Decimal, InvalidOperation
-from locale import getlocale
-from .text import ValueString
-
-
-def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: str = None):
-    """
-    Get a human-readable representation of a number of bytes.
-
-    `max_multiple` may be `K`, `M`, `G'` or `T'. 
-    """
-    return human_number(value, unit=unit, divider=divider, decimals=decimals, decimal_separator=decimal_separator, thousands_separator=thousands_separator, max_multiple=max_multiple)
-
-
-def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: str = None):
-    """
-    Get a human-readable representation of a number.
-
-    `max_multiple` may be `K`, `M`, `G'` or `T'. 
-    """
-    if value is None:
-        return ValueString('', None)
-
-    suffixes = []
-
-    # Append non-multiple suffix (bytes)
-    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
-    if unit:
-        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
-    else:
-        suffixes.append('')
-
-    # Append multiple suffixes
-    for multiple in ['K', 'M', 'G', 'T']:
-        suffixes.append(f' {multiple}{unit}')
-        if max_multiple and max_multiple.upper() == multiple:
-            break
-
-    i = 0
-    suffix = suffixes[i]
-    divided_value = value
-
-    while divided_value > 1000 and i < len(suffixes) - 1:
-        divided_value /= divider
-        i += 1
-        suffix = suffixes[i]
-
-    # Format value
-    if i == 0:
-        formatted_value = '{value:,.0f}'.format(value=divided_value)
-    else:
-        formatted_value = ('{value:,.'+str(decimals)+'f}').format(value=divided_value)
-
-    #  Replace separators
-    if decimal_separator is not None or thousands_separator is not None:
-        chars = formatted_value
-        formatted_value = ''
-        for c in chars:
-            if c == ',' and thousands_separator is not None:
-                formatted_value += thousands_separator
-            elif c == '.' and decimal_separator is not None:
-                formatted_value += decimal_separator
-            else:
-                formatted_value += c
-
-    # Display formatted value with suffix
-    return ValueString(f'{formatted_value}{suffix}', value)
-
-
-def parse_decimal(value: float|Decimal|str|None) -> float|Decimal:
-    """
-    Parse a decimal with variable decimal separator: may be formatted with comma decimal separator instead of dot.
-    """
-    if value is None or value == "":
-        return None
-        
-    if isinstance(value, (float,Decimal)):
-        return value   
-
-    value = str(value).replace(',', '.')
-
-    try:
-        return Decimal(value)
-    except InvalidOperation:
-        raise ValueError(f"invalid decimal value: {value}") from None
-
-
-def get_default_decimal_separator():
-    locale = getlocale()
-    if locale and locale[0] and locale[0].startswith('fr'):
-        return ','
-    else:
-        return '.'
-
+from __future__ import annotations
+from decimal import Decimal, InvalidOperation
+from locale import getlocale
+from .text import ValueString
+
+
+def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: str = None):
+    """
+    Get a human-readable representation of a number of bytes.
+
+    `max_multiple` may be `K`, `M`, `G'` or `T'. 
+    """
+    return human_number(value, unit=unit, divider=divider, decimals=decimals, decimal_separator=decimal_separator, thousands_separator=thousands_separator, max_multiple=max_multiple)
+
+
+def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: str = None):
+    """
+    Get a human-readable representation of a number.
+
+    `max_multiple` may be `K`, `M`, `G'` or `T'. 
+    """
+    if value is None:
+        return ValueString('', None)
+
+    suffixes = []
+
+    # Append non-multiple suffix (bytes)
+    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
+    if unit:
+        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
+    else:
+        suffixes.append('')
+
+    # Append multiple suffixes
+    for multiple in ['K', 'M', 'G', 'T']:
+        suffixes.append(f' {multiple}{unit}')
+        if max_multiple and max_multiple.upper() == multiple:
+            break
+
+    i = 0
+    suffix = suffixes[i]
+    divided_value = value
+
+    while divided_value > 1000 and i < len(suffixes) - 1:
+        divided_value /= divider
+        i += 1
+        suffix = suffixes[i]
+
+    # Format value
+    if i == 0:
+        formatted_value = '{value:,.0f}'.format(value=divided_value)
+    else:
+        formatted_value = ('{value:,.'+str(decimals)+'f}').format(value=divided_value)
+
+    #  Replace separators
+    if decimal_separator is not None or thousands_separator is not None:
+        chars = formatted_value
+        formatted_value = ''
+        for c in chars:
+            if c == ',' and thousands_separator is not None:
+                formatted_value += thousands_separator
+            elif c == '.' and decimal_separator is not None:
+                formatted_value += decimal_separator
+            else:
+                formatted_value += c
+
+    # Display formatted value with suffix
+    return ValueString(f'{formatted_value}{suffix}', value)
+
+
+def parse_decimal(value: float|Decimal|str|None) -> float|Decimal:
+    """
+    Parse a decimal with variable decimal separator: may be formatted with comma decimal separator instead of dot.
+    """
+    if value is None or value == "":
+        return None
+        
+    if isinstance(value, (float,Decimal)):
+        return value   
+
+    value = str(value).replace(',', '.')
+
+    try:
+        return Decimal(value)
+    except InvalidOperation:
+        raise ValueError(f"invalid decimal value: {value}") from None
+
+
+def get_default_decimal_separator():
+    locale = getlocale()
+    if locale and locale[0] and locale[0].startswith('fr'):
+        return ','
+    else:
+        return '.'
+
```

## zut/process.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-from __future__ import annotations
-import logging
-from subprocess import CompletedProcess, SubprocessError
-from .colors import Colors
-
-logger = logging.getLogger(__name__)
-
-
-
-def get_exit_code(return_value) -> int:    
-    if not isinstance(return_value, int):
-        return_value = 0 if return_value is None or return_value is True else 1
-    return return_value
-
-
-def check_completed_subprocess(cp: CompletedProcess, logger: logging.Logger = None, *, label: str = None, level: int|str = None, accept_returncode: int|list[int]|bool = False, accept_stdout: bool = False, accept_stderr: bool = False, maxlen: int = 200):
-    if not label:
-        label = cp.args[0]
-
-    if not logger and level is not None:
-        logger = globals()["logger"]
-    elif logger and level is None:
-        level = logging.ERROR
-
-
-    def is_returncode_issue(returncode: int):
-        if accept_returncode is True:
-            return False
-        elif isinstance(accept_returncode, int):
-            return returncode != accept_returncode
-        elif isinstance(accept_returncode, (list,tuple)):
-            return returncode not in accept_returncode
-        else:
-            return returncode != 0
-    
-
-    def extract_stream(content: str|bytes, name: str, color: str):
-        if not isinstance(content, str):
-            try:
-                content = content.decode('utf-8')
-            except UnicodeDecodeError:
-                content = content.decode('cp1252')
-        
-        data = content.strip()
-        if maxlen and len(data) > maxlen:
-            data = data[0:maxlen] + ''
-
-        result = ''
-        for line in data.splitlines():
-            result += f"\n{color}[{label} {name}]{Colors.RESET} {line}"
-        return result
-    
-
-    issue = False
-
-    if is_returncode_issue(cp.returncode):
-        message = f"{label} returned {Colors.YELLOW}code {cp.returncode}{Colors.RESET}"
-        issue = True
-    else:
-        message = f"{label} returned {Colors.CYAN}code {cp.returncode}{Colors.RESET}"
-    
-
-    result = extract_stream(cp.stdout, 'stdout', Colors.CYAN if accept_stdout else Colors.YELLOW)
-    if result:
-        message += result
-        if not accept_stdout:
-            issue = True
-
-    message += extract_stream(cp.stderr, 'stderr', Colors.CYAN if accept_stderr else Colors.YELLOW)
-    if result:
-        message += result
-        if not accept_stderr:
-            issue = True
-
-    if issue:
-        if logger:
-            logger.log(level, message)
-        else:
-            raise SubprocessError(message)
-    else:
-        if logger:
-            logger.log(logging.DEBUG, message)
-
-    return issue
+from __future__ import annotations
+import logging
+from subprocess import CompletedProcess, SubprocessError
+from .colors import Colors
+
+logger = logging.getLogger(__name__)
+
+
+
+def get_exit_code(return_value) -> int:    
+    if not isinstance(return_value, int):
+        return_value = 0 if return_value is None or return_value is True else 1
+    return return_value
+
+
+def check_completed_subprocess(cp: CompletedProcess, logger: logging.Logger = None, *, label: str = None, level: int|str = None, accept_returncode: int|list[int]|bool = False, accept_stdout: bool = False, accept_stderr: bool = False, maxlen: int = 200):
+    if not label:
+        label = cp.args[0]
+
+    if not logger and level is not None:
+        logger = globals()["logger"]
+    elif logger and level is None:
+        level = logging.ERROR
+
+
+    def is_returncode_issue(returncode: int):
+        if accept_returncode is True:
+            return False
+        elif isinstance(accept_returncode, int):
+            return returncode != accept_returncode
+        elif isinstance(accept_returncode, (list,tuple)):
+            return returncode not in accept_returncode
+        else:
+            return returncode != 0
+    
+
+    def extract_stream(content: str|bytes, name: str, color: str):
+        if not isinstance(content, str):
+            try:
+                content = content.decode('utf-8')
+            except UnicodeDecodeError:
+                content = content.decode('cp1252')
+        
+        data = content.strip()
+        if maxlen and len(data) > maxlen:
+            data = data[0:maxlen] + ''
+
+        result = ''
+        for line in data.splitlines():
+            result += f"\n{color}[{label} {name}]{Colors.RESET} {line}"
+        return result
+    
+
+    issue = False
+
+    if is_returncode_issue(cp.returncode):
+        message = f"{label} returned {Colors.YELLOW}code {cp.returncode}{Colors.RESET}"
+        issue = True
+    else:
+        message = f"{label} returned {Colors.CYAN}code {cp.returncode}{Colors.RESET}"
+    
+
+    result = extract_stream(cp.stdout, 'stdout', Colors.CYAN if accept_stdout else Colors.YELLOW)
+    if result:
+        message += result
+        if not accept_stdout:
+            issue = True
+
+    message += extract_stream(cp.stderr, 'stderr', Colors.CYAN if accept_stderr else Colors.YELLOW)
+    if result:
+        message += result
+        if not accept_stderr:
+            issue = True
+
+    if issue:
+        if logger:
+            logger.log(level, message)
+        else:
+            raise SubprocessError(message)
+    else:
+        if logger:
+            logger.log(logging.DEBUG, message)
+
+    return issue
```

## zut/tabular.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-from __future__ import annotations
-from typing import Any, Callable, Iterable
-
-class Row:
-    """
-    A row of tabular data.
-    """
-    def __init__(self, get: Iterable|Callable[[int],Any], *, headers: list[str], set: Callable[[int,Any]] = None):
-        if callable(get):
-            self._values = None
-            self._get = get
-            self._set = set
-        else:
-            # "get" is an Iterable
-            if set:
-                raise ValueError(f"argument \"set\" cannot be given if \"get\" is not a callable")
-            self._values = get if isinstance(get, list) else list(get)
-            self._get = None
-            self._set = None
-
-        self.headers = headers
-        self._header_indexes: dict[str,int] = None
-        
-        
-    def __len__(self):
-        return len(self.values)
-
-
-    @property
-    def values(self) -> list[Any]:
-        if self._values is not None:
-            return self._values
-        else:
-            return [self._get(index) for index in range(0, len(self.headers))]
-
-
-    def __getitem__(self, key: int|str):
-        if not isinstance(key, int):
-            key = self._get_header_index(key)
-            
-        if self._values is not None:
-            return self._values[key]
-        else:
-            return self._get(key)
-        
-
-    def __setitem__(self, key: int|str, value):
-        if not self._set:
-            raise ValueError(f"this row cannot be set")
-        
-        if not isinstance(key, int):
-            key = self._get_header_index(key)
-        
-        self._set(key, value)
-
-
-    def _get_header_index(self, header: str):
-        if self._header_indexes is None:
-            if not self.headers:
-                raise ValueError(f"cannot use string keys (no headers)")
-            self._header_indexes = {header: i for i, header in enumerate(self.headers)}
-        return self._header_indexes[header]
-
-
-    def as_dict(self):
-        return {header: self[i] for i, header in enumerate(self.headers)}
-
+from __future__ import annotations
+from typing import Any, Callable, Iterable
+
+class Row:
+    """
+    A row of tabular data.
+    """
+    def __init__(self, get: Iterable|Callable[[int],Any], *, headers: list[str], set: Callable[[int,Any]] = None):
+        if callable(get):
+            self._values = None
+            self._get = get
+            self._set = set
+        else:
+            # "get" is an Iterable
+            if set:
+                raise ValueError(f"argument \"set\" cannot be given if \"get\" is not a callable")
+            self._values = get if isinstance(get, list) else list(get)
+            self._get = None
+            self._set = None
+
+        self.headers = headers
+        self._header_indexes: dict[str,int] = None
+        
+        
+    def __len__(self):
+        return len(self.values)
+
+
+    @property
+    def values(self) -> list[Any]:
+        if self._values is not None:
+            return self._values
+        else:
+            return [self._get(index) for index in range(0, len(self.headers))]
+
+
+    def __getitem__(self, key: int|str):
+        if not isinstance(key, int):
+            key = self._get_header_index(key)
+            
+        if self._values is not None:
+            return self._values[key]
+        else:
+            return self._get(key)
+        
+
+    def __setitem__(self, key: int|str, value):
+        if not self._set:
+            raise ValueError(f"this row cannot be set")
+        
+        if not isinstance(key, int):
+            key = self._get_header_index(key)
+        
+        self._set(key, value)
+
+
+    def _get_header_index(self, header: str):
+        if self._header_indexes is None:
+            if not self.headers:
+                raise ValueError(f"cannot use string keys (no headers)")
+            self._header_indexes = {header: i for i, header in enumerate(self.headers)}
+        return self._header_indexes[header]
+
+
+    def as_dict(self):
+        return {header: self[i] for i, header in enumerate(self.headers)}
+
```

## zut/text.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-from __future__ import annotations
-import re
-import unicodedata
-from io import TextIOWrapper
-from typing import Generic, TypeVar
-
-T = TypeVar('T')
-
-
-class ValueString(str, Generic[T]):
-    """
-    A string internally associated to a value of a given type.
-    """
-    value: T
-
-    def __new__(cls, strvalue: str, value: T):
-        hb = super().__new__(cls, strvalue)
-        hb.value = value
-        return hb
-
-
-def slugify(value: str, allow_unicode: bool = False, separator: str = '-') -> str:
-    """ 
-    Generate a slug. Like `django.utils.text.slugify` with possibility to change the separator.
-    """
-    value = str(value)
-    if allow_unicode:
-        value = unicodedata.normalize("NFKC", value)
-    else:
-        value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
-    value = re.sub(r"[^\w\s-]", "", value.lower())
-    return re.sub(r"[-\s]+", separator, value).strip(f"{separator}_")
-
-
-def slugify_snake(value: str) -> str:
-    """
-    CamlCase => camel_case
-    """
-    value = str(value)
-    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
-    value = re.sub(r"[^\w\s-]", "", value) # not .lower()
-    value = re.sub(r"[-_\s]+", '_', value).strip('_')
-    value = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', value)
-    return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', value).lower()
-
-
-def remove_consecutive_whitespaces(s: str):
-    if s is None:
-        return None
-    return re.sub(r'\s+', ' ', s).strip()
-
-
-def remove_whitespaces(s):
-    if s is None:
-        return None
-    return re.sub(r'\s', '', s)
-
-
-def reconfigure_encoding(fp: TextIOWrapper, encoding: str) -> str:
-    """
-    Reconfigure utf-8 encoding to handle the BOM if any.
-    """
-    if encoding == 'utf-8':
-        text = fp.read()
-        if text and text[0] == '\ufeff':
-            encoding = 'utf-8-sig'
-            fp.reconfigure(encoding=encoding)
-        fp.seek(0)
-
-    return encoding
+from __future__ import annotations
+import re
+import unicodedata
+from io import TextIOWrapper
+from typing import Generic, TypeVar
+
+T = TypeVar('T')
+
+
+class ValueString(str, Generic[T]):
+    """
+    A string internally associated to a value of a given type.
+    """
+    value: T
+
+    def __new__(cls, strvalue: str, value: T):
+        hb = super().__new__(cls, strvalue)
+        hb.value = value
+        return hb
+
+
+def slugify(value: str, allow_unicode: bool = False, separator: str = '-') -> str:
+    """ 
+    Generate a slug. Like `django.utils.text.slugify` with possibility to change the separator.
+    """
+    value = str(value)
+    if allow_unicode:
+        value = unicodedata.normalize("NFKC", value)
+    else:
+        value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
+    value = re.sub(r"[^\w\s-]", "", value.lower())
+    return re.sub(r"[-\s]+", separator, value).strip(f"{separator}_")
+
+
+def slugify_snake(value: str) -> str:
+    """
+    CamlCase => camel_case
+    """
+    value = str(value)
+    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
+    value = re.sub(r"[^\w\s-]", "", value) # not .lower()
+    value = re.sub(r"[-_\s]+", '_', value).strip('_')
+    value = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', value)
+    return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', value).lower()
+
+
+def remove_consecutive_whitespaces(s: str):
+    if s is None:
+        return None
+    return re.sub(r'\s+', ' ', s).strip()
+
+
+def remove_whitespaces(s):
+    if s is None:
+        return None
+    return re.sub(r'\s', '', s)
+
+
+def reconfigure_encoding(fp: TextIOWrapper, encoding: str) -> str:
+    """
+    Reconfigure utf-8 encoding to handle the BOM if any.
+    """
+    if encoding == 'utf-8':
+        text = fp.read()
+        if text and text[0] == '\ufeff':
+            encoding = 'utf-8-sig'
+            fp.reconfigure(encoding=encoding)
+        fp.seek(0)
+
+    return encoding
```

## zut/typing.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-from __future__ import annotations
-import sys
-
-
-if sys.version_info[0:2] < (3, 8):
-    from typing_extensions import Literal, Protocol
-else:
-    from typing import Literal, Protocol
-    
-if sys.version_info[0:2] < (3, 11):
-    from typing_extensions import Self
-else:
-    from typing import Self
+from __future__ import annotations
+import sys
+
+
+if sys.version_info[0:2] < (3, 8):
+    from typing_extensions import Literal, Protocol
+else:
+    from typing import Literal, Protocol
+    
+if sys.version_info[0:2] < (3, 11):
+    from typing_extensions import Self
+else:
+    from typing import Self
```

## zut/venv.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-from __future__ import annotations
-import sys
-from pathlib import Path
-
-
-def get_venv() -> Path|None:
-    """
-    Return the path to the virtual environment if Python runs inside a virtual environment, None otherwise.
-    """
-    base_prefix = getattr(sys, "base_prefix", None) or getattr(sys, "real_prefix", None) or sys.prefix
-    if base_prefix == sys.prefix:
-        return None
-    return Path(sys.prefix)
-
-
-def is_in_venv(path: Path|str) -> bool:
-    if not isinstance(path, Path):
-        path = Path(path)
-    venv = get_venv()
-    if venv is None:
-        return False
-    return venv in path.parents
+from __future__ import annotations
+import sys
+from pathlib import Path
+
+
+def get_venv() -> Path|None:
+    """
+    Return the path to the virtual environment if Python runs inside a virtual environment, None otherwise.
+    """
+    base_prefix = getattr(sys, "base_prefix", None) or getattr(sys, "real_prefix", None) or sys.prefix
+    if base_prefix == sys.prefix:
+        return None
+    return Path(sys.prefix)
+
+
+def is_in_venv(path: Path|str) -> bool:
+    if not isinstance(path, Path):
+        path = Path(path)
+    venv = get_venv()
+    if venv is None:
+        return False
+    return venv in path.parents
```

## zut/db/__init__.py

```diff
@@ -1,47 +1,47 @@
-from __future__ import annotations
-import re
-from urllib.parse import urlparse
-from .commons import DbWrapper
-from .mssql import MssqlWrapper
-from .pg import PgWrapper
-
-
-def create_db_wrapper_with_schema_and_table(uri: str) -> tuple[DbWrapper, str, str]:
-    if uri.startswith('db:'):
-        uri = uri[3:]
-
-    r = urlparse(uri)
-
-    if r.scheme == PgWrapper.scheme:
-        wrapper_cls = PgWrapper
-    elif r.scheme == MssqlWrapper.scheme:
-        wrapper_cls = MssqlWrapper
-    elif r.scheme:
-        raise ValueError(f"unsupported db engine: {r.scheme}")
-    else:
-        raise ValueError(f"invalid db src: no scheme in {uri}")
-    
-    if not wrapper_cls.is_available():
-        raise ValueError(f"cannot use db {r.scheme} ({wrapper_cls.__name__} not available)")
-    
-    if r.fragment:
-        raise ValueError(f"invalid db src: unexpected fragment: {r.fragment}")
-    if r.query:
-        raise ValueError(f"invalid db src: unexpected query: {r.query}")
-    if r.params:
-        raise ValueError(f"invalid db src: unexpected params: {r.params}")
-    
-    m = re.match(r'^/(?P<name>[^/@\:]+)(/((?P<schema>[^/@\:\.]+)\.)?(?P<table>[^/@\:\.]+))?$', r.path)
-    if not m:
-        raise ValueError(f"invalid db src: invalid path: {r.path}")
-    
-    name = m['name']
-    table = m['table']
-    schema = (m['schema'] or wrapper_cls.default_schema_name) if table else None
-    
-    return wrapper_cls(name=name, host=r.hostname, port=r.port, user=r.username, password=r.password), schema, table
-
-
-def create_db_wrapper(uri: str) -> DbWrapper:
-    db, _, _ = create_db_wrapper_with_schema_and_table(uri)
-    return db
+from __future__ import annotations
+import re
+from urllib.parse import urlparse
+from .commons import DbWrapper, get_sql_file_data
+from .mssql import MssqlWrapper
+from .pg import PgWrapper
+
+
+def create_db_wrapper_with_schema_and_table(uri: str) -> tuple[DbWrapper, str, str]:
+    if uri.startswith('db:'):
+        uri = uri[3:]
+
+    r = urlparse(uri)
+
+    if r.scheme == PgWrapper.scheme:
+        wrapper_cls = PgWrapper
+    elif r.scheme == MssqlWrapper.scheme:
+        wrapper_cls = MssqlWrapper
+    elif r.scheme:
+        raise ValueError(f"unsupported db engine: {r.scheme}")
+    else:
+        raise ValueError(f"invalid db src: no scheme in {uri}")
+    
+    if not wrapper_cls.is_available():
+        raise ValueError(f"cannot use db {r.scheme} ({wrapper_cls.__name__} not available)")
+    
+    if r.fragment:
+        raise ValueError(f"invalid db src: unexpected fragment: {r.fragment}")
+    if r.query:
+        raise ValueError(f"invalid db src: unexpected query: {r.query}")
+    if r.params:
+        raise ValueError(f"invalid db src: unexpected params: {r.params}")
+    
+    m = re.match(r'^/(?P<name>[^/@\:]+)(/((?P<schema>[^/@\:\.]+)\.)?(?P<table>[^/@\:\.]+))?$', r.path)
+    if not m:
+        raise ValueError(f"invalid db src: invalid path: {r.path}")
+    
+    name = m['name']
+    table = m['table']
+    schema = (m['schema'] or wrapper_cls.default_schema_name) if table else None
+    
+    return wrapper_cls(name=name, host=r.hostname, port=r.port, user=r.username, password=r.password), schema, table
+
+
+def create_db_wrapper(uri: str) -> DbWrapper:
+    db, _, _ = create_db_wrapper_with_schema_and_table(uri)
+    return db
```

## zut/db/commons.py

```diff
@@ -1,936 +1,959 @@
-00000000: 0d0a 6672 6f6d 205f 5f66 7574 7572 655f  ..from __future_
-00000010: 5f20 696d 706f 7274 2061 6e6e 6f74 6174  _ import annotat
-00000020: 696f 6e73 0d0a 6672 6f6d 2064 6174 6574  ions..from datet
-00000030: 696d 6520 696d 706f 7274 2074 696d 657a  ime import timez
-00000040: 6f6e 650d 0a66 726f 6d20 696f 2069 6d70  one..from io imp
-00000050: 6f72 7420 494f 4261 7365 0d0a 696d 706f  ort IOBase..impo
-00000060: 7274 206c 6f67 6769 6e67 0d0a 6672 6f6d  rt logging..from
-00000070: 2070 6174 686c 6962 2069 6d70 6f72 7420   pathlib import 
-00000080: 5061 7468 0d0a 696d 706f 7274 2072 650d  Path..import re.
-00000090: 0a66 726f 6d20 7479 7069 6e67 2069 6d70  .from typing imp
-000000a0: 6f72 7420 416e 792c 2047 656e 6572 6963  ort Any, Generic
-000000b0: 2c20 5479 7065 5661 720d 0a66 726f 6d20  , TypeVar..from 
-000000c0: 7572 6c6c 6962 2e70 6172 7365 2069 6d70  urllib.parse imp
-000000d0: 6f72 7420 7175 6f74 650d 0a0d 0a6c 6f67  ort quote....log
-000000e0: 6765 7220 3d20 6c6f 6767 696e 672e 6765  ger = logging.ge
-000000f0: 744c 6f67 6765 7228 5f5f 6e61 6d65 5f5f  tLogger(__name__
-00000100: 290d 0a0d 0a54 5f43 6f6e 6e65 6374 696f  )....T_Connectio
-00000110: 6e20 3d20 5479 7065 5661 7228 2754 5f43  n = TypeVar('T_C
-00000120: 6f6e 6e65 6374 696f 6e27 290d 0a54 5f43  onnection')..T_C
-00000130: 7572 736f 7220 3d20 5479 7065 5661 7228  ursor = TypeVar(
-00000140: 2754 5f43 7572 736f 7227 290d 0a0d 0a63  'T_Cursor')....c
-00000150: 6c61 7373 2044 6257 7261 7070 6572 2847  lass DbWrapper(G
-00000160: 656e 6572 6963 5b54 5f43 6f6e 6e65 6374  eneric[T_Connect
-00000170: 696f 6e2c 2054 5f43 7572 736f 725d 293a  ion, T_Cursor]):
-00000180: 0d0a 2020 2020 4063 6c61 7373 6d65 7468  ..    @classmeth
-00000190: 6f64 0d0a 2020 2020 6465 6620 6973 5f61  od..    def is_a
-000001a0: 7661 696c 6162 6c65 2863 6c73 293a 0d0a  vailable(cls):..
-000001b0: 2020 2020 2020 2020 7261 6973 6520 4e6f          raise No
-000001c0: 7449 6d70 6c65 6d65 6e74 6564 4572 726f  tImplementedErro
-000001d0: 7228 290d 0a20 2020 200d 0a20 2020 2073  r()..    ..    s
-000001e0: 6368 656d 653a 2073 7472 0d0a 2020 2020  cheme: str..    
-000001f0: 6465 6661 756c 745f 7363 6865 6d61 5f6e  default_schema_n
-00000200: 616d 653a 2073 7472 0d0a 2020 2020 636f  ame: str..    co
-00000210: 6d70 6174 6962 6c65 5f64 6a61 6e67 6f5f  mpatible_django_
-00000220: 656e 6769 6e65 733a 206c 6973 745b 7374  engines: list[st
-00000230: 725d 0d0a 0d0a 2020 2020 6465 6620 5f5f  r]....    def __
-00000240: 696e 6974 5f5f 2873 656c 662c 202a 2c20  init__(self, *, 
-00000250: 6e61 6d65 3a20 7374 7220 3d20 4e6f 6e65  name: str = None
-00000260: 2c20 686f 7374 3a20 7374 7220 3d20 4e6f  , host: str = No
-00000270: 6e65 2c20 706f 7274 3a20 696e 7420 3d20  ne, port: int = 
-00000280: 4e6f 6e65 2c20 7573 6572 3a20 7374 7220  None, user: str 
-00000290: 3d20 4e6f 6e65 2c20 7061 7373 776f 7264  = None, password
-000002a0: 3a20 7374 7220 3d20 4e6f 6e65 2c20 646a  : str = None, dj
-000002b0: 616e 676f 5f61 6c69 6173 3a20 7374 7220  ango_alias: str 
-000002c0: 3d20 4e6f 6e65 2c20 636f 6e6e 6563 7469  = None, connecti
-000002d0: 6f6e 3a20 545f 436f 6e6e 6563 7469 6f6e  on: T_Connection
-000002e0: 203d 204e 6f6e 652c 206e 6169 7665 5f74   = None, naive_t
-000002f0: 7a3a 204c 6974 6572 616c 5b27 6c6f 6361  z: Literal['loca
-00000300: 6c27 5d7c 7469 6d65 7a6f 6e65 203d 2027  l']|timezone = '
-00000310: 6c6f 6361 6c27 293a 2020 2020 0d0a 2020  local'):    ..  
-00000320: 2020 2020 2020 6966 206e 6f74 2073 656c        if not sel
-00000330: 662e 6973 5f61 7661 696c 6162 6c65 2829  f.is_available()
-00000340: 3a0d 0a20 2020 2020 2020 2020 2020 2072  :..            r
-00000350: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-00000360: 6622 6361 6e6e 6f74 2075 7365 207b 7479  f"cannot use {ty
-00000370: 7065 2873 656c 6629 2e5f 5f6e 616d 655f  pe(self).__name_
-00000380: 5f7d 2028 6e6f 7420 6176 6169 6c61 626c  _} (not availabl
-00000390: 6529 2229 0d0a 2020 2020 2020 2020 0d0a  e)")..        ..
-000003a0: 2020 2020 2020 2020 6966 2064 6a61 6e67          if djang
-000003b0: 6f5f 616c 6961 7320 6973 206e 6f74 204e  o_alias is not N
-000003c0: 6f6e 653a 0d0a 2020 2020 2020 2020 2020  one:..          
-000003d0: 2020 6672 6f6d 2064 6a61 6e67 6f2e 636f    from django.co
-000003e0: 6e66 2069 6d70 6f72 7420 7365 7474 696e  nf import settin
-000003f0: 6773 0d0a 2020 2020 2020 2020 2020 2020  gs..            
-00000400: 4441 5441 4241 5345 5320 3d20 7365 7474  DATABASES = sett
-00000410: 696e 6773 2e44 4154 4142 4153 4553 0d0a  ings.DATABASES..
-00000420: 2020 2020 2020 2020 2020 2020 6966 206e              if n
-00000430: 6f74 2064 6a61 6e67 6f5f 616c 6961 7320  ot django_alias 
-00000440: 696e 2044 4154 4142 4153 4553 3a0d 0a20  in DATABASES:.. 
-00000450: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-00000460: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-00000470: 6622 6b65 7920 5c22 7b64 6a61 6e67 6f5f  f"key \"{django_
-00000480: 616c 6961 737d 5c22 206e 6f74 2066 6f75  alias}\" not fou
-00000490: 6e64 2069 6e20 646a 616e 676f 2044 4154  nd in django DAT
-000004a0: 4142 4153 4553 2073 6574 7469 6e67 7322  ABASES settings"
-000004b0: 290d 0a20 2020 2020 2020 2020 2020 2064  )..            d
-000004c0: 6174 6162 6173 655f 7365 7474 696e 6773  atabase_settings
-000004d0: 3a20 6469 6374 5b73 7472 2c41 6e79 5d20  : dict[str,Any] 
-000004e0: 3d20 4441 5441 4241 5345 535b 646a 616e  = DATABASES[djan
-000004f0: 676f 5f61 6c69 6173 5d0d 0a20 2020 2020  go_alias]..     
-00000500: 2020 2020 2020 2065 6e67 696e 6520 3d20         engine = 
-00000510: 6461 7461 6261 7365 5f73 6574 7469 6e67  database_setting
-00000520: 735b 2745 4e47 494e 4527 5d0d 0a20 2020  s['ENGINE']..   
-00000530: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-00000540: 656e 6769 6e65 2069 6e20 7365 6c66 2e63  engine in self.c
-00000550: 6f6d 7061 7469 626c 655f 646a 616e 676f  ompatible_django
-00000560: 5f65 6e67 696e 6573 3a0d 0a20 2020 2020  _engines:..     
-00000570: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-00000580: 2056 616c 7565 4572 726f 7228 6622 646a   ValueError(f"dj
-00000590: 616e 676f 2064 6174 6162 6173 6520 656e  ango database en
-000005a0: 6769 6e65 205c 227b 656e 6769 6e65 7d5c  gine \"{engine}\
-000005b0: 2220 6973 206e 6f74 2063 6f6d 7061 7469  " is not compati
-000005c0: 626c 6520 7769 7468 207b 7479 7065 2873  ble with {type(s
-000005d0: 656c 6629 2e5f 5f6e 616d 655f 5f7d 2229  elf).__name__}")
-000005e0: 0d0a 2020 2020 2020 2020 656c 7365 3a0d  ..        else:.
-000005f0: 0a20 2020 2020 2020 2020 2020 2064 6174  .            dat
-00000600: 6162 6173 655f 7365 7474 696e 6773 3a20  abase_settings: 
-00000610: 6469 6374 5b73 7472 2c41 6e79 5d20 3d20  dict[str,Any] = 
-00000620: 7b7d 0d0a 0d0a 2020 2020 2020 2020 7365  {}....        se
-00000630: 6c66 2e5f 6e61 6d65 3a20 7374 7220 3d20  lf._name: str = 
-00000640: 6e61 6d65 2069 6620 6e61 6d65 2069 7320  name if name is 
-00000650: 6e6f 7420 4e6f 6e65 2065 6c73 6520 6461  not None else da
-00000660: 7461 6261 7365 5f73 6574 7469 6e67 732e  tabase_settings.
-00000670: 6765 7428 274e 414d 4527 2c20 4e6f 6e65  get('NAME', None
-00000680: 290d 0a20 2020 2020 2020 2073 656c 662e  )..        self.
-00000690: 5f68 6f73 743a 2073 7472 203d 2068 6f73  _host: str = hos
-000006a0: 7420 6966 2068 6f73 7420 6973 206e 6f74  t if host is not
-000006b0: 204e 6f6e 6520 656c 7365 2064 6174 6162   None else datab
-000006c0: 6173 655f 7365 7474 696e 6773 2e67 6574  ase_settings.get
-000006d0: 2827 484f 5354 272c 204e 6f6e 6529 0d0a  ('HOST', None)..
-000006e0: 2020 2020 2020 2020 7365 6c66 2e5f 7573          self._us
-000006f0: 6572 3a20 7374 7220 3d20 7573 6572 2069  er: str = user i
-00000700: 6620 7573 6572 2069 7320 6e6f 7420 4e6f  f user is not No
-00000710: 6e65 2065 6c73 6520 6461 7461 6261 7365  ne else database
-00000720: 5f73 6574 7469 6e67 732e 6765 7428 2755  _settings.get('U
-00000730: 5345 5227 2c20 4e6f 6e65 290d 0a20 2020  SER', None)..   
-00000740: 2020 2020 2073 656c 662e 5f70 6173 7377       self._passw
-00000750: 6f72 643a 2073 7472 203d 2070 6173 7377  ord: str = passw
-00000760: 6f72 6420 6966 2070 6173 7377 6f72 6420  ord if password 
-00000770: 6973 206e 6f74 204e 6f6e 6520 656c 7365  is not None else
-00000780: 2064 6174 6162 6173 655f 7365 7474 696e   database_settin
-00000790: 6773 2e67 6574 2827 5041 5353 574f 5244  gs.get('PASSWORD
-000007a0: 272c 204e 6f6e 6529 0d0a 2020 2020 2020  ', None)..      
-000007b0: 2020 7365 6c66 2e5f 706f 7274 3a20 7374    self._port: st
-000007c0: 7220 3d20 706f 7274 2069 6620 706f 7274  r = port if port
-000007d0: 2069 7320 6e6f 7420 4e6f 6e65 2065 6c73   is not None els
-000007e0: 6520 6461 7461 6261 7365 5f73 6574 7469  e database_setti
-000007f0: 6e67 732e 6765 7428 2750 4f52 5427 2c20  ngs.get('PORT', 
-00000800: 4e6f 6e65 290d 0a0d 0a20 2020 2020 2020  None)....       
-00000810: 2069 6620 6973 696e 7374 616e 6365 2873   if isinstance(s
-00000820: 656c 662e 5f70 6f72 742c 2073 7472 293a  elf._port, str):
-00000830: 0d0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00000840: 6c66 2e5f 706f 7274 203d 2069 6e74 2873  lf._port = int(s
-00000850: 656c 662e 5f70 6f72 7429 0d0a 2020 2020  elf._port)..    
-00000860: 2020 2020 0d0a 2020 2020 2020 2020 7365      ..        se
-00000870: 6c66 2e5f 6e61 6976 655f 747a 203d 206e  lf._naive_tz = n
-00000880: 6169 7665 5f74 7a0d 0a20 2020 2020 2020  aive_tz..       
-00000890: 2022 2222 2049 6620 6e6f 7420 4e6f 6e65   """ If not None
-000008a0: 2c20 696e 6469 6361 7465 2077 6869 6368  , indicate which
-000008b0: 2074 696d 657a 6f6e 6520 7769 6c6c 2062   timezone will b
-000008c0: 6520 7573 6564 2066 6f72 206e 6169 7665  e used for naive
-000008d0: 2064 6174 6574 696d 6573 2e20 2222 220d   datetimes. """.
-000008e0: 0a0d 0a20 2020 2020 2020 2073 656c 662e  ...        self.
-000008f0: 5f63 6f6e 6e65 6374 696f 6e20 3d20 636f  _connection = co
-00000900: 6e6e 6563 7469 6f6e 0d0a 2020 2020 2020  nnection..      
-00000910: 2020 7365 6c66 2e5f 6d75 7374 5f63 6c6f    self._must_clo
-00000920: 7365 5f63 6f6e 6e65 6374 696f 6e20 3d20  se_connection = 
-00000930: 636f 6e6e 6563 7469 6f6e 2069 7320 4e6f  connection is No
-00000940: 6e65 0d0a 0d0a 0d0a 2020 2020 6465 6620  ne......    def 
-00000950: 5f5f 656e 7465 725f 5f28 7365 6c66 293a  __enter__(self):
-00000960: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00000970: 2073 656c 660d 0a0d 0a0d 0a20 2020 2064   self......    d
-00000980: 6566 205f 5f65 7869 745f 5f28 7365 6c66  ef __exit__(self
-00000990: 2c20 6578 635f 7479 7065 203d 204e 6f6e  , exc_type = Non
-000009a0: 652c 2065 7863 5f76 616c 203d 204e 6f6e  e, exc_val = Non
-000009b0: 652c 2065 7863 5f74 6220 3d20 4e6f 6e65  e, exc_tb = None
-000009c0: 293a 0d0a 2020 2020 2020 2020 6966 2073  ):..        if s
-000009d0: 656c 662e 5f63 6f6e 6e65 6374 696f 6e20  elf._connection 
-000009e0: 616e 6420 7365 6c66 2e5f 6d75 7374 5f63  and self._must_c
-000009f0: 6c6f 7365 5f63 6f6e 6e65 6374 696f 6e3a  lose_connection:
-00000a00: 0d0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00000a10: 6c66 2e5f 636f 6e6e 6563 7469 6f6e 2e63  lf._connection.c
-00000a20: 6c6f 7365 2829 0d0a 0d0a 0d0a 2020 2020  lose()......    
-00000a30: 6465 6620 6765 745f 7572 6928 7365 6c66  def get_uri(self
-00000a40: 2c20 7461 626c 653a 2073 7472 7c74 7570  , table: str|tup
-00000a50: 6c65 2c20 2a2c 2077 6974 685f 7061 7373  le, *, with_pass
-00000a60: 776f 7264 203d 2046 616c 7365 293a 0d0a  word = False):..
-00000a70: 2020 2020 2020 2020 7572 6920 3d20 6622          uri = f"
-00000a80: 7b73 656c 662e 7363 6865 6d65 7d3a 220d  {self.scheme}:".
-00000a90: 0a0d 0a20 2020 2020 2020 2069 6620 7365  ...        if se
-00000aa0: 6c66 2e5f 7573 6572 206f 7220 7365 6c66  lf._user or self
-00000ab0: 2e5f 686f 7374 3a0d 0a20 2020 2020 2020  ._host:..       
-00000ac0: 2020 2020 2075 7269 202b 3d20 272f 2f27       uri += '//'
-00000ad0: 0d0a 2020 2020 2020 2020 2020 2020 6966  ..            if
-00000ae0: 2073 656c 662e 5f75 7365 723a 0d0a 2020   self._user:..  
-00000af0: 2020 2020 2020 2020 2020 2020 2020 7572                ur
-00000b00: 6920 2b3d 2071 756f 7465 2873 656c 662e  i += quote(self.
-00000b10: 5f75 7365 7229 0d0a 2020 2020 2020 2020  _user)..        
-00000b20: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00000b30: 5f70 6173 7377 6f72 643a 0d0a 2020 2020  _password:..    
-00000b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b50: 7572 6920 2b3d 2027 3a27 202b 2028 7175  uri += ':' + (qu
-00000b60: 6f74 6528 7365 6c66 2e5f 7061 7373 776f  ote(self._passwo
-00000b70: 7264 2920 6966 2077 6974 685f 7061 7373  rd) if with_pass
-00000b80: 776f 7264 2065 6c73 6520 7265 2e73 7562  word else re.sub
-00000b90: 2872 272e 272c 2027 2a27 2c20 7365 6c66  (r'.', '*', self
-00000ba0: 2e5f 7061 7373 776f 7264 2929 0d0a 2020  ._password))..  
-00000bb0: 2020 2020 2020 2020 2020 2020 2020 7572                ur
-00000bc0: 6920 2b3d 2027 4027 0d0a 0d0a 2020 2020  i += '@'....    
-00000bd0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00000be0: 5f68 6f73 743a 0d0a 2020 2020 2020 2020  _host:..        
-00000bf0: 2020 2020 2020 2020 7572 6920 2b3d 2071          uri += q
-00000c00: 756f 7465 2873 656c 662e 5f68 6f73 7429  uote(self._host)
-00000c10: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000c20: 2020 6966 2073 656c 662e 5f70 6f72 743a    if self._port:
-00000c30: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000c40: 2020 2020 2020 7572 6920 2b3d 2066 273a        uri += f':
-00000c50: 7b73 656c 662e 5f70 6f72 747d 270d 0a0d  {self._port}'...
-00000c60: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
-00000c70: 2e5f 6e61 6d65 3a0d 0a20 2020 2020 2020  ._name:..       
-00000c80: 2020 2020 2075 7269 202b 3d20 6622 2f7b       uri += f"/{
-00000c90: 7175 6f74 6528 7365 6c66 2e5f 6e61 6d65  quote(self._name
-00000ca0: 297d 220d 0a0d 0a20 2020 2020 2020 2073  )}"....        s
-00000cb0: 6368 656d 612c 2074 6162 6c65 203d 2073  chema, table = s
-00000cc0: 656c 662e 7370 6c69 745f 6e61 6d65 2874  elf.split_name(t
-00000cd0: 6162 6c65 290d 0a20 2020 2020 2020 2069  able)..        i
-00000ce0: 6620 7461 626c 653a 0d0a 2020 2020 2020  f table:..      
-00000cf0: 2020 2020 2020 7572 6920 2b3d 2066 222f        uri += f"/
-00000d00: 220d 0a20 2020 2020 2020 2020 2020 2069  "..            i
-00000d10: 6620 7363 6865 6d61 3a0d 0a20 2020 2020  f schema:..     
-00000d20: 2020 2020 2020 2020 2020 2075 7269 202b             uri +
-00000d30: 3d20 7175 6f74 6528 7363 6865 6d61 290d  = quote(schema).
-00000d40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000d50: 2075 7269 202b 3d20 272e 270d 0a20 2020   uri += '.'..   
-00000d60: 2020 2020 2020 2020 2075 7269 202b 3d20           uri += 
-00000d70: 7175 6f74 6528 7461 626c 6529 0d0a 0d0a  quote(table)....
-00000d80: 2020 2020 2020 2020 7265 7475 726e 2075          return u
-00000d90: 7269 0d0a 0d0a 0d0a 2020 2020 4070 726f  ri......    @pro
-00000da0: 7065 7274 790d 0a20 2020 2064 6566 2063  perty..    def c
-00000db0: 6f6e 6e65 6374 696f 6e28 7365 6c66 293a  onnection(self):
-00000dc0: 0d0a 2020 2020 2020 2020 6966 206e 6f74  ..        if not
-00000dd0: 2073 656c 662e 5f63 6f6e 6e65 6374 696f   self._connectio
-00000de0: 6e3a 0d0a 2020 2020 2020 2020 2020 2020  n:..            
-00000df0: 7365 6c66 2e5f 636f 6e6e 6563 7469 6f6e  self._connection
-00000e00: 203d 2073 656c 662e 5f63 7265 6174 655f   = self._create_
-00000e10: 636f 6e6e 6563 7469 6f6e 2829 0d0a 2020  connection()..  
-00000e20: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00000e30: 662e 5f63 6f6e 6e65 6374 696f 6e0d 0a0d  f._connection...
-00000e40: 0a0d 0a20 2020 2064 6566 205f 6372 6561  ...    def _crea
-00000e50: 7465 5f63 6f6e 6e65 6374 696f 6e28 7365  te_connection(se
-00000e60: 6c66 2920 2d3e 2054 5f43 6f6e 6e65 6374  lf) -> T_Connect
-00000e70: 696f 6e3a 0d0a 2020 2020 2020 2020 7261  ion:..        ra
-00000e80: 6973 6520 4e6f 7449 6d70 6c65 6d65 6e74  ise NotImplement
-00000e90: 6564 4572 726f 7228 290d 0a0d 0a0d 0a20  edError()...... 
-00000ea0: 2020 2064 6566 2063 7572 736f 7228 7365     def cursor(se
-00000eb0: 6c66 2920 2d3e 2054 5f43 7572 736f 723a  lf) -> T_Cursor:
-00000ec0: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00000ed0: 2073 656c 662e 636f 6e6e 6563 7469 6f6e   self.connection
-00000ee0: 2e63 7572 736f 7228 290d 0a20 2020 200d  .cursor()..    .
-00000ef0: 0a20 2020 200d 0a20 2020 2064 6566 206c  .    ..    def l
-00000f00: 696d 6974 5f71 7565 7279 2873 656c 662c  imit_query(self,
-00000f10: 2071 7565 7279 3a20 7374 722c 206c 696d   query: str, lim
-00000f20: 6974 3a20 696e 7429 3a0d 0a20 2020 2020  it: int):..     
-00000f30: 2020 2069 6620 6c69 6d69 7420 6973 204e     if limit is N
-00000f40: 6f6e 653a 0d0a 2020 2020 2020 2020 2020  one:..          
-00000f50: 2020 7265 7475 726e 2071 7565 7279 0d0a    return query..
-00000f60: 2020 2020 0d0a 2020 2020 2020 2020 6966      ..        if
-00000f70: 206e 6f74 2069 7369 6e73 7461 6e63 6528   not isinstance(
-00000f80: 6c69 6d69 742c 2069 6e74 293a 0d0a 2020  limit, int):..  
-00000f90: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-00000fa0: 5661 6c75 6545 7272 6f72 2866 2269 6e76  ValueError(f"inv
-00000fb0: 616c 6964 2074 7970 6520 666f 7220 6c69  alid type for li
-00000fc0: 6d69 743a 207b 7479 7065 286c 696d 6974  mit: {type(limit
-00000fd0: 292e 5f5f 6e61 6d65 5f5f 7d20 2865 7870  ).__name__} (exp
-00000fe0: 6563 7465 6420 696e 7429 2229 0d0a 2020  ected int)")..  
-00000ff0: 2020 2020 2020 0d0a 2020 2020 2020 2020        ..        
-00001000: 696d 706f 7274 2073 716c 7061 7273 6520  import sqlparse 
-00001010: 2320 6e6f 7420 6174 2074 6865 2074 6f70  # not at the top
-00001020: 2062 6563 6175 7365 2074 6865 2065 6e64   because the end
-00001030: 7573 6572 206d 6967 6874 206e 6f74 206e  user might not n
-00001040: 6565 6420 7468 6973 2066 6561 7475 7265  eed this feature
-00001050: 0d0a 0d0a 2020 2020 2020 2020 2320 5061  ....        # Pa
-00001060: 7273 6520 5351 4c20 746f 2072 656d 6f76  rse SQL to remov
-00001070: 6520 746f 6b65 6e20 6265 666f 7265 2074  e token before t
-00001080: 6865 2053 454c 4543 5420 6b65 7977 6f72  he SELECT keywor
-00001090: 640d 0a20 2020 2020 2020 2023 2065 7861  d..        # exa
-000010a0: 6d70 6c65 3a20 5749 5448 2028 4354 4529  mple: WITH (CTE)
-000010b0: 2074 6f6b 656e 730d 0a20 2020 2020 2020   tokens..       
-000010c0: 2073 7461 7465 6d65 6e74 7320 3d20 7371   statements = sq
-000010d0: 6c70 6172 7365 2e70 6172 7365 2871 7565  lparse.parse(que
-000010e0: 7279 290d 0a20 2020 2020 2020 2069 6620  ry)..        if 
-000010f0: 6c65 6e28 7374 6174 656d 656e 7473 2920  len(statements) 
-00001100: 213d 2031 3a0d 0a20 2020 2020 2020 2020  != 1:..         
-00001110: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00001120: 726f 7228 6622 7175 6572 7920 636f 6e74  ror(f"query cont
-00001130: 6169 6e73 207b 6c65 6e28 7374 6174 656d  ains {len(statem
-00001140: 656e 7473 297d 2073 7461 7465 6d65 6e74  ents)} statement
-00001150: 7322 290d 0a0d 0a20 2020 2020 2020 2023  s")....        #
-00001160: 2047 6574 2066 6972 7374 2044 4d4c 206b   Get first DML k
-00001170: 6579 776f 7264 0d0a 2020 2020 2020 2020  eyword..        
-00001180: 646d 6c5f 6b65 7977 6f72 6420 3d20 4e6f  dml_keyword = No
-00001190: 6e65 0d0a 2020 2020 2020 2020 646d 6c5f  ne..        dml_
-000011a0: 6b65 7977 6f72 645f 696e 6465 7820 3d20  keyword_index = 
-000011b0: 4e6f 6e65 0d0a 2020 2020 2020 2020 6f72  None..        or
-000011c0: 6465 725f 6279 5f69 6e64 6578 203d 204e  der_by_index = N
-000011d0: 6f6e 650d 0a20 2020 2020 2020 2066 6f72  one..        for
-000011e0: 2069 2c20 746f 6b65 6e20 696e 2065 6e75   i, token in enu
-000011f0: 6d65 7261 7465 2873 7461 7465 6d65 6e74  merate(statement
-00001200: 735b 305d 2e74 6f6b 656e 7329 3a0d 0a20  s[0].tokens):.. 
-00001210: 2020 2020 2020 2020 2020 2069 6620 746f             if to
-00001220: 6b65 6e2e 7474 7970 6520 3d3d 2073 716c  ken.ttype == sql
-00001230: 7061 7273 652e 746f 6b65 6e73 2e44 4d4c  parse.tokens.DML
-00001240: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-00001250: 2020 2069 6620 646d 6c5f 6b65 7977 6f72     if dml_keywor
-00001260: 6420 6973 204e 6f6e 653a 0d0a 2020 2020  d is None:..    
-00001270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001280: 646d 6c5f 6b65 7977 6f72 6420 3d20 7374  dml_keyword = st
-00001290: 7228 746f 6b65 6e29 2e75 7070 6572 2829  r(token).upper()
-000012a0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000012b0: 2020 2020 2020 646d 6c5f 6b65 7977 6f72        dml_keywor
-000012c0: 645f 696e 6465 7820 3d20 690d 0a20 2020  d_index = i..   
-000012d0: 2020 2020 2020 2020 2065 6c69 6620 746f           elif to
-000012e0: 6b65 6e2e 7474 7970 6520 3d3d 2073 716c  ken.ttype == sql
-000012f0: 7061 7273 652e 746f 6b65 6e73 2e4b 6579  parse.tokens.Key
-00001300: 776f 7264 3a0d 0a20 2020 2020 2020 2020  word:..         
-00001310: 2020 2020 2020 2069 6620 6f72 6465 725f         if order_
-00001320: 6279 5f69 6e64 6578 2069 7320 4e6f 6e65  by_index is None
-00001330: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-00001340: 2020 2020 2020 206b 6579 776f 7264 203d         keyword =
-00001350: 2073 7472 2874 6f6b 656e 292e 7570 7065   str(token).uppe
-00001360: 7228 290d 0a20 2020 2020 2020 2020 2020  r()..           
-00001370: 2020 2020 2020 2020 2069 6620 6b65 7977           if keyw
-00001380: 6f72 6420 3d3d 2022 4f52 4445 5220 4259  ord == "ORDER BY
-00001390: 223a 0d0a 2020 2020 2020 2020 2020 2020  ":..            
-000013a0: 2020 2020 2020 2020 2020 2020 6f72 6465              orde
-000013b0: 725f 6279 5f69 6e64 6578 203d 2069 0d0a  r_by_index = i..
-000013c0: 0d0a 2020 2020 2020 2020 2320 4368 6563  ..        # Chec
-000013d0: 6b20 6966 2074 6865 2044 4d4c 206b 6579  k if the DML key
-000013e0: 776f 7264 2069 7320 5345 4c45 4354 0d0a  word is SELECT..
-000013f0: 2020 2020 2020 2020 6966 206e 6f74 2064          if not d
-00001400: 6d6c 5f6b 6579 776f 7264 3a0d 0a20 2020  ml_keyword:..   
-00001410: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-00001420: 616c 7565 4572 726f 7228 6622 6e6f 2053  alueError(f"no S
-00001430: 454c 4543 5420 666f 756e 6420 2871 7565  ELECT found (que
-00001440: 7279 2064 6f65 7320 6e6f 7420 636f 6e74  ry does not cont
-00001450: 6169 6e20 444d 4c20 6b65 7977 6f72 6429  ain DML keyword)
-00001460: 2229 0d0a 2020 2020 2020 2020 6966 2064  ")..        if d
-00001470: 6d6c 5f6b 6579 776f 7264 2021 3d20 2753  ml_keyword != 'S
-00001480: 454c 4543 5427 3a0d 0a20 2020 2020 2020  ELECT':..       
-00001490: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-000014a0: 4572 726f 7228 6622 6669 7273 7420 444d  Error(f"first DM
-000014b0: 4c20 6b65 7977 6f72 6420 6973 207b 646d  L keyword is {dm
-000014c0: 6c5f 6b65 7977 6f72 647d 2c20 6578 7065  l_keyword}, expe
-000014d0: 6374 6564 2053 454c 4543 5422 290d 0a0d  cted SELECT")...
-000014e0: 0a20 2020 2020 2020 2023 2047 6574 2070  .        # Get p
-000014f0: 6172 7420 6265 666f 7265 2053 454c 4543  art before SELEC
-00001500: 5420 2865 7861 6d70 6c65 3a20 5749 5448  T (example: WITH
-00001510: 290d 0a20 2020 2020 2020 2069 6620 646d  )..        if dm
-00001520: 6c5f 6b65 7977 6f72 645f 696e 6465 7820  l_keyword_index 
-00001530: 3e20 303a 0d0a 2020 2020 2020 2020 2020  > 0:..          
-00001540: 2020 746f 6b65 6e73 203d 2073 7461 7465    tokens = state
-00001550: 6d65 6e74 735b 305d 2e74 6f6b 656e 735b  ments[0].tokens[
-00001560: 3a64 6d6c 5f6b 6579 776f 7264 5f69 6e64  :dml_keyword_ind
-00001570: 6578 5d0d 0a20 2020 2020 2020 2020 2020  ex]..           
-00001580: 206c 696d 6974 6564 5f71 7565 7279 203d   limited_query =
-00001590: 2027 272e 6a6f 696e 2873 7472 2874 6f6b   ''.join(str(tok
-000015a0: 656e 2920 666f 7220 746f 6b65 6e20 696e  en) for token in
-000015b0: 2074 6f6b 656e 7329 0d0a 2020 2020 2020   tokens)..      
-000015c0: 2020 656c 7365 3a0d 0a20 2020 2020 2020    else:..       
-000015d0: 2020 2020 206c 696d 6974 6564 5f71 7565       limited_que
-000015e0: 7279 203d 2027 270d 0a20 2020 200d 0a20  ry = ''..    .. 
-000015f0: 2020 2020 2020 2023 2041 7070 656e 6420         # Append 
-00001600: 5345 4c45 4354 2062 6566 6f72 6520 4f52  SELECT before OR
-00001610: 4445 5220 4259 0d0a 2020 2020 2020 2020  DER BY..        
-00001620: 6966 206f 7264 6572 5f62 795f 696e 6465  if order_by_inde
-00001630: 7820 6973 206e 6f74 204e 6f6e 653a 0d0a  x is not None:..
-00001640: 2020 2020 2020 2020 2020 2020 746f 6b65              toke
-00001650: 6e73 203d 2073 7461 7465 6d65 6e74 735b  ns = statements[
-00001660: 305d 2e74 6f6b 656e 735b 646d 6c5f 6b65  0].tokens[dml_ke
-00001670: 7977 6f72 645f 696e 6465 783a 6f72 6465  yword_index:orde
-00001680: 725f 6279 5f69 6e64 6578 5d0d 0a20 2020  r_by_index]..   
-00001690: 2020 2020 2065 6c73 653a 0d0a 2020 2020       else:..    
-000016a0: 2020 2020 2020 2020 746f 6b65 6e73 203d          tokens =
-000016b0: 2073 7461 7465 6d65 6e74 735b 305d 2e74   statements[0].t
-000016c0: 6f6b 656e 735b 646d 6c5f 6b65 7977 6f72  okens[dml_keywor
-000016d0: 645f 696e 6465 783a 5d0d 0a0d 0a20 2020  d_index:]....   
-000016e0: 2020 2020 206c 696d 6974 6564 5f71 7565       limited_que
-000016f0: 7279 202b 3d20 7365 6c66 2e5f 6c69 6d69  ry += self._limi
-00001700: 745f 7061 7273 6564 5f71 7565 7279 2827  t_parsed_query('
-00001710: 272e 6a6f 696e 2873 7472 2874 6f6b 656e  '.join(str(token
-00001720: 2920 666f 7220 746f 6b65 6e20 696e 2074  ) for token in t
-00001730: 6f6b 656e 7329 2c20 6c69 6d69 743d 6c69  okens), limit=li
-00001740: 6d69 7429 0d0a 0d0a 2020 2020 2020 2020  mit)....        
-00001750: 2320 4170 7065 6e64 204f 5244 4552 2042  # Append ORDER B
-00001760: 590d 0a20 2020 2020 2020 2069 6620 6f72  Y..        if or
-00001770: 6465 725f 6279 5f69 6e64 6578 2069 7320  der_by_index is 
-00001780: 6e6f 7420 4e6f 6e65 3a0d 0a20 2020 2020  not None:..     
-00001790: 2020 2020 2020 2074 6f6b 656e 7320 3d20         tokens = 
-000017a0: 7374 6174 656d 656e 7473 5b30 5d2e 746f  statements[0].to
-000017b0: 6b65 6e73 5b6f 7264 6572 5f62 795f 696e  kens[order_by_in
-000017c0: 6465 783a 5d0d 0a20 2020 2020 2020 2020  dex:]..         
-000017d0: 2020 206c 696d 6974 6564 5f71 7565 7279     limited_query
-000017e0: 202b 3d20 275c 6e27 202b 2027 272e 6a6f   += '\n' + ''.jo
-000017f0: 696e 2873 7472 2874 6f6b 656e 2920 666f  in(str(token) fo
-00001800: 7220 746f 6b65 6e20 696e 2074 6f6b 656e  r token in token
-00001810: 7329 0d0a 0d0a 2020 2020 2020 2020 7265  s)....        re
-00001820: 7475 726e 206c 696d 6974 6564 5f71 7565  turn limited_que
-00001830: 7279 0d0a 2020 2020 0d0a 0d0a 2020 2020  ry..    ....    
-00001840: 6465 6620 5f6c 696d 6974 5f70 6172 7365  def _limit_parse
-00001850: 645f 7175 6572 7928 7365 6c66 2c20 7175  d_query(self, qu
-00001860: 6572 793a 2073 7472 2c20 6c69 6d69 743a  ery: str, limit:
-00001870: 2069 6e74 2920 2d3e 2073 7472 3a0d 0a20   int) -> str:.. 
-00001880: 2020 2020 2020 2072 6169 7365 204e 6f74         raise Not
-00001890: 496d 706c 656d 656e 7465 6445 7272 6f72  ImplementedError
-000018a0: 2829 0d0a 0d0a 0d0a 2020 2020 6465 6620  ()......    def 
-000018b0: 6275 696c 645f 7175 6572 795f 7769 7468  build_query_with
-000018c0: 5f70 6f73 6974 696f 6e61 6c5f 7061 7261  _positional_para
-000018d0: 6d73 2873 656c 662c 2071 7565 7279 3a20  ms(self, query: 
-000018e0: 7374 722c 2070 6172 616d 733a 206c 6973  str, params: lis
-000018f0: 747c 7475 706c 657c 6469 6374 293a 0d0a  t|tuple|dict):..
-00001900: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-00001910: 7461 6e63 6528 7061 7261 6d73 2c20 6469  tance(params, di
-00001920: 6374 293a 0d0a 2020 2020 2020 2020 2020  ct):..          
-00001930: 2020 6672 6f6d 2073 716c 7061 7261 6d73    from sqlparams
-00001940: 2069 6d70 6f72 7420 5351 4c50 6172 616d   import SQLParam
-00001950: 7320 2320 6e6f 7420 6174 2074 6865 2074  s # not at the t
-00001960: 6f70 2062 6563 6175 7365 2074 6865 2065  op because the e
-00001970: 6e64 7573 6572 206d 6967 6874 206e 6f74  nduser might not
-00001980: 206e 6565 6420 7468 6973 2066 6561 7475   need this featu
-00001990: 7265 0d0a 0d0a 2020 2020 2020 2020 2020  re....          
-000019a0: 2020 6966 206e 6f74 2068 6173 6174 7472    if not hasattr
-000019b0: 2873 656c 662e 5f5f 636c 6173 735f 5f2c  (self.__class__,
-000019c0: 2027 5f70 6172 616d 735f 666f 726d 6174   '_params_format
-000019d0: 7465 7227 293a 0d0a 2020 2020 2020 2020  ter'):..        
-000019e0: 2020 2020 2020 2020 7365 6c66 2e5f 5f63          self.__c
-000019f0: 6c61 7373 5f5f 2e5f 7061 7261 6d73 5f66  lass__._params_f
-00001a00: 6f72 6d61 7474 6572 203d 2053 514c 5061  ormatter = SQLPa
-00001a10: 7261 6d73 2827 6e61 6d65 6427 2c20 2771  rams('named', 'q
-00001a20: 6d61 726b 2729 0d0a 2020 2020 2020 2020  mark')..        
-00001a30: 2020 2020 7175 6572 792c 2070 6172 616d      query, param
-00001a40: 7320 3d20 7365 6c66 2e5f 5f63 6c61 7373  s = self.__class
-00001a50: 5f5f 2e5f 7061 7261 6d73 5f66 6f72 6d61  __._params_forma
-00001a60: 7474 6572 2e66 6f72 6d61 7428 7175 6572  tter.format(quer
-00001a70: 792c 2070 6172 616d 7329 0d0a 0d0a 2020  y, params)....  
-00001a80: 2020 2020 2020 7265 7475 726e 2071 7565        return que
-00001a90: 7279 2c20 7061 7261 6d73 0d0a 2020 2020  ry, params..    
-00001aa0: 2020 2020 0d0a 0d0a 2020 2020 6465 6620      ....    def 
-00001ab0: 6765 745f 6375 7273 6f72 5f63 6f6c 756d  get_cursor_colum
-00001ac0: 6e73 2873 656c 662c 2063 7572 736f 723a  ns(self, cursor:
-00001ad0: 2054 5f43 7572 736f 7229 202d 3e20 6c69   T_Cursor) -> li
-00001ae0: 7374 5b43 6f6c 756d 6e5d 3a0d 0a20 2020  st[Column]:..   
-00001af0: 2020 2020 2063 6f6c 756d 6e73 203d 205b       columns = [
-00001b00: 5d0d 0a0d 0a20 2020 2020 2020 2066 6f72  ]....        for
-00001b10: 2069 2c20 696e 666f 2069 6e20 656e 756d   i, info in enum
-00001b20: 6572 6174 6528 6375 7273 6f72 2e64 6573  erate(cursor.des
-00001b30: 6372 6970 7469 6f6e 293a 0d0a 2020 2020  cription):..    
-00001b40: 2020 2020 2020 2020 6e61 6d65 2c20 7079          name, py
-00001b50: 7468 6f6e 5f74 7970 652c 2064 6973 706c  thon_type, displ
-00001b60: 6179 5f73 697a 652c 2069 6e74 6572 6e61  ay_size, interna
-00001b70: 6c5f 7369 7a65 2c20 7072 6563 6973 696f  l_size, precisio
-00001b80: 6e2c 2073 6361 6c65 2c20 6e75 6c6c 5f6f  n, scale, null_o
-00001b90: 6b20 3d20 696e 666f 0d0a 2020 2020 2020  k = info..      
-00001ba0: 2020 2020 2020 636f 6c75 6d6e 203d 2043        column = C
-00001bb0: 6f6c 756d 6e28 6e61 6d65 2c20 706f 7369  olumn(name, posi
-00001bc0: 7469 6f6e 3d69 202b 2031 2c20 7079 7468  tion=i + 1, pyth
-00001bd0: 6f6e 5f74 7970 653d 7079 7468 6f6e 5f74  on_type=python_t
-00001be0: 7970 652c 2064 6973 706c 6179 5f73 697a  ype, display_siz
-00001bf0: 653d 6469 7370 6c61 795f 7369 7a65 2c20  e=display_size, 
-00001c00: 696e 7465 726e 616c 5f73 697a 653d 696e  internal_size=in
-00001c10: 7465 726e 616c 5f73 697a 652c 2070 7265  ternal_size, pre
-00001c20: 6369 7369 6f6e 3d70 7265 6369 7369 6f6e  cision=precision
-00001c30: 2c20 7363 616c 653d 7363 616c 652c 206e  , scale=scale, n
-00001c40: 756c 6c5f 6f6b 3d6e 756c 6c5f 6f6b 290d  ull_ok=null_ok).
-00001c50: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00001c60: 662e 5f66 6978 5f63 7572 736f 725f 636f  f._fix_cursor_co
-00001c70: 6c75 6d6e 5f64 6566 696e 6974 696f 6e28  lumn_definition(
-00001c80: 636f 6c75 6d6e 290d 0a20 2020 2020 2020  column)..       
-00001c90: 2020 2020 2063 6f6c 756d 6e2e 5f63 6865       column._che
-00001ca0: 636b 2829 0d0a 2020 2020 2020 2020 2020  ck()..          
-00001cb0: 2020 636f 6c75 6d6e 732e 6170 7065 6e64    columns.append
-00001cc0: 2863 6f6c 756d 6e29 0d0a 2020 2020 2020  (column)..      
-00001cd0: 2020 0d0a 2020 2020 2020 2020 7265 7475    ..        retu
-00001ce0: 726e 2063 6f6c 756d 6e73 0d0a 0d0a 0d0a  rn columns......
-00001cf0: 2020 2020 6465 6620 5f66 6978 5f63 7572      def _fix_cur
-00001d00: 736f 725f 636f 6c75 6d6e 5f64 6566 696e  sor_column_defin
-00001d10: 6974 696f 6e28 7365 6c66 2c20 636f 6c75  ition(self, colu
-00001d20: 6d6e 3a20 436f 6c75 6d6e 293a 0d0a 2020  mn: Column):..  
-00001d30: 2020 2020 2020 2222 220d 0a20 2020 2020        """..     
-00001d40: 2020 2046 6978 2061 2063 6f6c 756d 6e20     Fix a column 
-00001d50: 6465 6669 6e69 7469 6f6e 2c20 6166 7465  definition, afte
-00001d60: 7220 696e 7374 616e 6369 6174 696f 6e20  r instanciation 
-00001d70: 696e 2060 6765 745f 636f 6c75 6d6e 735f  in `get_columns_
-00001d80: 6672 6f6d 5f63 7572 736f 7260 2c20 666f  from_cursor`, fo
-00001d90: 7220 6120 7370 6563 6966 6963 2064 6174  r a specific dat
-00001da0: 6162 6173 6520 656e 6769 6e65 2e0d 0a20  abase engine... 
-00001db0: 2020 2020 2020 2022 2222 0d0a 2020 2020         """..    
-00001dc0: 2020 2020 7061 7373 0d0a 2020 2020 2020      pass..      
-00001dd0: 2020 0d0a 0d0a 2020 2020 6465 6620 6765    ....    def ge
-00001de0: 745f 6375 7273 6f72 5f63 6f6c 756d 6e5f  t_cursor_column_
-00001df0: 6e61 6d65 7328 7365 6c66 2c20 6375 7273  names(self, curs
-00001e00: 6f72 3a20 545f 4375 7273 6f72 2920 2d3e  or: T_Cursor) ->
-00001e10: 206c 6973 745b 7374 725d 3a0d 0a20 2020   list[str]:..   
-00001e20: 2020 2020 2072 6574 7572 6e20 5b69 6e66       return [inf
-00001e30: 6f5b 305d 2066 6f72 2069 6e66 6f20 696e  o[0] for info in
-00001e40: 2063 7572 736f 722e 6465 7363 7269 7074   cursor.descript
-00001e50: 696f 6e5d 0d0a 2020 2020 2020 2020 0d0a  ion]..        ..
-00001e60: 0d0a 2020 2020 6465 6620 6765 745f 7461  ..    def get_ta
-00001e70: 626c 655f 636f 6c75 6d6e 7328 7365 6c66  ble_columns(self
-00001e80: 2c20 7461 626c 653a 2073 7472 7c74 7570  , table: str|tup
-00001e90: 6c65 2920 2d3e 206c 6973 745b 436f 6c75  le) -> list[Colu
-00001ea0: 6d6e 5d3a 0d0a 2020 2020 2020 2020 7175  mn]:..        qu
-00001eb0: 6572 7920 3d20 7365 6c66 2e67 6574 5f73  ery = self.get_s
-00001ec0: 656c 6563 745f 7461 626c 655f 7175 6572  elect_table_quer
-00001ed0: 7928 7461 626c 652c 2073 6368 656d 615f  y(table, schema_
-00001ee0: 6f6e 6c79 3d54 7275 6529 0d0a 2020 2020  only=True)..    
-00001ef0: 2020 2020 7769 7468 2073 656c 662e 6578      with self.ex
-00001f00: 6563 7574 655f 6765 745f 6375 7273 6f72  ecute_get_cursor
-00001f10: 2871 7565 7279 2920 6173 2063 7572 736f  (query) as curso
-00001f20: 723a 0d0a 2020 2020 2020 2020 2020 2020  r:..            
-00001f30: 7265 7475 726e 2073 656c 662e 6765 745f  return self.get_
-00001f40: 6375 7273 6f72 5f63 6f6c 756d 6e73 2863  cursor_columns(c
-00001f50: 7572 736f 7229 0d0a 0d0a 0d0a 2020 2020  ursor)......    
-00001f60: 6465 6620 6765 745f 7461 626c 655f 636f  def get_table_co
-00001f70: 6c75 6d6e 5f6e 616d 6573 2873 656c 662c  lumn_names(self,
-00001f80: 2074 6162 6c65 3a20 7374 727c 7475 706c   table: str|tupl
-00001f90: 6529 202d 3e20 6c69 7374 5b73 7472 5d3a  e) -> list[str]:
-00001fa0: 0d0a 2020 2020 2020 2020 636f 6c75 6d6e  ..        column
-00001fb0: 5f6e 616d 6573 203d 205b 5d0d 0a20 2020  _names = []..   
-00001fc0: 2020 2020 2066 6f72 2063 6f6c 756d 6e20       for column 
-00001fd0: 696e 2073 656c 662e 6765 745f 7461 626c  in self.get_tabl
-00001fe0: 655f 636f 6c75 6d6e 7328 7461 626c 6529  e_columns(table)
-00001ff0: 3a0d 0a20 2020 2020 2020 2020 2020 2063  :..            c
-00002000: 6f6c 756d 6e5f 6e61 6d65 732e 6170 7065  olumn_names.appe
-00002010: 6e64 2863 6f6c 756d 6e2e 6e61 6d65 290d  nd(column.name).
-00002020: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-00002030: 636f 6c75 6d6e 5f6e 616d 6573 0d0a 0d0a  column_names....
-00002040: 0d0a 2020 2020 4063 6c61 7373 6d65 7468  ..    @classmeth
-00002050: 6f64 0d0a 2020 2020 6465 6620 7370 6c69  od..    def spli
-00002060: 745f 6e61 6d65 2863 6c73 2c20 6675 6c6c  t_name(cls, full
-00002070: 5f6e 616d 653a 2073 7472 7c74 7570 6c65  _name: str|tuple
-00002080: 2920 2d3e 2074 7570 6c65 5b73 7472 2c73  ) -> tuple[str,s
-00002090: 7472 5d3a 0d0a 2020 2020 2020 2020 6966  tr]:..        if
-000020a0: 2069 7369 6e73 7461 6e63 6528 6675 6c6c   isinstance(full
-000020b0: 5f6e 616d 652c 2074 7570 6c65 293a 0d0a  _name, tuple):..
-000020c0: 2020 2020 2020 2020 2020 2020 7265 7475              retu
-000020d0: 726e 2066 756c 6c5f 6e61 6d65 0d0a 2020  rn full_name..  
-000020e0: 2020 2020 2020 0d0a 2020 2020 2020 2020        ..        
-000020f0: 7472 793a 0d0a 2020 2020 2020 2020 2020  try:..          
-00002100: 2020 706f 7320 3d20 6675 6c6c 5f6e 616d    pos = full_nam
-00002110: 652e 696e 6465 7828 272e 2729 0d0a 2020  e.index('.')..  
-00002120: 2020 2020 2020 2020 2020 7363 6865 6d61            schema
-00002130: 5f6e 616d 6520 3d20 6675 6c6c 5f6e 616d  _name = full_nam
-00002140: 655b 303a 706f 735d 0d0a 2020 2020 2020  e[0:pos]..      
-00002150: 2020 2020 2020 6e61 6d65 203d 2066 756c        name = ful
-00002160: 6c5f 6e61 6d65 5b70 6f73 2b31 3a5d 0d0a  l_name[pos+1:]..
-00002170: 2020 2020 2020 2020 6578 6365 7074 2056          except V
-00002180: 616c 7565 4572 726f 723a 0d0a 2020 2020  alueError:..    
-00002190: 2020 2020 2020 2020 7363 6865 6d61 5f6e          schema_n
-000021a0: 616d 6520 3d20 636c 732e 6465 6661 756c  ame = cls.defaul
-000021b0: 745f 7363 6865 6d61 5f6e 616d 650d 0a20  t_schema_name.. 
-000021c0: 2020 2020 2020 2020 2020 206e 616d 6520             name 
-000021d0: 3d20 6675 6c6c 5f6e 616d 650d 0a0d 0a20  = full_name.... 
-000021e0: 2020 2020 2020 2072 6574 7572 6e20 2873         return (s
-000021f0: 6368 656d 615f 6e61 6d65 2c20 6e61 6d65  chema_name, name
-00002200: 290d 0a20 2020 200d 0a0d 0a20 2020 2064  )..    ....    d
-00002210: 6566 2067 6574 5f73 656c 6563 745f 7461  ef get_select_ta
-00002220: 626c 655f 7175 6572 7928 7365 6c66 2c20  ble_query(self, 
-00002230: 7461 626c 653a 2073 7472 7c74 7570 6c65  table: str|tuple
-00002240: 2c20 7363 6865 6d61 5f6f 6e6c 7920 3d20  , schema_only = 
-00002250: 4661 6c73 6529 3a0d 0a20 2020 2020 2020  False):..       
-00002260: 2022 2222 0d0a 2020 2020 2020 2020 4275   """..        Bu
-00002270: 696c 6420 6120 7175 6572 7920 6f6e 2074  ild a query on t
-00002280: 6865 2067 6976 656e 2074 6162 6c65 2e0d  he given table..
-00002290: 0a0d 0a20 2020 2020 2020 2049 6620 6073  ...        If `s
-000022a0: 6368 656d 615f 6f6e 6c79 6020 6973 2067  chema_only` is g
-000022b0: 6976 656e 2c20 6e6f 2072 6f77 2077 696c  iven, no row wil
-000022c0: 6c20 6265 2072 6574 7572 6e65 6420 2874  l be returned (t
-000022d0: 6869 7320 6973 2075 7365 6420 746f 2067  his is used to g
-000022e0: 6574 2069 6e66 6f72 6d61 7469 6f6e 206f  et information o
-000022f0: 6e20 7468 6520 7461 626c 6529 2e0d 0a20  n the table)... 
-00002300: 2020 2020 2020 204f 7468 6572 7769 7365         Otherwise
-00002310: 2c20 616c 6c20 726f 7773 2077 696c 6c20  , all rows will 
-00002320: 6265 2072 6574 7572 6e65 642e 0d0a 0d0a  be returned.....
-00002330: 2020 2020 2020 2020 5468 6520 7265 7475          The retu
-00002340: 726e 2074 7970 6520 6f66 2074 6869 7320  rn type of this 
-00002350: 6675 6e63 7469 6f6e 2064 6570 656e 6473  function depends
-00002360: 206f 6e20 7468 6520 6461 7461 6261 7365   on the database
-00002370: 2065 6e67 696e 652e 0d0a 2020 2020 2020   engine...      
-00002380: 2020 4974 2069 7320 7061 7373 6564 2064    It is passed d
-00002390: 6972 6563 746c 7920 746f 2074 6865 2063  irectly to the c
-000023a0: 7572 736f 7227 7320 6578 6563 7574 6520  ursor's execute 
-000023b0: 6675 6e63 7469 6f6e 2066 6f72 2074 6869  function for thi
-000023c0: 7320 656e 6769 6e65 2e0d 0a20 2020 2020  s engine...     
-000023d0: 2020 2022 2222 0d0a 2020 2020 2020 2020     """..        
-000023e0: 7261 6973 6520 4e6f 7449 6d70 6c65 6d65  raise NotImpleme
-000023f0: 6e74 6564 4572 726f 7228 290d 0a20 2020  ntedError()..   
-00002400: 200d 0a0d 0a20 2020 2064 6566 2065 7865   ....    def exe
-00002410: 6375 7465 5f67 6574 5f63 7572 736f 7228  cute_get_cursor(
-00002420: 7365 6c66 2c20 7175 6572 793a 2073 7472  self, query: str
-00002430: 2c20 7061 7261 6d73 3a20 6c69 7374 7c74  , params: list|t
-00002440: 7570 6c65 7c64 6963 7420 3d20 4e6f 6e65  uple|dict = None
-00002450: 2c20 6c69 6d69 743a 2069 6e74 203d 204e  , limit: int = N
-00002460: 6f6e 6529 202d 3e20 545f 4375 7273 6f72  one) -> T_Cursor
-00002470: 3a0d 0a20 2020 2020 2020 2022 2222 0d0a  :..        """..
-00002480: 2020 2020 2020 2020 4d75 7374 2062 6520          Must be 
-00002490: 636c 6f73 6564 2e0d 0a20 2020 2020 2020  closed...       
-000024a0: 2022 2222 0d0a 2020 2020 2020 2020 7261   """..        ra
-000024b0: 6973 6520 4e6f 7449 6d70 6c65 6d65 6e74  ise NotImplement
-000024c0: 6564 4572 726f 7228 290d 0a20 2020 200d  edError()..    .
-000024d0: 0a0d 0a20 2020 2064 6566 2065 7865 6375  ...    def execu
-000024e0: 7465 5f67 6574 5f73 6361 6c61 7228 7365  te_get_scalar(se
-000024f0: 6c66 2c20 7175 6572 793a 2073 7472 2c20  lf, query: str, 
-00002500: 7061 7261 6d73 3a20 6c69 7374 7c74 7570  params: list|tup
-00002510: 6c65 7c64 6963 7420 3d20 4e6f 6e65 2c20  le|dict = None, 
-00002520: 6c69 6d69 743a 2069 6e74 203d 204e 6f6e  limit: int = Non
-00002530: 6529 3a0d 0a20 2020 2020 2020 2077 6974  e):..        wit
-00002540: 6820 7365 6c66 2e65 7865 6375 7465 5f67  h self.execute_g
-00002550: 6574 5f63 7572 736f 7228 7175 6572 792c  et_cursor(query,
-00002560: 2070 6172 616d 732c 206c 696d 6974 3d6c   params, limit=l
-00002570: 696d 6974 2920 6173 2063 7572 736f 723a  imit) as cursor:
-00002580: 0d0a 2020 2020 2020 2020 2020 2020 7265  ..            re
-00002590: 7375 6c74 203d 206e 6578 7428 6375 7273  sult = next(curs
-000025a0: 6f72 290d 0a20 2020 2020 2020 2020 2020  or)..           
-000025b0: 200d 0a20 2020 2020 2020 2020 2020 2023   ..            #
-000025c0: 2043 6865 636b 206f 6e6c 7920 6f6e 6520   Check only one 
-000025d0: 726f 770d 0a20 2020 2020 2020 2020 2020  row..           
-000025e0: 2074 7279 3a0d 0a20 2020 2020 2020 2020   try:..         
-000025f0: 2020 2020 2020 206e 6578 7428 6375 7273         next(curs
-00002600: 6f72 290d 0a20 2020 2020 2020 2020 2020  or)..           
-00002610: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-00002620: 4572 726f 7228 6622 7365 7665 7261 6c20  Error(f"several 
-00002630: 726f 7773 2072 6574 7572 6e65 6420 6279  rows returned by
-00002640: 2071 7565 7279 2229 0d0a 2020 2020 2020   query")..      
-00002650: 2020 2020 2020 6578 6365 7074 2053 746f        except Sto
-00002660: 7049 7465 7261 7469 6f6e 3a0d 0a20 2020  pIteration:..   
-00002670: 2020 2020 2020 2020 2020 2020 2070 6173               pas
-00002680: 730d 0a0d 0a20 2020 2020 2020 2020 2020  s....           
-00002690: 2023 2043 6865 636b 206f 6e6c 7920 6f6e   # Check only on
-000026a0: 6520 7661 6c75 650d 0a20 2020 2020 2020  e value..       
-000026b0: 2020 2020 2069 6620 6c65 6e28 7265 7375       if len(resu
-000026c0: 6c74 2920 3e20 313a 0d0a 2020 2020 2020  lt) > 1:..      
-000026d0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-000026e0: 5661 6c75 6545 7272 6f72 2822 7365 7665  ValueError("seve
-000026f0: 7261 6c20 7661 6c75 6573 2072 6574 7572  ral values retur
-00002700: 6e65 6420 6279 2071 7565 7279 2229 0d0a  ned by query")..
-00002710: 0d0a 2020 2020 2020 2020 2020 2020 7265  ..            re
-00002720: 7475 726e 2072 6573 756c 745b 305d 0d0a  turn result[0]..
-00002730: 0d0a 0d0a 2020 2020 6465 6620 6578 6563  ....    def exec
-00002740: 7574 6528 7365 6c66 2c20 7175 6572 793a  ute(self, query:
-00002750: 2073 7472 2c20 7061 7261 6d73 3a20 6c69   str, params: li
-00002760: 7374 7c74 7570 6c65 7c64 6963 7420 3d20  st|tuple|dict = 
-00002770: 4e6f 6e65 2c20 6c69 6d69 743a 2069 6e74  None, limit: int
-00002780: 203d 204e 6f6e 6529 3a0d 0a20 2020 2020   = None):..     
-00002790: 2020 2077 6974 6820 7365 6c66 2e65 7865     with self.exe
-000027a0: 6375 7465 5f67 6574 5f63 7572 736f 7228  cute_get_cursor(
-000027b0: 7175 6572 792c 2070 6172 616d 732c 206c  query, params, l
-000027c0: 696d 6974 3d6c 696d 6974 293a 0d0a 2020  imit=limit):..  
-000027d0: 2020 2020 2020 2020 2020 7061 7373 0d0a            pass..
-000027e0: 0d0a 0d0a 2020 2020 6465 6620 6578 6563  ....    def exec
-000027f0: 7574 655f 6669 6c65 2873 656c 662c 2070  ute_file(self, p
-00002800: 6174 683a 2073 7472 7c50 6174 682c 2070  ath: str|Path, p
-00002810: 6172 616d 733a 206c 6973 747c 7475 706c  arams: list|tupl
-00002820: 657c 6469 6374 203d 204e 6f6e 652c 206c  e|dict = None, l
-00002830: 696d 6974 3a20 696e 7420 3d20 4e6f 6e65  imit: int = None
-00002840: 293a 0d0a 2020 2020 2020 2020 6966 206e  ):..        if n
-00002850: 6f74 2069 7369 6e73 7461 6e63 6528 7061  ot isinstance(pa
-00002860: 7468 2c20 5061 7468 293a 0d0a 2020 2020  th, Path):..    
-00002870: 2020 2020 2020 2020 7061 7468 203d 2050          path = P
-00002880: 6174 6828 7061 7468 290d 0a20 2020 2020  ath(path)..     
-00002890: 2020 2020 2020 200d 0a20 2020 2020 2020         ..       
-000028a0: 2071 7565 7279 203d 2070 6174 682e 7265   query = path.re
-000028b0: 6164 5f74 6578 7428 656e 636f 6469 6e67  ad_text(encoding
-000028c0: 3d27 7574 662d 3827 290d 0a20 2020 2020  ='utf-8')..     
-000028d0: 2020 2073 656c 662e 6578 6563 7574 6528     self.execute(
-000028e0: 7175 6572 792c 2070 6172 616d 732c 206c  query, params, l
-000028f0: 696d 6974 3d6c 696d 6974 290d 0a0d 0a0d  imit=limit).....
-00002900: 0a20 2020 2064 6566 2063 616c 6c5f 7072  .    def call_pr
-00002910: 6f63 6564 7572 6528 7365 6c66 2c20 6e61  ocedure(self, na
-00002920: 6d65 3a20 7374 727c 7475 706c 652c 202a  me: str|tuple, *
-00002930: 6172 6773 293a 0d0a 2020 2020 2020 2020  args):..        
-00002940: 7261 6973 6520 4e6f 7449 6d70 6c65 6d65  raise NotImpleme
-00002950: 6e74 6564 4572 726f 7228 290d 0a0d 0a0d  ntedError().....
-00002960: 0a20 2020 2064 6566 2074 6162 6c65 5f65  .    def table_e
-00002970: 7869 7374 7328 7365 6c66 2c20 7461 626c  xists(self, tabl
-00002980: 653a 2073 7472 7c74 7570 6c65 2920 2d3e  e: str|tuple) ->
-00002990: 2062 6f6f 6c3a 0d0a 2020 2020 2020 2020   bool:..        
-000029a0: 7261 6973 6520 4e6f 7449 6d70 6c65 6d65  raise NotImpleme
-000029b0: 6e74 6564 4572 726f 7228 290d 0a0d 0a0d  ntedError().....
-000029c0: 0a20 2020 2064 6566 2074 7275 6e63 6174  .    def truncat
-000029d0: 655f 7461 626c 6528 7365 6c66 2c20 7461  e_table(self, ta
-000029e0: 626c 653a 2073 7472 7c74 7570 6c65 293a  ble: str|tuple):
-000029f0: 0d0a 2020 2020 2020 2020 7261 6973 6520  ..        raise 
-00002a00: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr
-00002a10: 726f 7228 290d 0a0d 0a0d 0a20 2020 2064  ror()......    d
-00002a20: 6566 2063 6f70 795f 6672 6f6d 5f63 7376  ef copy_from_csv
-00002a30: 2873 656c 662c 2066 703a 2049 4f42 6173  (self, fp: IOBas
-00002a40: 652c 2074 6162 6c65 3a20 7374 727c 7475  e, table: str|tu
-00002a50: 706c 652c 2063 6f6c 756d 6e73 3a20 6c69  ple, columns: li
-00002a60: 7374 5b73 7472 5d20 3d20 4e6f 6e65 2c20  st[str] = None, 
-00002a70: 6465 6c69 6d69 7465 723a 2073 7472 203d  delimiter: str =
-00002a80: 204e 6f6e 652c 2071 756f 7465 6368 6172   None, quotechar
-00002a90: 3a20 7374 7220 3d20 2722 272c 206e 756c  : str = '"', nul
-00002aa0: 6c63 6861 723a 2073 7472 203d 2027 272c  lchar: str = '',
-00002ab0: 206e 6f68 6561 6465 723a 2062 6f6f 6c20   noheader: bool 
-00002ac0: 3d20 4661 6c73 6529 3a0d 0a20 2020 2020  = False):..     
-00002ad0: 2020 2072 6169 7365 204e 6f74 496d 706c     raise NotImpl
-00002ae0: 656d 656e 7465 6445 7272 6f72 2829 0d0a  ementedError()..
-00002af0: 2020 2020 0d0a 0d0a 2020 2020 6465 6620      ....    def 
-00002b00: 6465 706c 6f79 5f73 716c 2873 656c 662c  deploy_sql(self,
-00002b10: 202a 7061 7468 733a 2050 6174 687c 7374   *paths: Path|st
-00002b20: 722c 2065 6e63 6f64 696e 6720 3d20 2275  r, encoding = "u
-00002b30: 7466 2d38 2229 3a0d 0a20 2020 2020 2020  tf-8"):..       
-00002b40: 2061 6374 7561 6c5f 7061 7468 733a 206c   actual_paths: l
-00002b50: 6973 745b 5061 7468 5d20 3d20 5b5d 0d0a  ist[Path] = []..
-00002b60: 2020 2020 2020 2020 666f 7220 7061 7468          for path
-00002b70: 2069 6e20 7061 7468 733a 0d0a 2020 2020   in paths:..    
-00002b80: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-00002b90: 7461 6e63 6528 7061 7468 2c20 7374 7229  tance(path, str)
-00002ba0: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
-00002bb0: 2020 2070 6174 6820 3d20 5061 7468 2870     path = Path(p
-00002bc0: 6174 6829 0d0a 2020 2020 2020 2020 2020  ath)..          
-00002bd0: 2020 6163 7475 616c 5f70 6174 6873 2e61    actual_paths.a
-00002be0: 7070 656e 6428 7061 7468 290d 0a0d 0a20  ppend(path).... 
-00002bf0: 2020 2020 2020 2061 6374 7561 6c5f 7061         actual_pa
-00002c00: 7468 732e 736f 7274 2829 0d0a 0d0a 2020  ths.sort()....  
-00002c10: 2020 2020 2020 666f 7220 7061 7468 2069        for path i
-00002c20: 6e20 6163 7475 616c 5f70 6174 6873 3a0d  n actual_paths:.
-00002c30: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00002c40: 7061 7468 2e69 735f 6469 7228 293a 0d0a  path.is_dir():..
-00002c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002c60: 7375 6270 6174 6873 203d 2073 6f72 7465  subpaths = sorte
-00002c70: 6428 7061 7468 2e69 7465 7264 6972 2829  d(path.iterdir()
-00002c80: 290d 0a20 2020 2020 2020 2020 2020 2020  )..             
-00002c90: 2020 2073 656c 662e 6465 706c 6f79 5f73     self.deploy_s
-00002ca0: 716c 282a 7375 6270 6174 6873 2c20 656e  ql(*subpaths, en
-00002cb0: 636f 6469 6e67 3d65 6e63 6f64 696e 6729  coding=encoding)
-00002cc0: 0d0a 0d0a 2020 2020 2020 2020 2020 2020  ....            
-00002cd0: 656c 6966 206e 6f74 2070 6174 682e 6e61  elif not path.na
-00002ce0: 6d65 2e65 6e64 7377 6974 6828 222e 7371  me.endswith(".sq
-00002cf0: 6c22 293a 0d0a 2020 2020 2020 2020 2020  l"):..          
-00002d00: 2020 2020 2020 636f 6e74 696e 7565 2023        continue #
-00002d10: 2069 676e 6f72 650d 0a0d 0a20 2020 2020   ignore....     
-00002d20: 2020 2020 2020 2065 6c69 6620 7061 7468         elif path
-00002d30: 2e6e 616d 652e 656e 6473 7769 7468 2822  .name.endswith("
-00002d40: 5f72 6576 6572 742e 7371 6c22 293a 0d0a  _revert.sql"):..
-00002d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002d60: 636f 6e74 696e 7565 2023 2069 676e 6f72  continue # ignor
-00002d70: 650d 0a0d 0a20 2020 2020 2020 2020 2020  e....           
-00002d80: 2065 6c73 653a 0d0a 2020 2020 2020 2020   else:..        
-00002d90: 2020 2020 2020 2020 6c6f 6767 6572 2e69          logger.i
-00002da0: 6e66 6f28 2265 7865 6375 7465 2025 7322  nfo("execute %s"
-00002db0: 2c20 7061 7468 290d 0a20 2020 2020 2020  , path)..       
-00002dc0: 2020 2020 2020 2020 2071 7565 7279 203d           query =
-00002dd0: 2070 6174 682e 7265 6164 5f74 6578 7428   path.read_text(
-00002de0: 656e 636f 6469 6e67 3d65 6e63 6f64 696e  encoding=encodin
-00002df0: 6729 0d0a 2020 2020 2020 2020 2020 2020  g)..            
-00002e00: 2020 2020 7365 6c66 2e65 7865 6375 7465      self.execute
-00002e10: 2871 7565 7279 290d 0a0d 0a0d 0a20 2020  (query)......   
-00002e20: 2064 6566 2072 6576 6572 745f 7371 6c28   def revert_sql(
-00002e30: 7365 6c66 2c20 2a70 6174 6873 3a20 5061  self, *paths: Pa
-00002e40: 7468 7c73 7472 2c20 656e 636f 6469 6e67  th|str, encoding
-00002e50: 203d 2022 7574 662d 3822 293a 0d0a 2020   = "utf-8"):..  
-00002e60: 2020 2020 2020 6163 7475 616c 5f70 6174        actual_pat
-00002e70: 6873 3a20 6c69 7374 5b50 6174 685d 203d  hs: list[Path] =
-00002e80: 205b 5d0d 0a20 2020 2020 2020 2066 6f72   []..        for
-00002e90: 2070 6174 6820 696e 2070 6174 6873 3a0d   path in paths:.
-00002ea0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00002eb0: 6973 696e 7374 616e 6365 2870 6174 682c  isinstance(path,
-00002ec0: 2073 7472 293a 0d0a 2020 2020 2020 2020   str):..        
-00002ed0: 2020 2020 2020 2020 7061 7468 203d 2050          path = P
-00002ee0: 6174 6828 7061 7468 290d 0a20 2020 2020  ath(path)..     
-00002ef0: 2020 2020 2020 2061 6374 7561 6c5f 7061         actual_pa
-00002f00: 7468 732e 6170 7065 6e64 2870 6174 6829  ths.append(path)
-00002f10: 0d0a 0d0a 2020 2020 2020 2020 6163 7475  ....        actu
-00002f20: 616c 5f70 6174 6873 2e73 6f72 7428 7265  al_paths.sort(re
-00002f30: 7665 7273 653d 5472 7565 290d 0a0d 0a20  verse=True).... 
-00002f40: 2020 2020 2020 2066 6f72 2070 6174 6820         for path 
-00002f50: 696e 2061 6374 7561 6c5f 7061 7468 733a  in actual_paths:
-00002f60: 0d0a 2020 2020 2020 2020 2020 2020 6966  ..            if
-00002f70: 2070 6174 682e 6973 5f64 6972 2829 3a0d   path.is_dir():.
-00002f80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00002f90: 2073 7562 7061 7468 7320 3d20 736f 7274   subpaths = sort
-00002fa0: 6564 2870 6174 682e 6974 6572 6469 7228  ed(path.iterdir(
-00002fb0: 2929 0d0a 2020 2020 2020 2020 2020 2020  ))..            
-00002fc0: 2020 2020 7365 6c66 2e72 6576 6572 745f      self.revert_
-00002fd0: 7371 6c28 2a73 7562 7061 7468 732c 2065  sql(*subpaths, e
-00002fe0: 6e63 6f64 696e 673d 656e 636f 6469 6e67  ncoding=encoding
-00002ff0: 290d 0a0d 0a20 2020 2020 2020 2020 2020  )....           
-00003000: 2065 6c69 6620 6e6f 7420 7061 7468 2e6e   elif not path.n
-00003010: 616d 652e 656e 6473 7769 7468 2822 5f72  ame.endswith("_r
-00003020: 6576 6572 742e 7371 6c22 293a 0d0a 2020  evert.sql"):..  
-00003030: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00003040: 6e74 696e 7565 2023 2069 676e 6f72 650d  ntinue # ignore.
-00003050: 0a0d 0a20 2020 2020 2020 2020 2020 2065  ...            e
-00003060: 6c73 653a 0d0a 2020 2020 2020 2020 2020  lse:..          
-00003070: 2020 2020 2020 6c6f 6767 6572 2e69 6e66        logger.inf
-00003080: 6f28 2265 7865 6375 7465 2025 7322 2c20  o("execute %s", 
-00003090: 7061 7468 290d 0a20 2020 2020 2020 2020  path)..         
-000030a0: 2020 2020 2020 2071 7565 7279 203d 2070         query = p
-000030b0: 6174 682e 7265 6164 5f74 6578 7428 656e  ath.read_text(en
-000030c0: 636f 6469 6e67 3d65 6e63 6f64 696e 6729  coding=encoding)
-000030d0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000030e0: 2020 7365 6c66 2e65 7865 6375 7465 2871    self.execute(q
-000030f0: 7565 7279 290d 0a0d 0a0d 0a63 6c61 7373  uery)......class
-00003100: 2043 6f6c 756d 6e3a 0d0a 2020 2020 6465   Column:..    de
-00003110: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
-00003120: 206e 616d 653a 2073 7472 2c20 2a2c 2070   name: str, *, p
-00003130: 6f73 6974 696f 6e3a 2069 6e74 203d 204e  osition: int = N
-00003140: 6f6e 652c 2070 7974 686f 6e5f 7479 7065  one, python_type
-00003150: 3a20 7479 7065 203d 204e 6f6e 652c 206f  : type = None, o
-00003160: 7269 6769 6e61 6c5f 7479 7065 3a20 7374  riginal_type: st
-00003170: 7220 3d20 4e6f 6e65 2c20 7072 6563 6973  r = None, precis
-00003180: 696f 6e3a 2069 6e74 203d 204e 6f6e 652c  ion: int = None,
-00003190: 2073 6361 6c65 3a20 696e 7420 3d20 4e6f   scale: int = No
-000031a0: 6e65 2c20 6e75 6c6c 5f6f 6b3a 2062 6f6f  ne, null_ok: boo
-000031b0: 6c20 3d20 4e6f 6e65 2c20 6469 7370 6c61  l = None, displa
-000031c0: 795f 7369 7a65 3a20 696e 7420 3d20 4e6f  y_size: int = No
-000031d0: 6e65 2c20 696e 7465 726e 616c 5f73 697a  ne, internal_siz
-000031e0: 653a 2069 6e74 203d 204e 6f6e 652c 2070  e: int = None, p
-000031f0: 7269 6d61 7279 5f6b 6579 3a20 626f 6f6c  rimary_key: bool
-00003200: 203d 2046 616c 7365 2c20 6465 6661 756c   = False, defaul
-00003210: 743a 2073 7472 203d 204e 6f6e 6529 3a0d  t: str = None):.
-00003220: 0a20 2020 2020 2020 2073 656c 662e 6e61  .        self.na
-00003230: 6d65 3a20 7374 727c 4e6f 6e65 203d 206e  me: str|None = n
-00003240: 616d 650d 0a20 2020 2020 2020 2022 2222  ame..        """
-00003250: 204e 6f6d 2064 6520 6c61 2063 6f6c 6f6e   Nom de la colon
-00003260: 6e65 2e20 5065 7574 20c3 aa74 7265 2076  ne. Peut ..tre v
-00003270: 6964 6520 2870 6172 2065 7865 6d70 6c65  ide (par exemple
-00003280: 2070 6f75 7220 6c65 7320 636f 6c6f 6e6e   pour les colonn
-00003290: 6573 2063 616c 6375 6cc3 a965 7320 7361  es calcul..es sa
-000032a0: 6e73 2061 6c69 6173 2064 c3a9 6669 6e69  ns alias d..fini
-000032b0: 292e 2022 2222 0d0a 0d0a 2020 2020 2020  ). """....      
-000032c0: 2020 7365 6c66 2e70 6f73 6974 696f 6e3a    self.position:
-000032d0: 2069 6e74 7c4e 6f6e 6520 3d20 706f 7369   int|None = posi
-000032e0: 7469 6f6e 0d0a 2020 2020 2020 2020 2222  tion..        ""
-000032f0: 2220 4f72 6469 6e61 6c20 706f 7369 7469  " Ordinal positi
-00003300: 6f6e 2c20 7374 6172 7469 6e67 2066 726f  on, starting fro
-00003310: 6d20 312e 2022 2222 0d0a 0d0a 2020 2020  m 1. """....    
-00003320: 2020 2020 7365 6c66 2e70 7974 686f 6e5f      self.python_
-00003330: 7479 7065 3a20 7479 7065 7c4e 6f6e 6520  type: type|None 
-00003340: 3d20 7079 7468 6f6e 5f74 7970 650d 0a20  = python_type.. 
-00003350: 2020 2020 2020 2022 2222 2054 7970 6520         """ Type 
-00003360: 5079 7468 6f6e 2075 7469 6c69 73c3 a920  Python utilis.. 
-00003370: 7061 7220 6c65 2064 7269 7665 7220 706f  par le driver po
-00003380: 7572 2072 6570 72c3 a973 656e 7465 7220  ur repr..senter 
-00003390: 6c65 7320 7661 6c65 7572 732e 0d0a 2020  les valeurs...  
-000033a0: 2020 2020 2020 4174 7465 6e74 696f 6e20        Attention 
-000033b0: 3a20 6c65 7320 7661 6c65 7572 7320 6465  : les valeurs de
-000033c0: 2074 7970 6520 5351 4c20 4441 5445 206f   type SQL DATE o
-000033d0: 7520 4441 5445 5449 4d45 2070 6575 7665  u DATETIME peuve
-000033e0: 6e74 20c3 aa74 7265 206d 6170 70c3 a965  nt ..tre mapp..e
-000033f0: 7320 6175 2074 7970 6520 5079 7468 6f6e  s au type Python
-00003400: 2060 7374 7260 2073 7569 7661 6e74 206c   `str` suivant l
-00003410: 6520 6472 6976 6572 2075 7469 6c69 73c3  e driver utilis.
-00003420: a92e 2022 2222 0d0a 2020 2020 2020 2020  .. """..        
-00003430: 2320 4366 2e20 6874 7470 733a 2f2f 7374  # Cf. https://st
-00003440: 6163 6b6f 7665 7266 6c6f 772e 636f 6d2f  ackoverflow.com/
-00003450: 7175 6573 7469 6f6e 732f 3731 3732 3534  questions/717254
-00003460: 302f 7079 6f64 6263 2d72 6574 7572 6e73  0/pyodbc-returns
-00003470: 2d73 716c 2d73 6572 7665 722d 6461 7465  -sql-server-date
-00003480: 2d66 6965 6c64 732d 6173 2d73 7472 696e  -fields-as-strin
-00003490: 6773 0d0a 0d0a 2020 2020 2020 2020 7365  gs....        se
-000034a0: 6c66 2e6f 7269 6769 6e61 6c5f 7479 7065  lf.original_type
-000034b0: 3a20 7374 727c 4e6f 6e65 203d 206f 7269  : str|None = ori
-000034c0: 6769 6e61 6c5f 7479 7065 0d0a 0d0a 2020  ginal_type....  
-000034d0: 2020 2020 2020 7365 6c66 2e70 7265 6369        self.preci
-000034e0: 7369 6f6e 3a20 696e 747c 4e6f 6e65 203d  sion: int|None =
-000034f0: 2070 7265 6369 7369 6f6e 0d0a 2020 2020   precision..    
-00003500: 2020 2020 2222 2220 546f 7461 6c20 6e75      """ Total nu
-00003510: 6d62 6572 206f 6620 7369 676e 6966 6963  mber of signific
-00003520: 616e 7420 6469 6769 7473 2e0d 0a20 2020  ant digits...   
-00003530: 2020 2020 202d 206d 7373 716c 3a20 616c       - mssql: al
-00003540: 7761 7973 2073 6574 2e20 2222 220d 0a20  ways set. """.. 
-00003550: 2020 2020 2020 200d 0a20 2020 2020 2020         ..       
-00003560: 2073 656c 662e 7363 616c 653a 2069 6e74   self.scale: int
-00003570: 7c4e 6f6e 6520 3d20 7363 616c 650d 0a20  |None = scale.. 
-00003580: 2020 2020 2020 2022 2222 2043 6f75 6e74         """ Count
-00003590: 206f 6620 6465 6369 6d61 6c20 6469 6769   of decimal digi
-000035a0: 7473 2069 6e20 7468 6520 6672 6163 7469  ts in the fracti
-000035b0: 6f6e 616c 2070 6172 7420 6f66 2074 6865  onal part of the
-000035c0: 2074 7970 652e 0d0a 2020 2020 2020 2020   type...        
-000035d0: 2d20 6d73 7371 6c3a 2061 6c77 6179 7320  - mssql: always 
-000035e0: 7365 742c 2075 7369 6e67 2030 2069 6620  set, using 0 if 
-000035f0: 6e6f 6e65 2e20 2222 220d 0a0d 0a20 2020  none. """....   
-00003600: 2020 2020 2073 656c 662e 6e75 6c6c 5f6f       self.null_o
-00003610: 6b3a 2062 6f6f 6c7c 4e6f 6e65 203d 206e  k: bool|None = n
-00003620: 756c 6c5f 6f6b 0d0a 2020 2020 2020 2020  ull_ok..        
-00003630: 2222 2220 496e 6469 6361 7465 2069 6620  """ Indicate if 
-00003640: 7468 6520 636f 6c75 6d6e 2061 6363 6570  the column accep
-00003650: 7473 206e 756c 6c20 7661 6c75 652e 0d0a  ts null value...
-00003660: 2020 2020 2020 2020 2d20 7067 3a20 616c          - pg: al
-00003670: 7761 7973 204e 6f6e 6520 6173 206e 6f74  ways None as not
-00003680: 2065 6173 7920 746f 2072 6574 7269 6576   easy to retriev
-00003690: 6520 6672 6f6d 2074 6865 206c 6962 7071  e from the libpq
-000036a0: 2e0d 0a20 2020 2020 2020 2022 2222 0d0a  ...        """..
-000036b0: 0d0a 2020 2020 2020 2020 7365 6c66 2e64  ..        self.d
-000036c0: 6973 706c 6179 5f73 697a 653a 2069 6e74  isplay_size: int
-000036d0: 7c4e 6f6e 6520 3d20 6469 7370 6c61 795f  |None = display_
-000036e0: 7369 7a65 0d0a 2020 2020 2020 2020 2222  size..        ""
-000036f0: 2220 5468 6520 6163 7475 616c 206c 656e  " The actual len
-00003700: 6774 6820 6f66 2074 6865 2063 6f6c 756d  gth of the colum
-00003710: 6e20 696e 2062 7974 6573 2e0d 0a20 2020  n in bytes...   
-00003720: 2020 2020 202d 206d 7373 716c 3a20 616c       - mssql: al
-00003730: 7761 7973 204e 6f6e 6520 286f 7269 6769  ways None (origi
-00003740: 6e61 6c6c 7920 616c 7761 7973 207a 6572  nally always zer
-00003750: 6f29 2e20 2222 220d 0a0d 0a20 2020 2020  o). """....     
-00003760: 2020 2073 656c 662e 696e 7465 726e 616c     self.internal
-00003770: 5f73 697a 653a 2069 6e74 7c4e 6f6e 6520  _size: int|None 
-00003780: 3d20 696e 7465 726e 616c 5f73 697a 650d  = internal_size.
-00003790: 0a20 2020 2020 2020 2022 2222 2054 6865  .        """ The
-000037a0: 2073 697a 6520 696e 2062 7974 6573 206f   size in bytes o
-000037b0: 6620 7468 6520 636f 6c75 6d6e 2061 7373  f the column ass
-000037c0: 6f63 6961 7465 6420 746f 2074 6869 7320  ociated to this 
-000037d0: 636f 6c75 6d6e 206f 6e20 7468 6520 7365  column on the se
-000037e0: 7276 6572 2e0d 0a20 2020 2020 2020 202d  rver...        -
-000037f0: 206d 7373 716c 3a20 616c 7761 7973 204e   mssql: always N
-00003800: 6f6e 6520 286f 7269 6769 6e61 6c6c 7920  one (originally 
-00003810: 616c 7761 7973 2073 616d 6520 6173 2070  always same as p
-00003820: 7265 6369 7369 6f6e 292e 2022 2222 0d0a  recision). """..
-00003830: 0d0a 2020 2020 2020 2020 7365 6c66 2e70  ..        self.p
-00003840: 7269 6d61 7279 5f6b 6579 3a20 626f 6f6c  rimary_key: bool
-00003850: 203d 2070 7269 6d61 7279 5f6b 6579 0d0a   = primary_key..
-00003860: 0d0a 2020 2020 2020 2020 7365 6c66 2e64  ..        self.d
-00003870: 6566 6175 6c74 3a20 7374 727c 4e6f 6e65  efault: str|None
-00003880: 203d 2064 6566 6175 6c74 0d0a 0d0a 0d0a   = default......
-00003890: 2020 2020 6465 6620 5f63 6865 636b 2873      def _check(s
-000038a0: 656c 6629 3a0d 0a20 2020 2020 2020 2022  elf):..        "
-000038b0: 2222 0d0a 2020 2020 2020 2020 456e 7375  ""..        Ensu
-000038c0: 7265 2063 6f6e 7369 7374 656e 6379 206f  re consistency o
-000038d0: 6620 636f 6c75 6d6e 2064 6566 696e 6974  f column definit
-000038e0: 696f 6e2e 0d0a 0d0a 2020 2020 2020 2020  ion.....        
-000038f0: 4361 6c6c 6564 2061 6674 6572 2063 7265  Called after cre
-00003900: 6174 696f 6e20 6f66 2043 6f6c 756d 6e20  ation of Column 
-00003910: 696e 7374 616e 6365 2061 6e64 2070 6f74  instance and pot
-00003920: 656e 7469 616c 2066 6978 2061 6461 7074  ential fix adapt
-00003930: 6564 2074 6f20 7370 6563 6966 6963 2044  ed to specific D
-00003940: 6174 6162 6173 6520 656e 6769 6e65 732e  atabase engines.
-00003950: 0d0a 2020 2020 2020 2020 2222 220d 0a20  ..        """.. 
-00003960: 2020 2020 2020 2069 6620 7365 6c66 2e70         if self.p
-00003970: 7974 686f 6e5f 7479 7065 2069 7320 6e6f  ython_type is no
-00003980: 7420 4e6f 6e65 2061 6e64 206e 6f74 2069  t None and not i
-00003990: 7369 6e73 7461 6e63 6528 7365 6c66 2e70  sinstance(self.p
-000039a0: 7974 686f 6e5f 7479 7065 2c20 7479 7065  ython_type, type
-000039b0: 293a 0d0a 2020 2020 2020 2020 2020 2020  ):..            
-000039c0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-000039d0: 2866 2269 6e76 616c 6964 2070 7974 686f  (f"invalid pytho
-000039e0: 6e5f 7479 7065 205c 227b 7365 6c66 2e70  n_type \"{self.p
-000039f0: 7974 686f 6e5f 7479 7065 7d5c 2220 287b  ython_type}\" ({
-00003a00: 7479 7065 2873 656c 662e 7079 7468 6f6e  type(self.python
-00003a10: 5f74 7970 6529 2e5f 5f6e 616d 655f 5f7d  _type).__name__}
-00003a20: 293a 2065 7870 6563 7465 6420 7479 7065  ): expected type
-00003a30: 2069 6e73 7461 6e63 6522 290d 0a0d 0a0d   instance").....
-00003a40: 0a20 2020 2064 6566 205f 5f73 7472 5f5f  .    def __str__
-00003a50: 2873 656c 6629 202d 3e20 7374 723a 0d0a  (self) -> str:..
-00003a60: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00003a70: 656c 662e 6e61 6d65 0d0a                 elf.name..
+00000000: 0a66 726f 6d20 5f5f 6675 7475 7265 5f5f  .from __future__
+00000010: 2069 6d70 6f72 7420 616e 6e6f 7461 7469   import annotati
+00000020: 6f6e 730a 6672 6f6d 2064 6174 6574 696d  ons.from datetim
+00000030: 6520 696d 706f 7274 2074 696d 657a 6f6e  e import timezon
+00000040: 650a 6672 6f6d 2069 6f20 696d 706f 7274  e.from io import
+00000050: 2049 4f42 6173 650a 696d 706f 7274 206c   IOBase.import l
+00000060: 6f67 6769 6e67 0a66 726f 6d20 7061 7468  ogging.from path
+00000070: 6c69 6220 696d 706f 7274 2050 6174 680a  lib import Path.
+00000080: 696d 706f 7274 2072 650a 6672 6f6d 2074  import re.from t
+00000090: 7970 696e 6720 696d 706f 7274 2041 6e79  yping import Any
+000000a0: 2c20 4765 6e65 7269 632c 2054 7970 6556  , Generic, TypeV
+000000b0: 6172 0a66 726f 6d20 7572 6c6c 6962 2e70  ar.from urllib.p
+000000c0: 6172 7365 2069 6d70 6f72 7420 7175 6f74  arse import quot
+000000d0: 650a 0a6c 6f67 6765 7220 3d20 6c6f 6767  e..logger = logg
+000000e0: 696e 672e 6765 744c 6f67 6765 7228 5f5f  ing.getLogger(__
+000000f0: 6e61 6d65 5f5f 290a 0a54 5f43 6f6e 6e65  name__)..T_Conne
+00000100: 6374 696f 6e20 3d20 5479 7065 5661 7228  ction = TypeVar(
+00000110: 2754 5f43 6f6e 6e65 6374 696f 6e27 290a  'T_Connection').
+00000120: 545f 4375 7273 6f72 203d 2054 7970 6556  T_Cursor = TypeV
+00000130: 6172 2827 545f 4375 7273 6f72 2729 0a0a  ar('T_Cursor')..
+00000140: 636c 6173 7320 4462 5772 6170 7065 7228  class DbWrapper(
+00000150: 4765 6e65 7269 635b 545f 436f 6e6e 6563  Generic[T_Connec
+00000160: 7469 6f6e 2c20 545f 4375 7273 6f72 5d29  tion, T_Cursor])
+00000170: 3a0a 2020 2020 4063 6c61 7373 6d65 7468  :.    @classmeth
+00000180: 6f64 0a20 2020 2064 6566 2069 735f 6176  od.    def is_av
+00000190: 6169 6c61 626c 6528 636c 7329 3a0a 2020  ailable(cls):.  
+000001a0: 2020 2020 2020 7261 6973 6520 4e6f 7449        raise NotI
+000001b0: 6d70 6c65 6d65 6e74 6564 4572 726f 7228  mplementedError(
+000001c0: 290a 2020 2020 0a20 2020 2073 6368 656d  ).    .    schem
+000001d0: 653a 2073 7472 0a20 2020 2064 6566 6175  e: str.    defau
+000001e0: 6c74 5f73 6368 656d 615f 6e61 6d65 3a20  lt_schema_name: 
+000001f0: 7374 720a 2020 2020 636f 6d70 6174 6962  str.    compatib
+00000200: 6c65 5f64 6a61 6e67 6f5f 656e 6769 6e65  le_django_engine
+00000210: 733a 206c 6973 745b 7374 725d 0a0a 2020  s: list[str]..  
+00000220: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+00000230: 656c 662c 202a 2c20 6e61 6d65 3a20 7374  elf, *, name: st
+00000240: 7220 3d20 4e6f 6e65 2c20 686f 7374 3a20  r = None, host: 
+00000250: 7374 7220 3d20 4e6f 6e65 2c20 706f 7274  str = None, port
+00000260: 3a20 696e 7420 3d20 4e6f 6e65 2c20 7573  : int = None, us
+00000270: 6572 3a20 7374 7220 3d20 4e6f 6e65 2c20  er: str = None, 
+00000280: 7061 7373 776f 7264 3a20 7374 7220 3d20  password: str = 
+00000290: 4e6f 6e65 2c20 646a 616e 676f 5f61 6c69  None, django_ali
+000002a0: 6173 3a20 7374 7220 3d20 4e6f 6e65 2c20  as: str = None, 
+000002b0: 636f 6e6e 6563 7469 6f6e 3a20 545f 436f  connection: T_Co
+000002c0: 6e6e 6563 7469 6f6e 203d 204e 6f6e 652c  nnection = None,
+000002d0: 206e 6169 7665 5f74 7a3a 204c 6974 6572   naive_tz: Liter
+000002e0: 616c 5b27 6c6f 6361 6c27 5d7c 7469 6d65  al['local']|time
+000002f0: 7a6f 6e65 203d 2027 6c6f 6361 6c27 293a  zone = 'local'):
+00000300: 2020 2020 0a20 2020 2020 2020 2069 6620      .        if 
+00000310: 6e6f 7420 7365 6c66 2e69 735f 6176 6169  not self.is_avai
+00000320: 6c61 626c 6528 293a 0a20 2020 2020 2020  lable():.       
+00000330: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+00000340: 4572 726f 7228 6622 6361 6e6e 6f74 2075  Error(f"cannot u
+00000350: 7365 207b 7479 7065 2873 656c 6629 2e5f  se {type(self)._
+00000360: 5f6e 616d 655f 5f7d 2028 6e6f 7420 6176  _name__} (not av
+00000370: 6169 6c61 626c 6529 2229 0a20 2020 2020  ailable)").     
+00000380: 2020 200a 2020 2020 2020 2020 6966 2064     .        if d
+00000390: 6a61 6e67 6f5f 616c 6961 7320 6973 206e  jango_alias is n
+000003a0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
+000003b0: 2020 2020 2066 726f 6d20 646a 616e 676f       from django
+000003c0: 2e63 6f6e 6620 696d 706f 7274 2073 6574  .conf import set
+000003d0: 7469 6e67 730a 2020 2020 2020 2020 2020  tings.          
+000003e0: 2020 4441 5441 4241 5345 5320 3d20 7365    DATABASES = se
+000003f0: 7474 696e 6773 2e44 4154 4142 4153 4553  ttings.DATABASES
+00000400: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00000410: 6e6f 7420 646a 616e 676f 5f61 6c69 6173  not django_alias
+00000420: 2069 6e20 4441 5441 4241 5345 533a 0a20   in DATABASES:. 
+00000430: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00000440: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00000450: 6622 6b65 7920 5c22 7b64 6a61 6e67 6f5f  f"key \"{django_
+00000460: 616c 6961 737d 5c22 206e 6f74 2066 6f75  alias}\" not fou
+00000470: 6e64 2069 6e20 646a 616e 676f 2044 4154  nd in django DAT
+00000480: 4142 4153 4553 2073 6574 7469 6e67 7322  ABASES settings"
+00000490: 290a 2020 2020 2020 2020 2020 2020 6461  ).            da
+000004a0: 7461 6261 7365 5f73 6574 7469 6e67 733a  tabase_settings:
+000004b0: 2064 6963 745b 7374 722c 416e 795d 203d   dict[str,Any] =
+000004c0: 2044 4154 4142 4153 4553 5b64 6a61 6e67   DATABASES[djang
+000004d0: 6f5f 616c 6961 735d 0a20 2020 2020 2020  o_alias].       
+000004e0: 2020 2020 2065 6e67 696e 6520 3d20 6461       engine = da
+000004f0: 7461 6261 7365 5f73 6574 7469 6e67 735b  tabase_settings[
+00000500: 2745 4e47 494e 4527 5d0a 2020 2020 2020  'ENGINE'].      
+00000510: 2020 2020 2020 6966 206e 6f74 2065 6e67        if not eng
+00000520: 696e 6520 696e 2073 656c 662e 636f 6d70  ine in self.comp
+00000530: 6174 6962 6c65 5f64 6a61 6e67 6f5f 656e  atible_django_en
+00000540: 6769 6e65 733a 0a20 2020 2020 2020 2020  gines:.         
+00000550: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+00000560: 7565 4572 726f 7228 6622 646a 616e 676f  ueError(f"django
+00000570: 2064 6174 6162 6173 6520 656e 6769 6e65   database engine
+00000580: 205c 227b 656e 6769 6e65 7d5c 2220 6973   \"{engine}\" is
+00000590: 206e 6f74 2063 6f6d 7061 7469 626c 6520   not compatible 
+000005a0: 7769 7468 207b 7479 7065 2873 656c 6629  with {type(self)
+000005b0: 2e5f 5f6e 616d 655f 5f7d 2229 0a20 2020  .__name__}").   
+000005c0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+000005d0: 2020 2020 2020 2064 6174 6162 6173 655f         database_
+000005e0: 7365 7474 696e 6773 3a20 6469 6374 5b73  settings: dict[s
+000005f0: 7472 2c41 6e79 5d20 3d20 7b7d 0a0a 2020  tr,Any] = {}..  
+00000600: 2020 2020 2020 7365 6c66 2e5f 6e61 6d65        self._name
+00000610: 3a20 7374 7220 3d20 6e61 6d65 2069 6620  : str = name if 
+00000620: 6e61 6d65 2069 7320 6e6f 7420 4e6f 6e65  name is not None
+00000630: 2065 6c73 6520 6461 7461 6261 7365 5f73   else database_s
+00000640: 6574 7469 6e67 732e 6765 7428 274e 414d  ettings.get('NAM
+00000650: 4527 2c20 4e6f 6e65 290a 2020 2020 2020  E', None).      
+00000660: 2020 7365 6c66 2e5f 686f 7374 3a20 7374    self._host: st
+00000670: 7220 3d20 686f 7374 2069 6620 686f 7374  r = host if host
+00000680: 2069 7320 6e6f 7420 4e6f 6e65 2065 6c73   is not None els
+00000690: 6520 6461 7461 6261 7365 5f73 6574 7469  e database_setti
+000006a0: 6e67 732e 6765 7428 2748 4f53 5427 2c20  ngs.get('HOST', 
+000006b0: 4e6f 6e65 290a 2020 2020 2020 2020 7365  None).        se
+000006c0: 6c66 2e5f 7573 6572 3a20 7374 7220 3d20  lf._user: str = 
+000006d0: 7573 6572 2069 6620 7573 6572 2069 7320  user if user is 
+000006e0: 6e6f 7420 4e6f 6e65 2065 6c73 6520 6461  not None else da
+000006f0: 7461 6261 7365 5f73 6574 7469 6e67 732e  tabase_settings.
+00000700: 6765 7428 2755 5345 5227 2c20 4e6f 6e65  get('USER', None
+00000710: 290a 2020 2020 2020 2020 7365 6c66 2e5f  ).        self._
+00000720: 7061 7373 776f 7264 3a20 7374 7220 3d20  password: str = 
+00000730: 7061 7373 776f 7264 2069 6620 7061 7373  password if pass
+00000740: 776f 7264 2069 7320 6e6f 7420 4e6f 6e65  word is not None
+00000750: 2065 6c73 6520 6461 7461 6261 7365 5f73   else database_s
+00000760: 6574 7469 6e67 732e 6765 7428 2750 4153  ettings.get('PAS
+00000770: 5357 4f52 4427 2c20 4e6f 6e65 290a 2020  SWORD', None).  
+00000780: 2020 2020 2020 7365 6c66 2e5f 706f 7274        self._port
+00000790: 3a20 7374 7220 3d20 706f 7274 2069 6620  : str = port if 
+000007a0: 706f 7274 2069 7320 6e6f 7420 4e6f 6e65  port is not None
+000007b0: 2065 6c73 6520 6461 7461 6261 7365 5f73   else database_s
+000007c0: 6574 7469 6e67 732e 6765 7428 2750 4f52  ettings.get('POR
+000007d0: 5427 2c20 4e6f 6e65 290a 0a20 2020 2020  T', None)..     
+000007e0: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
+000007f0: 2873 656c 662e 5f70 6f72 742c 2073 7472  (self._port, str
+00000800: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s
+00000810: 656c 662e 5f70 6f72 7420 3d20 696e 7428  elf._port = int(
+00000820: 7365 6c66 2e5f 706f 7274 290a 2020 2020  self._port).    
+00000830: 2020 2020 0a20 2020 2020 2020 2073 656c      .        sel
+00000840: 662e 5f6e 6169 7665 5f74 7a20 3d20 6e61  f._naive_tz = na
+00000850: 6976 655f 747a 0a20 2020 2020 2020 2022  ive_tz.        "
+00000860: 2222 2049 6620 6e6f 7420 4e6f 6e65 2c20  "" If not None, 
+00000870: 696e 6469 6361 7465 2077 6869 6368 2074  indicate which t
+00000880: 696d 657a 6f6e 6520 7769 6c6c 2062 6520  imezone will be 
+00000890: 7573 6564 2066 6f72 206e 6169 7665 2064  used for naive d
+000008a0: 6174 6574 696d 6573 2e20 2222 220a 0a20  atetimes. """.. 
+000008b0: 2020 2020 2020 2073 656c 662e 5f63 6f6e         self._con
+000008c0: 6e65 6374 696f 6e20 3d20 636f 6e6e 6563  nection = connec
+000008d0: 7469 6f6e 0a20 2020 2020 2020 2073 656c  tion.        sel
+000008e0: 662e 5f6d 7573 745f 636c 6f73 655f 636f  f._must_close_co
+000008f0: 6e6e 6563 7469 6f6e 203d 2063 6f6e 6e65  nnection = conne
+00000900: 6374 696f 6e20 6973 204e 6f6e 650a 0a0a  ction is None...
+00000910: 2020 2020 6465 6620 5f5f 656e 7465 725f      def __enter_
+00000920: 5f28 7365 6c66 293a 0a20 2020 2020 2020  _(self):.       
+00000930: 2072 6574 7572 6e20 7365 6c66 0a0a 0a20   return self... 
+00000940: 2020 2064 6566 205f 5f65 7869 745f 5f28     def __exit__(
+00000950: 7365 6c66 2c20 6578 635f 7479 7065 203d  self, exc_type =
+00000960: 204e 6f6e 652c 2065 7863 5f76 616c 203d   None, exc_val =
+00000970: 204e 6f6e 652c 2065 7863 5f74 6220 3d20   None, exc_tb = 
+00000980: 4e6f 6e65 293a 0a20 2020 2020 2020 2069  None):.        i
+00000990: 6620 7365 6c66 2e5f 636f 6e6e 6563 7469  f self._connecti
+000009a0: 6f6e 2061 6e64 2073 656c 662e 5f6d 7573  on and self._mus
+000009b0: 745f 636c 6f73 655f 636f 6e6e 6563 7469  t_close_connecti
+000009c0: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
+000009d0: 7365 6c66 2e5f 636f 6e6e 6563 7469 6f6e  self._connection
+000009e0: 2e63 6c6f 7365 2829 0a0a 0a20 2020 2064  .close()...    d
+000009f0: 6566 2067 6574 5f75 7269 2873 656c 662c  ef get_uri(self,
+00000a00: 2074 6162 6c65 3a20 7374 727c 7475 706c   table: str|tupl
+00000a10: 6520 3d20 4e6f 6e65 2c20 2a2c 2077 6974  e = None, *, wit
+00000a20: 685f 7061 7373 776f 7264 203d 2046 616c  h_password = Fal
+00000a30: 7365 293a 0a20 2020 2020 2020 2075 7269  se):.        uri
+00000a40: 203d 2066 227b 7365 6c66 2e73 6368 656d   = f"{self.schem
+00000a50: 657d 3a22 0a0a 2020 2020 2020 2020 6966  e}:"..        if
+00000a60: 2073 656c 662e 5f75 7365 7220 6f72 2073   self._user or s
+00000a70: 656c 662e 5f68 6f73 743a 0a20 2020 2020  elf._host:.     
+00000a80: 2020 2020 2020 2075 7269 202b 3d20 272f         uri += '/
+00000a90: 2f27 0a20 2020 2020 2020 2020 2020 2069  /'.            i
+00000aa0: 6620 7365 6c66 2e5f 7573 6572 3a0a 2020  f self._user:.  
+00000ab0: 2020 2020 2020 2020 2020 2020 2020 7572                ur
+00000ac0: 6920 2b3d 2071 756f 7465 2873 656c 662e  i += quote(self.
+00000ad0: 5f75 7365 7229 0a20 2020 2020 2020 2020  _user).         
+00000ae0: 2020 2020 2020 2069 6620 7365 6c66 2e5f         if self._
+00000af0: 7061 7373 776f 7264 3a0a 2020 2020 2020  password:.      
+00000b00: 2020 2020 2020 2020 2020 2020 2020 7572                ur
+00000b10: 6920 2b3d 2027 3a27 202b 2028 7175 6f74  i += ':' + (quot
+00000b20: 6528 7365 6c66 2e5f 7061 7373 776f 7264  e(self._password
+00000b30: 2920 6966 2077 6974 685f 7061 7373 776f  ) if with_passwo
+00000b40: 7264 2065 6c73 6520 7265 2e73 7562 2872  rd else re.sub(r
+00000b50: 272e 272c 2027 2a27 2c20 7365 6c66 2e5f  '.', '*', self._
+00000b60: 7061 7373 776f 7264 2929 0a20 2020 2020  password)).     
+00000b70: 2020 2020 2020 2020 2020 2075 7269 202b             uri +
+00000b80: 3d20 2740 270a 0a20 2020 2020 2020 2020  = '@'..         
+00000b90: 2020 2069 6620 7365 6c66 2e5f 686f 7374     if self._host
+00000ba0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00000bb0: 2020 7572 6920 2b3d 2071 756f 7465 2873    uri += quote(s
+00000bc0: 656c 662e 5f68 6f73 7429 0a20 2020 2020  elf._host).     
+00000bd0: 2020 2020 2020 2020 2020 2069 6620 7365             if se
+00000be0: 6c66 2e5f 706f 7274 3a0a 2020 2020 2020  lf._port:.      
+00000bf0: 2020 2020 2020 2020 2020 2020 2020 7572                ur
+00000c00: 6920 2b3d 2066 273a 7b73 656c 662e 5f70  i += f':{self._p
+00000c10: 6f72 747d 270a 0a20 2020 2020 2020 2069  ort}'..        i
+00000c20: 6620 7365 6c66 2e5f 6e61 6d65 3a0a 2020  f self._name:.  
+00000c30: 2020 2020 2020 2020 2020 7572 6920 2b3d            uri +=
+00000c40: 2066 222f 7b71 756f 7465 2873 656c 662e   f"/{quote(self.
+00000c50: 5f6e 616d 6529 7d22 0a0a 2020 2020 2020  _name)}"..      
+00000c60: 2020 6966 2074 6162 6c65 3a0a 2020 2020    if table:.    
+00000c70: 2020 2020 2020 2020 7363 6865 6d61 2c20          schema, 
+00000c80: 7461 626c 6520 3d20 7365 6c66 2e73 706c  table = self.spl
+00000c90: 6974 5f6e 616d 6528 7461 626c 6529 0a20  it_name(table). 
+00000ca0: 2020 2020 2020 2020 2020 2075 7269 202b             uri +
+00000cb0: 3d20 6622 2f22 0a20 2020 2020 2020 2020  = f"/".         
+00000cc0: 2020 2069 6620 7363 6865 6d61 3a0a 2020     if schema:.  
+00000cd0: 2020 2020 2020 2020 2020 2020 2020 7572                ur
+00000ce0: 6920 2b3d 2071 756f 7465 2873 6368 656d  i += quote(schem
+00000cf0: 6129 0a20 2020 2020 2020 2020 2020 2020  a).             
+00000d00: 2020 2075 7269 202b 3d20 272e 270a 2020     uri += '.'.  
+00000d10: 2020 2020 2020 2020 2020 7572 6920 2b3d            uri +=
+00000d20: 2071 756f 7465 2874 6162 6c65 290a 0a20   quote(table).. 
+00000d30: 2020 2020 2020 2072 6574 7572 6e20 7572         return ur
+00000d40: 690a 0a0a 2020 2020 4070 726f 7065 7274  i...    @propert
+00000d50: 790a 2020 2020 6465 6620 636f 6e6e 6563  y.    def connec
+00000d60: 7469 6f6e 2873 656c 6629 3a0a 2020 2020  tion(self):.    
+00000d70: 2020 2020 6966 206e 6f74 2073 656c 662e      if not self.
+00000d80: 5f63 6f6e 6e65 6374 696f 6e3a 0a20 2020  _connection:.   
+00000d90: 2020 2020 2020 2020 2073 656c 662e 5f63           self._c
+00000da0: 6f6e 6e65 6374 696f 6e20 3d20 7365 6c66  onnection = self
+00000db0: 2e5f 6372 6561 7465 5f63 6f6e 6e65 6374  ._create_connect
+00000dc0: 696f 6e28 290a 2020 2020 2020 2020 7265  ion().        re
+00000dd0: 7475 726e 2073 656c 662e 5f63 6f6e 6e65  turn self._conne
+00000de0: 6374 696f 6e0a 0a0a 2020 2020 6465 6620  ction...    def 
+00000df0: 5f63 7265 6174 655f 636f 6e6e 6563 7469  _create_connecti
+00000e00: 6f6e 2873 656c 6629 202d 3e20 545f 436f  on(self) -> T_Co
+00000e10: 6e6e 6563 7469 6f6e 3a0a 2020 2020 2020  nnection:.      
+00000e20: 2020 7261 6973 6520 4e6f 7449 6d70 6c65    raise NotImple
+00000e30: 6d65 6e74 6564 4572 726f 7228 290a 0a0a  mentedError()...
+00000e40: 2020 2020 6465 6620 6375 7273 6f72 2873      def cursor(s
+00000e50: 656c 6629 202d 3e20 545f 4375 7273 6f72  elf) -> T_Cursor
+00000e60: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
+00000e70: 2073 656c 662e 636f 6e6e 6563 7469 6f6e   self.connection
+00000e80: 2e63 7572 736f 7228 290a 2020 2020 0a20  .cursor().    . 
+00000e90: 2020 200a 2020 2020 6465 6620 6c69 6d69     .    def limi
+00000ea0: 745f 7175 6572 7928 7365 6c66 2c20 7175  t_query(self, qu
+00000eb0: 6572 793a 2073 7472 2c20 6c69 6d69 743a  ery: str, limit:
+00000ec0: 2069 6e74 293a 0a20 2020 2020 2020 2069   int):.        i
+00000ed0: 6620 6c69 6d69 7420 6973 204e 6f6e 653a  f limit is None:
+00000ee0: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret
+00000ef0: 7572 6e20 7175 6572 790a 2020 2020 0a20  urn query.    . 
+00000f00: 2020 2020 2020 2069 6620 6e6f 7420 6973         if not is
+00000f10: 696e 7374 616e 6365 286c 696d 6974 2c20  instance(limit, 
+00000f20: 696e 7429 3a0a 2020 2020 2020 2020 2020  int):.          
+00000f30: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+00000f40: 6f72 2866 2269 6e76 616c 6964 2074 7970  or(f"invalid typ
+00000f50: 6520 666f 7220 6c69 6d69 743a 207b 7479  e for limit: {ty
+00000f60: 7065 286c 696d 6974 292e 5f5f 6e61 6d65  pe(limit).__name
+00000f70: 5f5f 7d20 2865 7870 6563 7465 6420 696e  __} (expected in
+00000f80: 7429 2229 0a20 2020 2020 2020 200a 2020  t)").        .  
+00000f90: 2020 2020 2020 696d 706f 7274 2073 716c        import sql
+00000fa0: 7061 7273 6520 2320 6e6f 7420 6174 2074  parse # not at t
+00000fb0: 6865 2074 6f70 2062 6563 6175 7365 2074  he top because t
+00000fc0: 6865 2065 6e64 7573 6572 206d 6967 6874  he enduser might
+00000fd0: 206e 6f74 206e 6565 6420 7468 6973 2066   not need this f
+00000fe0: 6561 7475 7265 0a0a 2020 2020 2020 2020  eature..        
+00000ff0: 2320 5061 7273 6520 5351 4c20 746f 2072  # Parse SQL to r
+00001000: 656d 6f76 6520 746f 6b65 6e20 6265 666f  emove token befo
+00001010: 7265 2074 6865 2053 454c 4543 5420 6b65  re the SELECT ke
+00001020: 7977 6f72 640a 2020 2020 2020 2020 2320  yword.        # 
+00001030: 6578 616d 706c 653a 2057 4954 4820 2843  example: WITH (C
+00001040: 5445 2920 746f 6b65 6e73 0a20 2020 2020  TE) tokens.     
+00001050: 2020 2073 7461 7465 6d65 6e74 7320 3d20     statements = 
+00001060: 7371 6c70 6172 7365 2e70 6172 7365 2871  sqlparse.parse(q
+00001070: 7565 7279 290a 2020 2020 2020 2020 6966  uery).        if
+00001080: 206c 656e 2873 7461 7465 6d65 6e74 7329   len(statements)
+00001090: 2021 3d20 313a 0a20 2020 2020 2020 2020   != 1:.         
+000010a0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+000010b0: 726f 7228 6622 7175 6572 7920 636f 6e74  ror(f"query cont
+000010c0: 6169 6e73 207b 6c65 6e28 7374 6174 656d  ains {len(statem
+000010d0: 656e 7473 297d 2073 7461 7465 6d65 6e74  ents)} statement
+000010e0: 7322 290a 0a20 2020 2020 2020 2023 2047  s")..        # G
+000010f0: 6574 2066 6972 7374 2044 4d4c 206b 6579  et first DML key
+00001100: 776f 7264 0a20 2020 2020 2020 2064 6d6c  word.        dml
+00001110: 5f6b 6579 776f 7264 203d 204e 6f6e 650a  _keyword = None.
+00001120: 2020 2020 2020 2020 646d 6c5f 6b65 7977          dml_keyw
+00001130: 6f72 645f 696e 6465 7820 3d20 4e6f 6e65  ord_index = None
+00001140: 0a20 2020 2020 2020 206f 7264 6572 5f62  .        order_b
+00001150: 795f 696e 6465 7820 3d20 4e6f 6e65 0a20  y_index = None. 
+00001160: 2020 2020 2020 2066 6f72 2069 2c20 746f         for i, to
+00001170: 6b65 6e20 696e 2065 6e75 6d65 7261 7465  ken in enumerate
+00001180: 2873 7461 7465 6d65 6e74 735b 305d 2e74  (statements[0].t
+00001190: 6f6b 656e 7329 3a0a 2020 2020 2020 2020  okens):.        
+000011a0: 2020 2020 6966 2074 6f6b 656e 2e74 7479      if token.tty
+000011b0: 7065 203d 3d20 7371 6c70 6172 7365 2e74  pe == sqlparse.t
+000011c0: 6f6b 656e 732e 444d 4c3a 0a20 2020 2020  okens.DML:.     
+000011d0: 2020 2020 2020 2020 2020 2069 6620 646d             if dm
+000011e0: 6c5f 6b65 7977 6f72 6420 6973 204e 6f6e  l_keyword is Non
+000011f0: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+00001200: 2020 2020 2020 2064 6d6c 5f6b 6579 776f         dml_keywo
+00001210: 7264 203d 2073 7472 2874 6f6b 656e 292e  rd = str(token).
+00001220: 7570 7065 7228 290a 2020 2020 2020 2020  upper().        
+00001230: 2020 2020 2020 2020 2020 2020 646d 6c5f              dml_
+00001240: 6b65 7977 6f72 645f 696e 6465 7820 3d20  keyword_index = 
+00001250: 690a 2020 2020 2020 2020 2020 2020 656c  i.            el
+00001260: 6966 2074 6f6b 656e 2e74 7479 7065 203d  if token.ttype =
+00001270: 3d20 7371 6c70 6172 7365 2e74 6f6b 656e  = sqlparse.token
+00001280: 732e 4b65 7977 6f72 643a 0a20 2020 2020  s.Keyword:.     
+00001290: 2020 2020 2020 2020 2020 2069 6620 6f72             if or
+000012a0: 6465 725f 6279 5f69 6e64 6578 2069 7320  der_by_index is 
+000012b0: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
+000012c0: 2020 2020 2020 2020 2020 6b65 7977 6f72            keywor
+000012d0: 6420 3d20 7374 7228 746f 6b65 6e29 2e75  d = str(token).u
+000012e0: 7070 6572 2829 0a20 2020 2020 2020 2020  pper().         
+000012f0: 2020 2020 2020 2020 2020 2069 6620 6b65             if ke
+00001300: 7977 6f72 6420 3d3d 2022 4f52 4445 5220  yword == "ORDER 
+00001310: 4259 223a 0a20 2020 2020 2020 2020 2020  BY":.           
+00001320: 2020 2020 2020 2020 2020 2020 206f 7264               ord
+00001330: 6572 5f62 795f 696e 6465 7820 3d20 690a  er_by_index = i.
+00001340: 0a20 2020 2020 2020 2023 2043 6865 636b  .        # Check
+00001350: 2069 6620 7468 6520 444d 4c20 6b65 7977   if the DML keyw
+00001360: 6f72 6420 6973 2053 454c 4543 540a 2020  ord is SELECT.  
+00001370: 2020 2020 2020 6966 206e 6f74 2064 6d6c        if not dml
+00001380: 5f6b 6579 776f 7264 3a0a 2020 2020 2020  _keyword:.      
+00001390: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+000013a0: 6545 7272 6f72 2866 226e 6f20 5345 4c45  eError(f"no SELE
+000013b0: 4354 2066 6f75 6e64 2028 7175 6572 7920  CT found (query 
+000013c0: 646f 6573 206e 6f74 2063 6f6e 7461 696e  does not contain
+000013d0: 2044 4d4c 206b 6579 776f 7264 2922 290a   DML keyword)").
+000013e0: 2020 2020 2020 2020 6966 2064 6d6c 5f6b          if dml_k
+000013f0: 6579 776f 7264 2021 3d20 2753 454c 4543  eyword != 'SELEC
+00001400: 5427 3a0a 2020 2020 2020 2020 2020 2020  T':.            
+00001410: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00001420: 2866 2266 6972 7374 2044 4d4c 206b 6579  (f"first DML key
+00001430: 776f 7264 2069 7320 7b64 6d6c 5f6b 6579  word is {dml_key
+00001440: 776f 7264 7d2c 2065 7870 6563 7465 6420  word}, expected 
+00001450: 5345 4c45 4354 2229 0a0a 2020 2020 2020  SELECT")..      
+00001460: 2020 2320 4765 7420 7061 7274 2062 6566    # Get part bef
+00001470: 6f72 6520 5345 4c45 4354 2028 6578 616d  ore SELECT (exam
+00001480: 706c 653a 2057 4954 4829 0a20 2020 2020  ple: WITH).     
+00001490: 2020 2069 6620 646d 6c5f 6b65 7977 6f72     if dml_keywor
+000014a0: 645f 696e 6465 7820 3e20 303a 0a20 2020  d_index > 0:.   
+000014b0: 2020 2020 2020 2020 2074 6f6b 656e 7320           tokens 
+000014c0: 3d20 7374 6174 656d 656e 7473 5b30 5d2e  = statements[0].
+000014d0: 746f 6b65 6e73 5b3a 646d 6c5f 6b65 7977  tokens[:dml_keyw
+000014e0: 6f72 645f 696e 6465 785d 0a20 2020 2020  ord_index].     
+000014f0: 2020 2020 2020 206c 696d 6974 6564 5f71         limited_q
+00001500: 7565 7279 203d 2027 272e 6a6f 696e 2873  uery = ''.join(s
+00001510: 7472 2874 6f6b 656e 2920 666f 7220 746f  tr(token) for to
+00001520: 6b65 6e20 696e 2074 6f6b 656e 7329 0a20  ken in tokens). 
+00001530: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+00001540: 2020 2020 2020 2020 206c 696d 6974 6564           limited
+00001550: 5f71 7565 7279 203d 2027 270a 2020 2020  _query = ''.    
+00001560: 0a20 2020 2020 2020 2023 2041 7070 656e  .        # Appen
+00001570: 6420 5345 4c45 4354 2062 6566 6f72 6520  d SELECT before 
+00001580: 4f52 4445 5220 4259 0a20 2020 2020 2020  ORDER BY.       
+00001590: 2069 6620 6f72 6465 725f 6279 5f69 6e64   if order_by_ind
+000015a0: 6578 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ex is not None:.
+000015b0: 2020 2020 2020 2020 2020 2020 746f 6b65              toke
+000015c0: 6e73 203d 2073 7461 7465 6d65 6e74 735b  ns = statements[
+000015d0: 305d 2e74 6f6b 656e 735b 646d 6c5f 6b65  0].tokens[dml_ke
+000015e0: 7977 6f72 645f 696e 6465 783a 6f72 6465  yword_index:orde
+000015f0: 725f 6279 5f69 6e64 6578 5d0a 2020 2020  r_by_index].    
+00001600: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00001610: 2020 2020 2020 746f 6b65 6e73 203d 2073        tokens = s
+00001620: 7461 7465 6d65 6e74 735b 305d 2e74 6f6b  tatements[0].tok
+00001630: 656e 735b 646d 6c5f 6b65 7977 6f72 645f  ens[dml_keyword_
+00001640: 696e 6465 783a 5d0a 0a20 2020 2020 2020  index:]..       
+00001650: 206c 696d 6974 6564 5f71 7565 7279 202b   limited_query +
+00001660: 3d20 7365 6c66 2e5f 6c69 6d69 745f 7061  = self._limit_pa
+00001670: 7273 6564 5f71 7565 7279 2827 272e 6a6f  rsed_query(''.jo
+00001680: 696e 2873 7472 2874 6f6b 656e 2920 666f  in(str(token) fo
+00001690: 7220 746f 6b65 6e20 696e 2074 6f6b 656e  r token in token
+000016a0: 7329 2c20 6c69 6d69 743d 6c69 6d69 7429  s), limit=limit)
+000016b0: 0a0a 2020 2020 2020 2020 2320 4170 7065  ..        # Appe
+000016c0: 6e64 204f 5244 4552 2042 590a 2020 2020  nd ORDER BY.    
+000016d0: 2020 2020 6966 206f 7264 6572 5f62 795f      if order_by_
+000016e0: 696e 6465 7820 6973 206e 6f74 204e 6f6e  index is not Non
+000016f0: 653a 0a20 2020 2020 2020 2020 2020 2074  e:.            t
+00001700: 6f6b 656e 7320 3d20 7374 6174 656d 656e  okens = statemen
+00001710: 7473 5b30 5d2e 746f 6b65 6e73 5b6f 7264  ts[0].tokens[ord
+00001720: 6572 5f62 795f 696e 6465 783a 5d0a 2020  er_by_index:].  
+00001730: 2020 2020 2020 2020 2020 6c69 6d69 7465            limite
+00001740: 645f 7175 6572 7920 2b3d 2027 5c6e 2720  d_query += '\n' 
+00001750: 2b20 2727 2e6a 6f69 6e28 7374 7228 746f  + ''.join(str(to
+00001760: 6b65 6e29 2066 6f72 2074 6f6b 656e 2069  ken) for token i
+00001770: 6e20 746f 6b65 6e73 290a 0a20 2020 2020  n tokens)..     
+00001780: 2020 2072 6574 7572 6e20 6c69 6d69 7465     return limite
+00001790: 645f 7175 6572 790a 2020 2020 0a0a 2020  d_query.    ..  
+000017a0: 2020 6465 6620 5f6c 696d 6974 5f70 6172    def _limit_par
+000017b0: 7365 645f 7175 6572 7928 7365 6c66 2c20  sed_query(self, 
+000017c0: 7175 6572 793a 2073 7472 2c20 6c69 6d69  query: str, limi
+000017d0: 743a 2069 6e74 2920 2d3e 2073 7472 3a0a  t: int) -> str:.
+000017e0: 2020 2020 2020 2020 7261 6973 6520 4e6f          raise No
+000017f0: 7449 6d70 6c65 6d65 6e74 6564 4572 726f  tImplementedErro
+00001800: 7228 290a 0a0a 2020 2020 6465 6620 6275  r()...    def bu
+00001810: 696c 645f 7175 6572 795f 7769 7468 5f70  ild_query_with_p
+00001820: 6f73 6974 696f 6e61 6c5f 7061 7261 6d73  ositional_params
+00001830: 2873 656c 662c 2071 7565 7279 3a20 7374  (self, query: st
+00001840: 722c 2070 6172 616d 733a 206c 6973 747c  r, params: list|
+00001850: 7475 706c 657c 6469 6374 293a 0a20 2020  tuple|dict):.   
+00001860: 2020 2020 2069 6620 6973 696e 7374 616e       if isinstan
+00001870: 6365 2870 6172 616d 732c 2064 6963 7429  ce(params, dict)
+00001880: 3a0a 2020 2020 2020 2020 2020 2020 6672  :.            fr
+00001890: 6f6d 2073 716c 7061 7261 6d73 2069 6d70  om sqlparams imp
+000018a0: 6f72 7420 5351 4c50 6172 616d 7320 2320  ort SQLParams # 
+000018b0: 6e6f 7420 6174 2074 6865 2074 6f70 2062  not at the top b
+000018c0: 6563 6175 7365 2074 6865 2065 6e64 7573  ecause the endus
+000018d0: 6572 206d 6967 6874 206e 6f74 206e 6565  er might not nee
+000018e0: 6420 7468 6973 2066 6561 7475 7265 0a0a  d this feature..
+000018f0: 2020 2020 2020 2020 2020 2020 6966 206e              if n
+00001900: 6f74 2068 6173 6174 7472 2873 656c 662e  ot hasattr(self.
+00001910: 5f5f 636c 6173 735f 5f2c 2027 5f70 6172  __class__, '_par
+00001920: 616d 735f 666f 726d 6174 7465 7227 293a  ams_formatter'):
+00001930: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001940: 2073 656c 662e 5f5f 636c 6173 735f 5f2e   self.__class__.
+00001950: 5f70 6172 616d 735f 666f 726d 6174 7465  _params_formatte
+00001960: 7220 3d20 5351 4c50 6172 616d 7328 276e  r = SQLParams('n
+00001970: 616d 6564 272c 2027 716d 6172 6b27 290a  amed', 'qmark').
+00001980: 2020 2020 2020 2020 2020 2020 7175 6572              quer
+00001990: 792c 2070 6172 616d 7320 3d20 7365 6c66  y, params = self
+000019a0: 2e5f 5f63 6c61 7373 5f5f 2e5f 7061 7261  .__class__._para
+000019b0: 6d73 5f66 6f72 6d61 7474 6572 2e66 6f72  ms_formatter.for
+000019c0: 6d61 7428 7175 6572 792c 2070 6172 616d  mat(query, param
+000019d0: 7329 0a0a 2020 2020 2020 2020 7265 7475  s)..        retu
+000019e0: 726e 2071 7565 7279 2c20 7061 7261 6d73  rn query, params
+000019f0: 0a20 2020 2020 2020 200a 0a20 2020 2064  .        ..    d
+00001a00: 6566 2067 6574 5f63 7572 736f 725f 636f  ef get_cursor_co
+00001a10: 6c75 6d6e 7328 7365 6c66 2c20 6375 7273  lumns(self, curs
+00001a20: 6f72 3a20 545f 4375 7273 6f72 2920 2d3e  or: T_Cursor) ->
+00001a30: 206c 6973 745b 436f 6c75 6d6e 5d3a 0a20   list[Column]:. 
+00001a40: 2020 2020 2020 2063 6f6c 756d 6e73 203d         columns =
+00001a50: 205b 5d0a 0a20 2020 2020 2020 2066 6f72   []..        for
+00001a60: 2069 2c20 696e 666f 2069 6e20 656e 756d   i, info in enum
+00001a70: 6572 6174 6528 6375 7273 6f72 2e64 6573  erate(cursor.des
+00001a80: 6372 6970 7469 6f6e 293a 0a20 2020 2020  cription):.     
+00001a90: 2020 2020 2020 206e 616d 652c 2070 7974         name, pyt
+00001aa0: 686f 6e5f 7479 7065 2c20 6469 7370 6c61  hon_type, displa
+00001ab0: 795f 7369 7a65 2c20 696e 7465 726e 616c  y_size, internal
+00001ac0: 5f73 697a 652c 2070 7265 6369 7369 6f6e  _size, precision
+00001ad0: 2c20 7363 616c 652c 206e 756c 6c5f 6f6b  , scale, null_ok
+00001ae0: 203d 2069 6e66 6f0a 2020 2020 2020 2020   = info.        
+00001af0: 2020 2020 636f 6c75 6d6e 203d 2043 6f6c      column = Col
+00001b00: 756d 6e28 6e61 6d65 2c20 706f 7369 7469  umn(name, positi
+00001b10: 6f6e 3d69 202b 2031 2c20 7079 7468 6f6e  on=i + 1, python
+00001b20: 5f74 7970 653d 7079 7468 6f6e 5f74 7970  _type=python_typ
+00001b30: 652c 2064 6973 706c 6179 5f73 697a 653d  e, display_size=
+00001b40: 6469 7370 6c61 795f 7369 7a65 2c20 696e  display_size, in
+00001b50: 7465 726e 616c 5f73 697a 653d 696e 7465  ternal_size=inte
+00001b60: 726e 616c 5f73 697a 652c 2070 7265 6369  rnal_size, preci
+00001b70: 7369 6f6e 3d70 7265 6369 7369 6f6e 2c20  sion=precision, 
+00001b80: 7363 616c 653d 7363 616c 652c 206e 756c  scale=scale, nul
+00001b90: 6c5f 6f6b 3d6e 756c 6c5f 6f6b 290a 2020  l_ok=null_ok).  
+00001ba0: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00001bb0: 6669 785f 6375 7273 6f72 5f63 6f6c 756d  fix_cursor_colum
+00001bc0: 6e5f 6465 6669 6e69 7469 6f6e 2863 6f6c  n_definition(col
+00001bd0: 756d 6e29 0a20 2020 2020 2020 2020 2020  umn).           
+00001be0: 2063 6f6c 756d 6e2e 5f63 6865 636b 2829   column._check()
+00001bf0: 0a20 2020 2020 2020 2020 2020 2063 6f6c  .            col
+00001c00: 756d 6e73 2e61 7070 656e 6428 636f 6c75  umns.append(colu
+00001c10: 6d6e 290a 2020 2020 2020 2020 0a20 2020  mn).        .   
+00001c20: 2020 2020 2072 6574 7572 6e20 636f 6c75       return colu
+00001c30: 6d6e 730a 0a0a 2020 2020 6465 6620 5f66  mns...    def _f
+00001c40: 6978 5f63 7572 736f 725f 636f 6c75 6d6e  ix_cursor_column
+00001c50: 5f64 6566 696e 6974 696f 6e28 7365 6c66  _definition(self
+00001c60: 2c20 636f 6c75 6d6e 3a20 436f 6c75 6d6e  , column: Column
+00001c70: 293a 0a20 2020 2020 2020 2022 2222 0a20  ):.        """. 
+00001c80: 2020 2020 2020 2046 6978 2061 2063 6f6c         Fix a col
+00001c90: 756d 6e20 6465 6669 6e69 7469 6f6e 2c20  umn definition, 
+00001ca0: 6166 7465 7220 696e 7374 616e 6369 6174  after instanciat
+00001cb0: 696f 6e20 696e 2060 6765 745f 636f 6c75  ion in `get_colu
+00001cc0: 6d6e 735f 6672 6f6d 5f63 7572 736f 7260  mns_from_cursor`
+00001cd0: 2c20 666f 7220 6120 7370 6563 6966 6963  , for a specific
+00001ce0: 2064 6174 6162 6173 6520 656e 6769 6e65   database engine
+00001cf0: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
+00001d00: 2020 2020 2020 7061 7373 0a20 2020 2020        pass.     
+00001d10: 2020 200a 0a20 2020 2064 6566 2067 6574     ..    def get
+00001d20: 5f63 7572 736f 725f 636f 6c75 6d6e 5f6e  _cursor_column_n
+00001d30: 616d 6573 2873 656c 662c 2063 7572 736f  ames(self, curso
+00001d40: 723a 2054 5f43 7572 736f 7229 202d 3e20  r: T_Cursor) -> 
+00001d50: 6c69 7374 5b73 7472 5d3a 0a20 2020 2020  list[str]:.     
+00001d60: 2020 2072 6574 7572 6e20 5b69 6e66 6f5b     return [info[
+00001d70: 305d 2066 6f72 2069 6e66 6f20 696e 2063  0] for info in c
+00001d80: 7572 736f 722e 6465 7363 7269 7074 696f  ursor.descriptio
+00001d90: 6e5d 0a20 2020 2020 2020 200a 0a20 2020  n].        ..   
+00001da0: 2064 6566 2067 6574 5f74 6162 6c65 5f63   def get_table_c
+00001db0: 6f6c 756d 6e73 2873 656c 662c 2074 6162  olumns(self, tab
+00001dc0: 6c65 3a20 7374 727c 7475 706c 6529 202d  le: str|tuple) -
+00001dd0: 3e20 6c69 7374 5b43 6f6c 756d 6e5d 3a0a  > list[Column]:.
+00001de0: 2020 2020 2020 2020 7175 6572 7920 3d20          query = 
+00001df0: 7365 6c66 2e67 6574 5f73 656c 6563 745f  self.get_select_
+00001e00: 7461 626c 655f 7175 6572 7928 7461 626c  table_query(tabl
+00001e10: 652c 2073 6368 656d 615f 6f6e 6c79 3d54  e, schema_only=T
+00001e20: 7275 6529 0a20 2020 2020 2020 2077 6974  rue).        wit
+00001e30: 6820 7365 6c66 2e65 7865 6375 7465 5f67  h self.execute_g
+00001e40: 6574 5f63 7572 736f 7228 7175 6572 7929  et_cursor(query)
+00001e50: 2061 7320 6375 7273 6f72 3a0a 2020 2020   as cursor:.    
+00001e60: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00001e70: 656c 662e 6765 745f 6375 7273 6f72 5f63  elf.get_cursor_c
+00001e80: 6f6c 756d 6e73 2863 7572 736f 7229 0a0a  olumns(cursor)..
+00001e90: 0a20 2020 2064 6566 2067 6574 5f74 6162  .    def get_tab
+00001ea0: 6c65 5f63 6f6c 756d 6e5f 6e61 6d65 7328  le_column_names(
+00001eb0: 7365 6c66 2c20 7461 626c 653a 2073 7472  self, table: str
+00001ec0: 7c74 7570 6c65 2920 2d3e 206c 6973 745b  |tuple) -> list[
+00001ed0: 7374 725d 3a0a 2020 2020 2020 2020 636f  str]:.        co
+00001ee0: 6c75 6d6e 5f6e 616d 6573 203d 205b 5d0a  lumn_names = [].
+00001ef0: 2020 2020 2020 2020 666f 7220 636f 6c75          for colu
+00001f00: 6d6e 2069 6e20 7365 6c66 2e67 6574 5f74  mn in self.get_t
+00001f10: 6162 6c65 5f63 6f6c 756d 6e73 2874 6162  able_columns(tab
+00001f20: 6c65 293a 0a20 2020 2020 2020 2020 2020  le):.           
+00001f30: 2063 6f6c 756d 6e5f 6e61 6d65 732e 6170   column_names.ap
+00001f40: 7065 6e64 2863 6f6c 756d 6e2e 6e61 6d65  pend(column.name
+00001f50: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+00001f60: 2063 6f6c 756d 6e5f 6e61 6d65 730a 0a0a   column_names...
+00001f70: 2020 2020 4063 6c61 7373 6d65 7468 6f64      @classmethod
+00001f80: 0a20 2020 2064 6566 2073 706c 6974 5f6e  .    def split_n
+00001f90: 616d 6528 636c 732c 2066 756c 6c5f 6e61  ame(cls, full_na
+00001fa0: 6d65 3a20 7374 727c 7475 706c 6529 202d  me: str|tuple) -
+00001fb0: 3e20 7475 706c 655b 7374 722c 7374 725d  > tuple[str,str]
+00001fc0: 3a0a 2020 2020 2020 2020 6966 2069 7369  :.        if isi
+00001fd0: 6e73 7461 6e63 6528 6675 6c6c 5f6e 616d  nstance(full_nam
+00001fe0: 652c 2074 7570 6c65 293a 0a20 2020 2020  e, tuple):.     
+00001ff0: 2020 2020 2020 2072 6574 7572 6e20 6675         return fu
+00002000: 6c6c 5f6e 616d 650a 2020 2020 2020 2020  ll_name.        
+00002010: 0a20 2020 2020 2020 2074 7279 3a0a 2020  .        try:.  
+00002020: 2020 2020 2020 2020 2020 706f 7320 3d20            pos = 
+00002030: 6675 6c6c 5f6e 616d 652e 696e 6465 7828  full_name.index(
+00002040: 272e 2729 0a20 2020 2020 2020 2020 2020  '.').           
+00002050: 2073 6368 656d 615f 6e61 6d65 203d 2066   schema_name = f
+00002060: 756c 6c5f 6e61 6d65 5b30 3a70 6f73 5d0a  ull_name[0:pos].
+00002070: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
+00002080: 203d 2066 756c 6c5f 6e61 6d65 5b70 6f73   = full_name[pos
+00002090: 2b31 3a5d 0a20 2020 2020 2020 2065 7863  +1:].        exc
+000020a0: 6570 7420 5661 6c75 6545 7272 6f72 3a0a  ept ValueError:.
+000020b0: 2020 2020 2020 2020 2020 2020 7363 6865              sche
+000020c0: 6d61 5f6e 616d 6520 3d20 636c 732e 6465  ma_name = cls.de
+000020d0: 6661 756c 745f 7363 6865 6d61 5f6e 616d  fault_schema_nam
+000020e0: 650a 2020 2020 2020 2020 2020 2020 6e61  e.            na
+000020f0: 6d65 203d 2066 756c 6c5f 6e61 6d65 0a0a  me = full_name..
+00002100: 2020 2020 2020 2020 7265 7475 726e 2028          return (
+00002110: 7363 6865 6d61 5f6e 616d 652c 206e 616d  schema_name, nam
+00002120: 6529 0a20 2020 200a 0a20 2020 2064 6566  e).    ..    def
+00002130: 2067 6574 5f73 656c 6563 745f 7461 626c   get_select_tabl
+00002140: 655f 7175 6572 7928 7365 6c66 2c20 7461  e_query(self, ta
+00002150: 626c 653a 2073 7472 7c74 7570 6c65 2c20  ble: str|tuple, 
+00002160: 7363 6865 6d61 5f6f 6e6c 7920 3d20 4661  schema_only = Fa
+00002170: 6c73 6529 3a0a 2020 2020 2020 2020 2222  lse):.        ""
+00002180: 220a 2020 2020 2020 2020 4275 696c 6420  ".        Build 
+00002190: 6120 7175 6572 7920 6f6e 2074 6865 2067  a query on the g
+000021a0: 6976 656e 2074 6162 6c65 2e0a 0a20 2020  iven table...   
+000021b0: 2020 2020 2049 6620 6073 6368 656d 615f       If `schema_
+000021c0: 6f6e 6c79 6020 6973 2067 6976 656e 2c20  only` is given, 
+000021d0: 6e6f 2072 6f77 2077 696c 6c20 6265 2072  no row will be r
+000021e0: 6574 7572 6e65 6420 2874 6869 7320 6973  eturned (this is
+000021f0: 2075 7365 6420 746f 2067 6574 2069 6e66   used to get inf
+00002200: 6f72 6d61 7469 6f6e 206f 6e20 7468 6520  ormation on the 
+00002210: 7461 626c 6529 2e0a 2020 2020 2020 2020  table)..        
+00002220: 4f74 6865 7277 6973 652c 2061 6c6c 2072  Otherwise, all r
+00002230: 6f77 7320 7769 6c6c 2062 6520 7265 7475  ows will be retu
+00002240: 726e 6564 2e0a 0a20 2020 2020 2020 2054  rned...        T
+00002250: 6865 2072 6574 7572 6e20 7479 7065 206f  he return type o
+00002260: 6620 7468 6973 2066 756e 6374 696f 6e20  f this function 
+00002270: 6465 7065 6e64 7320 6f6e 2074 6865 2064  depends on the d
+00002280: 6174 6162 6173 6520 656e 6769 6e65 2e0a  atabase engine..
+00002290: 2020 2020 2020 2020 4974 2069 7320 7061          It is pa
+000022a0: 7373 6564 2064 6972 6563 746c 7920 746f  ssed directly to
+000022b0: 2074 6865 2063 7572 736f 7227 7320 6578   the cursor's ex
+000022c0: 6563 7574 6520 6675 6e63 7469 6f6e 2066  ecute function f
+000022d0: 6f72 2074 6869 7320 656e 6769 6e65 2e0a  or this engine..
+000022e0: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
+000022f0: 2020 2020 7261 6973 6520 4e6f 7449 6d70      raise NotImp
+00002300: 6c65 6d65 6e74 6564 4572 726f 7228 290a  lementedError().
+00002310: 2020 2020 0a0a 2020 2020 6465 6620 6578      ..    def ex
+00002320: 6563 7574 655f 6765 745f 6375 7273 6f72  ecute_get_cursor
+00002330: 2873 656c 662c 2071 7565 7279 3a20 7374  (self, query: st
+00002340: 722c 2070 6172 616d 733a 206c 6973 747c  r, params: list|
+00002350: 7475 706c 657c 6469 6374 203d 204e 6f6e  tuple|dict = Non
+00002360: 652c 206c 696d 6974 3a20 696e 7420 3d20  e, limit: int = 
+00002370: 4e6f 6e65 2920 2d3e 2054 5f43 7572 736f  None) -> T_Curso
+00002380: 723a 0a20 2020 2020 2020 2022 2222 0a20  r:.        """. 
+00002390: 2020 2020 2020 204d 7573 7420 6265 2063         Must be c
+000023a0: 6c6f 7365 642e 0a20 2020 2020 2020 2022  losed..        "
+000023b0: 2222 0a20 2020 2020 2020 2072 6169 7365  "".        raise
+000023c0: 204e 6f74 496d 706c 656d 656e 7465 6445   NotImplementedE
+000023d0: 7272 6f72 2829 0a20 2020 200a 0a20 2020  rror().    ..   
+000023e0: 2064 6566 2065 7865 6375 7465 5f67 6574   def execute_get
+000023f0: 5f73 6361 6c61 7228 7365 6c66 2c20 7175  _scalar(self, qu
+00002400: 6572 793a 2073 7472 2c20 7061 7261 6d73  ery: str, params
+00002410: 3a20 6c69 7374 7c74 7570 6c65 7c64 6963  : list|tuple|dic
+00002420: 7420 3d20 4e6f 6e65 2c20 6c69 6d69 743a  t = None, limit:
+00002430: 2069 6e74 203d 204e 6f6e 6529 3a0a 2020   int = None):.  
+00002440: 2020 2020 2020 7769 7468 2073 656c 662e        with self.
+00002450: 6578 6563 7574 655f 6765 745f 6375 7273  execute_get_curs
+00002460: 6f72 2871 7565 7279 2c20 7061 7261 6d73  or(query, params
+00002470: 2c20 6c69 6d69 743d 6c69 6d69 7429 2061  , limit=limit) a
+00002480: 7320 6375 7273 6f72 3a0a 2020 2020 2020  s cursor:.      
+00002490: 2020 2020 2020 7265 7375 6c74 203d 206e        result = n
+000024a0: 6578 7428 6375 7273 6f72 290a 2020 2020  ext(cursor).    
+000024b0: 2020 2020 2020 2020 0a20 2020 2020 2020          .       
+000024c0: 2020 2020 2023 2043 6865 636b 206f 6e6c       # Check onl
+000024d0: 7920 6f6e 6520 726f 770a 2020 2020 2020  y one row.      
+000024e0: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     
+000024f0: 2020 2020 2020 2020 2020 206e 6578 7428             next(
+00002500: 6375 7273 6f72 290a 2020 2020 2020 2020  cursor).        
+00002510: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
+00002520: 6c75 6545 7272 6f72 2866 2273 6576 6572  lueError(f"sever
+00002530: 616c 2072 6f77 7320 7265 7475 726e 6564  al rows returned
+00002540: 2062 7920 7175 6572 7922 290a 2020 2020   by query").    
+00002550: 2020 2020 2020 2020 6578 6365 7074 2053          except S
+00002560: 746f 7049 7465 7261 7469 6f6e 3a0a 2020  topIteration:.  
+00002570: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00002580: 7373 0a0a 2020 2020 2020 2020 2020 2020  ss..            
+00002590: 2320 4368 6563 6b20 6f6e 6c79 206f 6e65  # Check only one
+000025a0: 2076 616c 7565 0a20 2020 2020 2020 2020   value.         
+000025b0: 2020 2069 6620 6c65 6e28 7265 7375 6c74     if len(result
+000025c0: 2920 3e20 313a 0a20 2020 2020 2020 2020  ) > 1:.         
+000025d0: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+000025e0: 7565 4572 726f 7228 2273 6576 6572 616c  ueError("several
+000025f0: 2076 616c 7565 7320 7265 7475 726e 6564   values returned
+00002600: 2062 7920 7175 6572 7922 290a 0a20 2020   by query")..   
+00002610: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+00002620: 7265 7375 6c74 5b30 5d0a 0a0a 2020 2020  result[0]...    
+00002630: 6465 6620 6578 6563 7574 6528 7365 6c66  def execute(self
+00002640: 2c20 7175 6572 793a 2073 7472 2c20 7061  , query: str, pa
+00002650: 7261 6d73 3a20 6c69 7374 7c74 7570 6c65  rams: list|tuple
+00002660: 7c64 6963 7420 3d20 4e6f 6e65 2c20 6c69  |dict = None, li
+00002670: 6d69 743a 2069 6e74 203d 204e 6f6e 6529  mit: int = None)
+00002680: 3a0a 2020 2020 2020 2020 7769 7468 2073  :.        with s
+00002690: 656c 662e 6578 6563 7574 655f 6765 745f  elf.execute_get_
+000026a0: 6375 7273 6f72 2871 7565 7279 2c20 7061  cursor(query, pa
+000026b0: 7261 6d73 2c20 6c69 6d69 743d 6c69 6d69  rams, limit=limi
+000026c0: 7429 3a0a 2020 2020 2020 2020 2020 2020  t):.            
+000026d0: 7061 7373 0a0a 0a20 2020 2064 6566 2065  pass...    def e
+000026e0: 7865 6375 7465 5f66 696c 6528 7365 6c66  xecute_file(self
+000026f0: 2c20 7061 7468 3a20 7374 727c 5061 7468  , path: str|Path
+00002700: 2c20 7061 7261 6d73 3a20 6c69 7374 7c74  , params: list|t
+00002710: 7570 6c65 7c64 6963 7420 3d20 4e6f 6e65  uple|dict = None
+00002720: 2c20 6c69 6d69 743a 2069 6e74 203d 204e  , limit: int = N
+00002730: 6f6e 6529 3a0a 2020 2020 2020 2020 6966  one):.        if
+00002740: 206e 6f74 2069 7369 6e73 7461 6e63 6528   not isinstance(
+00002750: 7061 7468 2c20 5061 7468 293a 0a20 2020  path, Path):.   
+00002760: 2020 2020 2020 2020 2070 6174 6820 3d20           path = 
+00002770: 5061 7468 2870 6174 6829 0a20 2020 2020  Path(path).     
+00002780: 2020 2020 2020 200a 2020 2020 2020 2020         .        
+00002790: 7175 6572 7920 3d20 7061 7468 2e72 6561  query = path.rea
+000027a0: 645f 7465 7874 2865 6e63 6f64 696e 673d  d_text(encoding=
+000027b0: 2775 7466 2d38 2729 0a20 2020 2020 2020  'utf-8').       
+000027c0: 2073 656c 662e 6578 6563 7574 6528 7175   self.execute(qu
+000027d0: 6572 792c 2070 6172 616d 732c 206c 696d  ery, params, lim
+000027e0: 6974 3d6c 696d 6974 290a 0a0a 2020 2020  it=limit)...    
+000027f0: 6465 6620 6361 6c6c 5f70 726f 6365 6475  def call_procedu
+00002800: 7265 2873 656c 662c 206e 616d 653a 2073  re(self, name: s
+00002810: 7472 7c74 7570 6c65 2c20 2a61 7267 7329  tr|tuple, *args)
+00002820: 3a0a 2020 2020 2020 2020 7261 6973 6520  :.        raise 
+00002830: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr
+00002840: 726f 7228 290a 0a0a 2020 2020 6465 6620  ror()...    def 
+00002850: 7461 626c 655f 6578 6973 7473 2873 656c  table_exists(sel
+00002860: 662c 2074 6162 6c65 3a20 7374 727c 7475  f, table: str|tu
+00002870: 706c 6529 202d 3e20 626f 6f6c 3a0a 2020  ple) -> bool:.  
+00002880: 2020 2020 2020 7261 6973 6520 4e6f 7449        raise NotI
+00002890: 6d70 6c65 6d65 6e74 6564 4572 726f 7228  mplementedError(
+000028a0: 290a 0a0a 2020 2020 6465 6620 7472 756e  )...    def trun
+000028b0: 6361 7465 5f74 6162 6c65 2873 656c 662c  cate_table(self,
+000028c0: 2074 6162 6c65 3a20 7374 727c 7475 706c   table: str|tupl
+000028d0: 6529 3a0a 2020 2020 2020 2020 7261 6973  e):.        rais
+000028e0: 6520 4e6f 7449 6d70 6c65 6d65 6e74 6564  e NotImplemented
+000028f0: 4572 726f 7228 290a 0a0a 2020 2020 6465  Error()...    de
+00002900: 6620 636f 7079 5f66 726f 6d5f 6373 7628  f copy_from_csv(
+00002910: 7365 6c66 2c20 6670 3a20 494f 4261 7365  self, fp: IOBase
+00002920: 2c20 7461 626c 653a 2073 7472 7c74 7570  , table: str|tup
+00002930: 6c65 2c20 636f 6c75 6d6e 733a 206c 6973  le, columns: lis
+00002940: 745b 7374 725d 203d 204e 6f6e 652c 2064  t[str] = None, d
+00002950: 656c 696d 6974 6572 3a20 7374 7220 3d20  elimiter: str = 
+00002960: 4e6f 6e65 2c20 7175 6f74 6563 6861 723a  None, quotechar:
+00002970: 2073 7472 203d 2027 2227 2c20 6e75 6c6c   str = '"', null
+00002980: 6368 6172 3a20 7374 7220 3d20 2727 2c20  char: str = '', 
+00002990: 6e6f 6865 6164 6572 3a20 626f 6f6c 203d  noheader: bool =
+000029a0: 2046 616c 7365 293a 0a20 2020 2020 2020   False):.       
+000029b0: 2072 6169 7365 204e 6f74 496d 706c 656d   raise NotImplem
+000029c0: 656e 7465 6445 7272 6f72 2829 0a20 2020  entedError().   
+000029d0: 200a 0a20 2020 2064 6566 2064 6570 6c6f   ..    def deplo
+000029e0: 795f 7371 6c28 7365 6c66 2c20 2a70 6174  y_sql(self, *pat
+000029f0: 6873 3a20 5061 7468 7c73 7472 2c20 656e  hs: Path|str, en
+00002a00: 636f 6469 6e67 203d 2022 7574 662d 3822  coding = "utf-8"
+00002a10: 2c20 2a2a 6b77 6172 6773 293a 0a20 2020  , **kwargs):.   
+00002a20: 2020 2020 2066 6f72 2064 6174 6120 696e       for data in
+00002a30: 2067 6574 5f73 716c 5f66 696c 655f 6461   get_sql_file_da
+00002a40: 7461 282a 7061 7468 732c 2065 6e63 6f64  ta(*paths, encod
+00002a50: 696e 673d 656e 636f 6469 6e67 2c20 2a2a  ing=encoding, **
+00002a60: 6b77 6172 6773 293a 0a20 2020 2020 2020  kwargs):.       
+00002a70: 2020 2020 206c 6f67 6765 722e 696e 666f       logger.info
+00002a80: 2822 6578 6563 7574 6520 2573 222c 2064  ("execute %s", d
+00002a90: 6174 612e 7061 7468 290a 2020 2020 2020  ata.path).      
+00002aa0: 2020 2020 2020 7365 6c66 2e65 7865 6375        self.execu
+00002ab0: 7465 2864 6174 612e 7371 6c29 0a0a 0a20  te(data.sql)... 
+00002ac0: 2020 2064 6566 2072 6576 6572 745f 7371     def revert_sq
+00002ad0: 6c28 7365 6c66 2c20 2a70 6174 6873 3a20  l(self, *paths: 
+00002ae0: 5061 7468 7c73 7472 2c20 656e 636f 6469  Path|str, encodi
+00002af0: 6e67 203d 2022 7574 662d 3822 293a 0a20  ng = "utf-8"):. 
+00002b00: 2020 2020 2020 2061 6374 7561 6c5f 7061         actual_pa
+00002b10: 7468 733a 206c 6973 745b 5061 7468 5d20  ths: list[Path] 
+00002b20: 3d20 5b5d 0a20 2020 2020 2020 2066 6f72  = [].        for
+00002b30: 2070 6174 6820 696e 2070 6174 6873 3a0a   path in paths:.
+00002b40: 2020 2020 2020 2020 2020 2020 6966 2069              if i
+00002b50: 7369 6e73 7461 6e63 6528 7061 7468 2c20  sinstance(path, 
+00002b60: 7374 7229 3a0a 2020 2020 2020 2020 2020  str):.          
+00002b70: 2020 2020 2020 7061 7468 203d 2050 6174        path = Pat
+00002b80: 6828 7061 7468 290a 2020 2020 2020 2020  h(path).        
+00002b90: 2020 2020 6163 7475 616c 5f70 6174 6873      actual_paths
+00002ba0: 2e61 7070 656e 6428 7061 7468 290a 0a20  .append(path).. 
+00002bb0: 2020 2020 2020 2061 6374 7561 6c5f 7061         actual_pa
+00002bc0: 7468 732e 736f 7274 2872 6576 6572 7365  ths.sort(reverse
+00002bd0: 3d54 7275 6529 0a0a 2020 2020 2020 2020  =True)..        
+00002be0: 666f 7220 7061 7468 2069 6e20 6163 7475  for path in actu
+00002bf0: 616c 5f70 6174 6873 3a0a 2020 2020 2020  al_paths:.      
+00002c00: 2020 2020 2020 6966 2070 6174 682e 6973        if path.is
+00002c10: 5f64 6972 2829 3a0a 2020 2020 2020 2020  _dir():.        
+00002c20: 2020 2020 2020 2020 7375 6270 6174 6873          subpaths
+00002c30: 203d 2073 6f72 7465 6428 7061 7468 2e69   = sorted(path.i
+00002c40: 7465 7264 6972 2829 290a 2020 2020 2020  terdir()).      
+00002c50: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
+00002c60: 6576 6572 745f 7371 6c28 2a73 7562 7061  evert_sql(*subpa
+00002c70: 7468 732c 2065 6e63 6f64 696e 673d 656e  ths, encoding=en
+00002c80: 636f 6469 6e67 290a 0a20 2020 2020 2020  coding)..       
+00002c90: 2020 2020 2065 6c69 6620 6e6f 7420 7061       elif not pa
+00002ca0: 7468 2e6e 616d 652e 656e 6473 7769 7468  th.name.endswith
+00002cb0: 2822 5f72 6576 6572 742e 7371 6c22 293a  ("_revert.sql"):
+00002cc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002cd0: 2063 6f6e 7469 6e75 6520 2320 6967 6e6f   continue # igno
+00002ce0: 7265 0a0a 2020 2020 2020 2020 2020 2020  re..            
+00002cf0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00002d00: 2020 2020 2020 6c6f 6767 6572 2e69 6e66        logger.inf
+00002d10: 6f28 2265 7865 6375 7465 2025 7322 2c20  o("execute %s", 
+00002d20: 7061 7468 290a 2020 2020 2020 2020 2020  path).          
+00002d30: 2020 2020 2020 7175 6572 7920 3d20 7061        query = pa
+00002d40: 7468 2e72 6561 645f 7465 7874 2865 6e63  th.read_text(enc
+00002d50: 6f64 696e 673d 656e 636f 6469 6e67 290a  oding=encoding).
+00002d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002d70: 7365 6c66 2e65 7865 6375 7465 2871 7565  self.execute(que
+00002d80: 7279 290a 0a0a 636c 6173 7320 436f 6c75  ry)...class Colu
+00002d90: 6d6e 3a0a 2020 2020 6465 6620 5f5f 696e  mn:.    def __in
+00002da0: 6974 5f5f 2873 656c 662c 206e 616d 653a  it__(self, name:
+00002db0: 2073 7472 2c20 2a2c 2070 6f73 6974 696f   str, *, positio
+00002dc0: 6e3a 2069 6e74 203d 204e 6f6e 652c 2070  n: int = None, p
+00002dd0: 7974 686f 6e5f 7479 7065 3a20 7479 7065  ython_type: type
+00002de0: 203d 204e 6f6e 652c 206f 7269 6769 6e61   = None, origina
+00002df0: 6c5f 7479 7065 3a20 7374 7220 3d20 4e6f  l_type: str = No
+00002e00: 6e65 2c20 7072 6563 6973 696f 6e3a 2069  ne, precision: i
+00002e10: 6e74 203d 204e 6f6e 652c 2073 6361 6c65  nt = None, scale
+00002e20: 3a20 696e 7420 3d20 4e6f 6e65 2c20 6e75  : int = None, nu
+00002e30: 6c6c 5f6f 6b3a 2062 6f6f 6c20 3d20 4e6f  ll_ok: bool = No
+00002e40: 6e65 2c20 6469 7370 6c61 795f 7369 7a65  ne, display_size
+00002e50: 3a20 696e 7420 3d20 4e6f 6e65 2c20 696e  : int = None, in
+00002e60: 7465 726e 616c 5f73 697a 653a 2069 6e74  ternal_size: int
+00002e70: 203d 204e 6f6e 652c 2070 7269 6d61 7279   = None, primary
+00002e80: 5f6b 6579 3a20 626f 6f6c 203d 2046 616c  _key: bool = Fal
+00002e90: 7365 2c20 6465 6661 756c 743a 2073 7472  se, default: str
+00002ea0: 203d 204e 6f6e 6529 3a0a 2020 2020 2020   = None):.      
+00002eb0: 2020 7365 6c66 2e6e 616d 653a 2073 7472    self.name: str
+00002ec0: 7c4e 6f6e 6520 3d20 6e61 6d65 0a20 2020  |None = name.   
+00002ed0: 2020 2020 2022 2222 204e 6f6d 2064 6520       """ Nom de 
+00002ee0: 6c61 2063 6f6c 6f6e 6e65 2e20 5065 7574  la colonne. Peut
+00002ef0: 20c3 aa74 7265 2076 6964 6520 2870 6172   ..tre vide (par
+00002f00: 2065 7865 6d70 6c65 2070 6f75 7220 6c65   exemple pour le
+00002f10: 7320 636f 6c6f 6e6e 6573 2063 616c 6375  s colonnes calcu
+00002f20: 6cc3 a965 7320 7361 6e73 2061 6c69 6173  l..es sans alias
+00002f30: 2064 c3a9 6669 6e69 292e 2022 2222 0a0a   d..fini). """..
+00002f40: 2020 2020 2020 2020 7365 6c66 2e70 6f73          self.pos
+00002f50: 6974 696f 6e3a 2069 6e74 7c4e 6f6e 6520  ition: int|None 
+00002f60: 3d20 706f 7369 7469 6f6e 0a20 2020 2020  = position.     
+00002f70: 2020 2022 2222 204f 7264 696e 616c 2070     """ Ordinal p
+00002f80: 6f73 6974 696f 6e2c 2073 7461 7274 696e  osition, startin
+00002f90: 6720 6672 6f6d 2031 2e20 2222 220a 0a20  g from 1. """.. 
+00002fa0: 2020 2020 2020 2073 656c 662e 7079 7468         self.pyth
+00002fb0: 6f6e 5f74 7970 653a 2074 7970 657c 4e6f  on_type: type|No
+00002fc0: 6e65 203d 2070 7974 686f 6e5f 7479 7065  ne = python_type
+00002fd0: 0a20 2020 2020 2020 2022 2222 2054 7970  .        """ Typ
+00002fe0: 6520 5079 7468 6f6e 2075 7469 6c69 73c3  e Python utilis.
+00002ff0: a920 7061 7220 6c65 2064 7269 7665 7220  . par le driver 
+00003000: 706f 7572 2072 6570 72c3 a973 656e 7465  pour repr..sente
+00003010: 7220 6c65 7320 7661 6c65 7572 732e 0a20  r les valeurs.. 
+00003020: 2020 2020 2020 2041 7474 656e 7469 6f6e         Attention
+00003030: 203a 206c 6573 2076 616c 6575 7273 2064   : les valeurs d
+00003040: 6520 7479 7065 2053 514c 2044 4154 4520  e type SQL DATE 
+00003050: 6f75 2044 4154 4554 494d 4520 7065 7576  ou DATETIME peuv
+00003060: 656e 7420 c3aa 7472 6520 6d61 7070 c3a9  ent ..tre mapp..
+00003070: 6573 2061 7520 7479 7065 2050 7974 686f  es au type Pytho
+00003080: 6e20 6073 7472 6020 7375 6976 616e 7420  n `str` suivant 
+00003090: 6c65 2064 7269 7665 7220 7574 696c 6973  le driver utilis
+000030a0: c3a9 2e20 2222 220a 2020 2020 2020 2020  ... """.        
+000030b0: 2320 4366 2e20 6874 7470 733a 2f2f 7374  # Cf. https://st
+000030c0: 6163 6b6f 7665 7266 6c6f 772e 636f 6d2f  ackoverflow.com/
+000030d0: 7175 6573 7469 6f6e 732f 3731 3732 3534  questions/717254
+000030e0: 302f 7079 6f64 6263 2d72 6574 7572 6e73  0/pyodbc-returns
+000030f0: 2d73 716c 2d73 6572 7665 722d 6461 7465  -sql-server-date
+00003100: 2d66 6965 6c64 732d 6173 2d73 7472 696e  -fields-as-strin
+00003110: 6773 0a0a 2020 2020 2020 2020 7365 6c66  gs..        self
+00003120: 2e6f 7269 6769 6e61 6c5f 7479 7065 3a20  .original_type: 
+00003130: 7374 727c 4e6f 6e65 203d 206f 7269 6769  str|None = origi
+00003140: 6e61 6c5f 7479 7065 0a0a 2020 2020 2020  nal_type..      
+00003150: 2020 7365 6c66 2e70 7265 6369 7369 6f6e    self.precision
+00003160: 3a20 696e 747c 4e6f 6e65 203d 2070 7265  : int|None = pre
+00003170: 6369 7369 6f6e 0a20 2020 2020 2020 2022  cision.        "
+00003180: 2222 2054 6f74 616c 206e 756d 6265 7220  "" Total number 
+00003190: 6f66 2073 6967 6e69 6669 6361 6e74 2064  of significant d
+000031a0: 6967 6974 732e 0a20 2020 2020 2020 202d  igits..        -
+000031b0: 206d 7373 716c 3a20 616c 7761 7973 2073   mssql: always s
+000031c0: 6574 2e20 2222 220a 2020 2020 2020 2020  et. """.        
+000031d0: 0a20 2020 2020 2020 2073 656c 662e 7363  .        self.sc
+000031e0: 616c 653a 2069 6e74 7c4e 6f6e 6520 3d20  ale: int|None = 
+000031f0: 7363 616c 650a 2020 2020 2020 2020 2222  scale.        ""
+00003200: 2220 436f 756e 7420 6f66 2064 6563 696d  " Count of decim
+00003210: 616c 2064 6967 6974 7320 696e 2074 6865  al digits in the
+00003220: 2066 7261 6374 696f 6e61 6c20 7061 7274   fractional part
+00003230: 206f 6620 7468 6520 7479 7065 2e0a 2020   of the type..  
+00003240: 2020 2020 2020 2d20 6d73 7371 6c3a 2061        - mssql: a
+00003250: 6c77 6179 7320 7365 742c 2075 7369 6e67  lways set, using
+00003260: 2030 2069 6620 6e6f 6e65 2e20 2222 220a   0 if none. """.
+00003270: 0a20 2020 2020 2020 2073 656c 662e 6e75  .        self.nu
+00003280: 6c6c 5f6f 6b3a 2062 6f6f 6c7c 4e6f 6e65  ll_ok: bool|None
+00003290: 203d 206e 756c 6c5f 6f6b 0a20 2020 2020   = null_ok.     
+000032a0: 2020 2022 2222 2049 6e64 6963 6174 6520     """ Indicate 
+000032b0: 6966 2074 6865 2063 6f6c 756d 6e20 6163  if the column ac
+000032c0: 6365 7074 7320 6e75 6c6c 2076 616c 7565  cepts null value
+000032d0: 2e0a 2020 2020 2020 2020 2d20 7067 3a20  ..        - pg: 
+000032e0: 616c 7761 7973 204e 6f6e 6520 6173 206e  always None as n
+000032f0: 6f74 2065 6173 7920 746f 2072 6574 7269  ot easy to retri
+00003300: 6576 6520 6672 6f6d 2074 6865 206c 6962  eve from the lib
+00003310: 7071 2e0a 2020 2020 2020 2020 2222 220a  pq..        """.
+00003320: 0a20 2020 2020 2020 2073 656c 662e 6469  .        self.di
+00003330: 7370 6c61 795f 7369 7a65 3a20 696e 747c  splay_size: int|
+00003340: 4e6f 6e65 203d 2064 6973 706c 6179 5f73  None = display_s
+00003350: 697a 650a 2020 2020 2020 2020 2222 2220  ize.        """ 
+00003360: 5468 6520 6163 7475 616c 206c 656e 6774  The actual lengt
+00003370: 6820 6f66 2074 6865 2063 6f6c 756d 6e20  h of the column 
+00003380: 696e 2062 7974 6573 2e0a 2020 2020 2020  in bytes..      
+00003390: 2020 2d20 6d73 7371 6c3a 2061 6c77 6179    - mssql: alway
+000033a0: 7320 4e6f 6e65 2028 6f72 6967 696e 616c  s None (original
+000033b0: 6c79 2061 6c77 6179 7320 7a65 726f 292e  ly always zero).
+000033c0: 2022 2222 0a0a 2020 2020 2020 2020 7365   """..        se
+000033d0: 6c66 2e69 6e74 6572 6e61 6c5f 7369 7a65  lf.internal_size
+000033e0: 3a20 696e 747c 4e6f 6e65 203d 2069 6e74  : int|None = int
+000033f0: 6572 6e61 6c5f 7369 7a65 0a20 2020 2020  ernal_size.     
+00003400: 2020 2022 2222 2054 6865 2073 697a 6520     """ The size 
+00003410: 696e 2062 7974 6573 206f 6620 7468 6520  in bytes of the 
+00003420: 636f 6c75 6d6e 2061 7373 6f63 6961 7465  column associate
+00003430: 6420 746f 2074 6869 7320 636f 6c75 6d6e  d to this column
+00003440: 206f 6e20 7468 6520 7365 7276 6572 2e0a   on the server..
+00003450: 2020 2020 2020 2020 2d20 6d73 7371 6c3a          - mssql:
+00003460: 2061 6c77 6179 7320 4e6f 6e65 2028 6f72   always None (or
+00003470: 6967 696e 616c 6c79 2061 6c77 6179 7320  iginally always 
+00003480: 7361 6d65 2061 7320 7072 6563 6973 696f  same as precisio
+00003490: 6e29 2e20 2222 220a 0a20 2020 2020 2020  n). """..       
+000034a0: 2073 656c 662e 7072 696d 6172 795f 6b65   self.primary_ke
+000034b0: 793a 2062 6f6f 6c20 3d20 7072 696d 6172  y: bool = primar
+000034c0: 795f 6b65 790a 0a20 2020 2020 2020 2073  y_key..        s
+000034d0: 656c 662e 6465 6661 756c 743a 2073 7472  elf.default: str
+000034e0: 7c4e 6f6e 6520 3d20 6465 6661 756c 740a  |None = default.
+000034f0: 0a0a 2020 2020 6465 6620 5f63 6865 636b  ..    def _check
+00003500: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00003510: 2222 220a 2020 2020 2020 2020 456e 7375  """.        Ensu
+00003520: 7265 2063 6f6e 7369 7374 656e 6379 206f  re consistency o
+00003530: 6620 636f 6c75 6d6e 2064 6566 696e 6974  f column definit
+00003540: 696f 6e2e 0a0a 2020 2020 2020 2020 4361  ion...        Ca
+00003550: 6c6c 6564 2061 6674 6572 2063 7265 6174  lled after creat
+00003560: 696f 6e20 6f66 2043 6f6c 756d 6e20 696e  ion of Column in
+00003570: 7374 616e 6365 2061 6e64 2070 6f74 656e  stance and poten
+00003580: 7469 616c 2066 6978 2061 6461 7074 6564  tial fix adapted
+00003590: 2074 6f20 7370 6563 6966 6963 2044 6174   to specific Dat
+000035a0: 6162 6173 6520 656e 6769 6e65 732e 0a20  abase engines.. 
+000035b0: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
+000035c0: 2020 2069 6620 7365 6c66 2e70 7974 686f     if self.pytho
+000035d0: 6e5f 7479 7065 2069 7320 6e6f 7420 4e6f  n_type is not No
+000035e0: 6e65 2061 6e64 206e 6f74 2069 7369 6e73  ne and not isins
+000035f0: 7461 6e63 6528 7365 6c66 2e70 7974 686f  tance(self.pytho
+00003600: 6e5f 7479 7065 2c20 7479 7065 293a 0a20  n_type, type):. 
+00003610: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+00003620: 2056 616c 7565 4572 726f 7228 6622 696e   ValueError(f"in
+00003630: 7661 6c69 6420 7079 7468 6f6e 5f74 7970  valid python_typ
+00003640: 6520 5c22 7b73 656c 662e 7079 7468 6f6e  e \"{self.python
+00003650: 5f74 7970 657d 5c22 2028 7b74 7970 6528  _type}\" ({type(
+00003660: 7365 6c66 2e70 7974 686f 6e5f 7479 7065  self.python_type
+00003670: 292e 5f5f 6e61 6d65 5f5f 7d29 3a20 6578  ).__name__}): ex
+00003680: 7065 6374 6564 2074 7970 6520 696e 7374  pected type inst
+00003690: 616e 6365 2229 0a0a 0a20 2020 2064 6566  ance")...    def
+000036a0: 205f 5f73 7472 5f5f 2873 656c 6629 202d   __str__(self) -
+000036b0: 3e20 7374 723a 0a20 2020 2020 2020 2072  > str:.        r
+000036c0: 6574 7572 6e20 7365 6c66 2e6e 616d 650a  eturn self.name.
+000036d0: 0a0a 636c 6173 7320 5371 6c46 696c 6544  ..class SqlFileD
+000036e0: 6174 613a 0a20 2020 2064 6566 205f 5f69  ata:.    def __i
+000036f0: 6e69 745f 5f28 7365 6c66 2c20 7061 7468  nit__(self, path
+00003700: 3a20 5061 7468 2c20 656e 636f 6469 6e67  : Path, encoding
+00003710: 3a20 7374 722c 202a 2a6b 7761 7267 7329  : str, **kwargs)
+00003720: 3a0a 2020 2020 2020 2020 7365 6c66 2e70  :.        self.p
+00003730: 6174 683a 2050 6174 6820 3d20 7061 7468  ath: Path = path
+00003740: 0a20 2020 2020 2020 2073 656c 662e 7371  .        self.sq
+00003750: 6c3a 2073 7472 203d 2073 656c 662e 7061  l: str = self.pa
+00003760: 7468 2e72 6561 645f 7465 7874 2865 6e63  th.read_text(enc
+00003770: 6f64 696e 673d 656e 636f 6469 6e67 292e  oding=encoding).
+00003780: 666f 726d 6174 282a 2a6b 7761 7267 7329  format(**kwargs)
+00003790: 0a20 2020 2020 2020 2073 656c 662e 7265  .        self.re
+000037a0: 7665 7273 655f 7371 6c3a 2073 7472 203d  verse_sql: str =
+000037b0: 204e 6f6e 650a 0a20 2020 2020 2020 2070   None..        p
+000037c0: 6f73 203d 2073 656c 662e 7371 6c2e 6669  os = self.sql.fi
+000037d0: 6e64 2827 2d2d 2372 6576 6572 7365 2729  nd('--#reverse')
+000037e0: 0a20 2020 2020 2020 2069 6620 706f 7320  .        if pos 
+000037f0: 3e20 303a 0a20 2020 2020 2020 2020 2020  > 0:.           
+00003800: 2073 656c 662e 7265 7665 7273 655f 7371   self.reverse_sq
+00003810: 6c20 3d20 7365 6c66 2e73 716c 5b70 6f73  l = self.sql[pos
+00003820: 2b6c 656e 2827 2d2d 2372 6576 6572 7365  +len('--#reverse
+00003830: 2729 3a5d 2e73 7472 6970 2829 0a20 2020  '):].strip().   
+00003840: 2020 2020 2020 2020 2073 656c 662e 7371           self.sq
+00003850: 6c20 3d20 7365 6c66 2e73 716c 5b3a 706f  l = self.sql[:po
+00003860: 735d 2e73 7472 6970 2829 0a0a 2020 2020  s].strip()..    
+00003870: 2020 2020 7265 7665 7273 655f 7061 7468      reverse_path
+00003880: 203d 2073 656c 662e 7061 7468 2e6a 6f69   = self.path.joi
+00003890: 6e70 6174 6828 7365 6c66 2e70 6174 682e  npath(self.path.
+000038a0: 7374 656d 202b 2027 5f72 6576 6572 7365  stem + '_reverse
+000038b0: 2e73 716c 2729 0a20 2020 2020 2020 2069  .sql').        i
+000038c0: 6620 7265 7665 7273 655f 7061 7468 2e65  f reverse_path.e
+000038d0: 7869 7374 7328 293a 0a20 2020 2020 2020  xists():.       
+000038e0: 2020 2020 2073 656c 662e 7265 7665 7273       self.revers
+000038f0: 655f 7371 6c20 3d20 2873 656c 662e 7265  e_sql = (self.re
+00003900: 7665 7273 655f 7371 6c20 2b20 273b 5c6e  verse_sql + ';\n
+00003910: 2720 6966 2073 656c 662e 7265 7665 7273  ' if self.revers
+00003920: 655f 7371 6c20 656c 7365 2027 2729 202b  e_sql else '') +
+00003930: 2072 6576 6572 7365 5f70 6174 682e 7265   reverse_path.re
+00003940: 6164 5f74 6578 7428 656e 636f 6469 6e67  ad_text(encoding
+00003950: 3d65 6e63 6f64 696e 6729 2e66 6f72 6d61  =encoding).forma
+00003960: 7428 2a2a 6b77 6172 6773 290a 0a0a 6465  t(**kwargs)...de
+00003970: 6620 6765 745f 7371 6c5f 6669 6c65 5f64  f get_sql_file_d
+00003980: 6174 6128 2a70 6174 6873 3a20 5061 7468  ata(*paths: Path
+00003990: 7c73 7472 2c20 656e 636f 6469 6e67 3a20  |str, encoding: 
+000039a0: 7374 7220 3d20 2275 7466 2d38 222c 202a  str = "utf-8", *
+000039b0: 2a6b 7761 7267 7329 202d 3e20 6c69 7374  *kwargs) -> list
+000039c0: 5b53 716c 4669 6c65 4461 7461 5d3a 0a20  [SqlFileData]:. 
+000039d0: 2020 2070 6174 6873 3a20 6c69 7374 5b50     paths: list[P
+000039e0: 6174 685d 203d 205b 5061 7468 2870 6174  ath] = [Path(pat
+000039f0: 6829 2069 6620 6e6f 7420 6973 696e 7374  h) if not isinst
+00003a00: 616e 6365 2870 6174 682c 2050 6174 6829  ance(path, Path)
+00003a10: 2065 6c73 6520 7061 7468 2066 6f72 2070   else path for p
+00003a20: 6174 6820 696e 2070 6174 6873 5d0a 2020  ath in paths].  
+00003a30: 2020 7061 7468 732e 736f 7274 2829 0a0a    paths.sort()..
+00003a40: 2020 2020 7265 7375 6c74 7320 3d20 5b5d      results = []
+00003a50: 0a20 2020 2066 6f72 2070 6174 6820 696e  .    for path in
+00003a60: 2070 6174 6873 3a0a 2020 2020 2020 2020   paths:.        
+00003a70: 6966 2070 6174 682e 6973 5f64 6972 2829  if path.is_dir()
+00003a80: 3a0a 2020 2020 2020 2020 2020 2020 7375  :.            su
+00003a90: 6270 6174 6873 203d 2073 6f72 7465 6428  bpaths = sorted(
+00003aa0: 7061 7468 2e69 7465 7264 6972 2829 290a  path.iterdir()).
+00003ab0: 2020 2020 2020 2020 2020 2020 7265 7375              resu
+00003ac0: 6c74 7320 2b3d 2067 6574 5f73 716c 5f66  lts += get_sql_f
+00003ad0: 696c 655f 6461 7461 282a 7375 6270 6174  ile_data(*subpat
+00003ae0: 6873 2c20 656e 636f 6469 6e67 3d65 6e63  hs, encoding=enc
+00003af0: 6f64 696e 672c 202a 2a6b 7761 7267 7329  oding, **kwargs)
+00003b00: 0a0a 2020 2020 2020 2020 656c 6966 2070  ..        elif p
+00003b10: 6174 682e 6e61 6d65 2e6c 6f77 6572 2829  ath.name.lower()
+00003b20: 2e65 6e64 7377 6974 6828 275f 7265 7665  .endswith('_reve
+00003b30: 7273 652e 7371 6c27 293a 0a20 2020 2020  rse.sql'):.     
+00003b40: 2020 2020 2020 2070 6173 7320 2320 6967         pass # ig
+00003b50: 6e6f 7265 640a 0a20 2020 2020 2020 2065  nored..        e
+00003b60: 6c69 6620 7061 7468 2e73 7566 6669 782e  lif path.suffix.
+00003b70: 6c6f 7765 7228 2920 3d3d 2027 2e73 716c  lower() == '.sql
+00003b80: 273a 0a20 2020 2020 2020 2020 2020 2072  ':.            r
+00003b90: 6573 756c 7473 2e61 7070 656e 6428 5371  esults.append(Sq
+00003ba0: 6c46 696c 6544 6174 6128 7061 7468 2c20  lFileData(path, 
+00003bb0: 656e 636f 6469 6e67 3d65 6e63 6f64 696e  encoding=encodin
+00003bc0: 672c 202a 2a6b 7761 7267 7329 290a 0a20  g, **kwargs)).. 
+00003bd0: 2020 2072 6574 7572 6e20 7265 7375 6c74     return result
+00003be0: 730a                                     s.
```

## zut/db/mssql.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-"""
-Accs  la base de donnes interface.
-"""
-from __future__ import annotations
-import logging
-import re
-from .commons import Column, DbWrapper
-
-try:
-    from pyodbc import Connection, Cursor, connect
-    _available = True
-except ImportError:
-    Connection = type(None)
-    Cursor = type(None)
-    _available = False
-
-logger = logging.getLogger(__name__)
-
-
-class MssqlWrapper(DbWrapper[Connection, Cursor]):
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-    scheme = 'mssql'
-    default_schema_name = 'dbo'
-    compatible_django_engines = [
-        'mssql', # `mssql-django` (Microsoft official fork of `django-mssql-backend`): https://github.com/microsoft/mssql-django
-        'sql_server.pyodbc', # `django-mssql-backend`: https://pypi.org/project/django-mssql-backend/
-    ]
-
-    def _create_connection(self) -> Connection:
-        server = self._host or 'localhost'
-        if self._port:
-            server += f',{self._port}'
-        
-        connection_string = 'Driver={SQL Server};Server=%s;Database=%s;' % (server, self._name)
-        loggable_connection_string = connection_string
-
-        if self._user:
-            connection_string += 'User=%s;Password=%s;' % (self._user, self._password)
-            loggable_connection_string += 'User=%s;Password=%s;' % (self._user, re.sub(r'.', '*', self._password))
-        else:
-            credentials = 'Trusted_Connection=yes;'
-            connection_string += credentials
-            loggable_connection_string += credentials
-
-        logger.debug("create mssql connection: %s", loggable_connection_string)
-        return connect(connection_string)
-    
-
-    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
-        # Example of positional param: cursor.execute(... ? TODO, *["bar"])
-        # Example of named param:      not possible directly, use build_query_with_positional_params()
-        if limit is not None:
-            query = self.limit_query(query, limit=limit)
-        
-        if isinstance(params, dict):
-            query, params = self.build_query_with_positional_params(query, params)
-        elif not params:
-            params = []
-        
-        cursor = self.cursor()
-        cursor.execute(query, *params)
-        return cursor
-
-
-    def _limit_parsed_query(self, query: str, limit: int):
-        return f"SELECT TOP {limit} * FROM ({query}) s"
-
-
-    def _fix_cursor_column_definition(self, column: Column):
-        # internal_size isn't meaningfull: it always equals precision
-        column.internal_size = None
+"""
+Accs  la base de donnes interface.
+"""
+from __future__ import annotations
+import logging
+import re
+from .commons import Column, DbWrapper
+
+try:
+    from pyodbc import Connection, Cursor, connect
+    _available = True
+except ImportError:
+    Connection = type(None)
+    Cursor = type(None)
+    _available = False
+
+logger = logging.getLogger(__name__)
+
+
+class MssqlWrapper(DbWrapper[Connection, Cursor]):
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+    scheme = 'mssql'
+    default_schema_name = 'dbo'
+    compatible_django_engines = [
+        'mssql', # `mssql-django` (Microsoft official fork of `django-mssql-backend`): https://github.com/microsoft/mssql-django
+        'sql_server.pyodbc', # `django-mssql-backend`: https://pypi.org/project/django-mssql-backend/
+    ]
+
+    def _create_connection(self) -> Connection:
+        server = self._host or 'localhost'
+        if self._port:
+            server += f',{self._port}'
+        
+        connection_string = 'Driver={SQL Server};Server=%s;Database=%s;' % (server, self._name)
+        loggable_connection_string = connection_string
+
+        if self._user:
+            connection_string += 'User=%s;Password=%s;' % (self._user, self._password)
+            loggable_connection_string += 'User=%s;Password=%s;' % (self._user, re.sub(r'.', '*', self._password))
+        else:
+            credentials = 'Trusted_Connection=yes;'
+            connection_string += credentials
+            loggable_connection_string += credentials
+
+        logger.debug("create mssql connection: %s", loggable_connection_string)
+        return connect(connection_string)
+    
+
+    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
+        # Example of positional param: cursor.execute(... ? TODO, *["bar"])
+        # Example of named param:      not possible directly, use build_query_with_positional_params()
+        if limit is not None:
+            query = self.limit_query(query, limit=limit)
+        
+        if isinstance(params, dict):
+            query, params = self.build_query_with_positional_params(query, params)
+        elif not params:
+            params = []
+        
+        cursor = self.cursor()
+        cursor.execute(query, *params)
+        return cursor
+
+
+    def _limit_parsed_query(self, query: str, limit: int):
+        return f"SELECT TOP {limit} * FROM ({query}) s"
+
+
+    def _fix_cursor_column_definition(self, column: Column):
+        # internal_size isn't meaningfull: it always equals precision
+        column.internal_size = None
```

## zut/db/pg.py

 * *Ordering differences only*

```diff
@@ -1,257 +1,257 @@
-from __future__ import annotations
-from io import IOBase
-import re
-import logging
-from typing import Any
-from ..csv import get_default_csv_delimiter
-from .commons import Column, DbWrapper
-
-try:
-    from psycopg2 import connect, sql, DataError
-    from psycopg2.extensions import connection, cursor, string_types
-    _available = True
-except ImportError:
-    connection = type(None)
-    cursor = type(None)
-    _available = False
-
-logger = logging.getLogger(__name__)
-
-
-class PgWrapper(DbWrapper[connection, cursor]):
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-    scheme = 'pg'
-    default_schema_name = 'public'
-    compatible_django_engines = ['django.db.backends.postgresql', 'django.contrib.gis.db.backends.postgis']
-
-    def _create_connection(self):
-        connect_kwargs = {'database': self._name}        
-        if self._host:
-            connect_kwargs['host'] = self._host
-        if self._user:
-            connect_kwargs['user'] = self._user
-        if self._password:
-            connect_kwargs['password'] = self._password
-        if self._port:
-            connect_kwargs['port'] = self._port
-
-        if logger.isEnabledFor(logging.DEBUG):
-            loggable_connect_kwargs = dict(connect_kwargs)
-            if 'password' in loggable_connect_kwargs:
-                loggable_connect_kwargs['password'] = re.sub(r'.', '*', loggable_connect_kwargs['password'])
-
-            logger.debug(f"create pg connection: %s", ', '.join(f"{key}={value}" for key, value in loggable_connect_kwargs.items()))
-
-        conn = connect(**connect_kwargs)
-        conn.autocommit = True
-        return conn
-    
-
-    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
-        # Example of positional param: cursor.execute("INSERT INTO foo VALUES (%s)", ["bar"])
-        # Example of named param: cursor.execute("INSERT INTO foo VALUES (%(foo)s)", {"foo": "bar"})
-        if limit is not None:
-            query = self.limit_query(query, limit=limit)
-        
-        if params is None:
-            params = []
-
-        cursor = self.cursor()
-        cursor.execute(query, params)
-        return cursor
-    
-
-    def _limit_parsed_query(self, query: str, limit: int):
-        return f"SELECT * FROM ({query}) s LIMIT {limit}"
-
-
-    def get_select_table_query(self, table: str|tuple, schema_only = False):
-        schema, table = self.split_name(table)
-        
-        query = 'SELECT * FROM {}.{}'
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-        if schema_only:
-            query += ' WHERE false'
-
-        return sql.SQL(query).format(*params)
-    
-
-    def call_procedure(self, name: str|tuple, *args):
-        schema_name, name = self.split_name(name)
-
-        query = "CALL {}.{}("
-        params = [sql.Identifier(schema_name), sql.Identifier(name)]
-
-        first = True
-        for arg in args:
-            if first:
-                first = False
-            else:
-                query += ", "
-            query += "{}"
-            params += [self.get_flexible_param(arg)]
-
-        query += ")"
-
-        logger.info(f"execute {schema_name}.{name}")
-
-        previous_notices_len = len(self.connection.notices)
-        log_handler = PgLogHandler(f"pg:{schema_name}.{name}")
-        try:
-            with self.cursor() as cursor:
-                cursor.execute(sql.SQL(query).format(*params))
-        finally:
-            for notice in self.connection.notices[previous_notices_len:]:
-                log_handler.append(notice, (schema_name, name))
-
-
-    def get_flexible_param(self, value: Any) -> sql.Composable:
-        if value is None:
-            return sql.SQL("null")
-        elif value == '__now__':
-            return sql.SQL("NOW()")
-        elif isinstance(value, sql.Composable):
-            return value
-        else:
-            return sql.Literal(value)
-
-
-    def table_exists(self, table: str|tuple) -> bool:
-        schema, table = self.split_name(table)
-
-        query = "SELECT EXISTS (SELECT FROM pg_tables WHERE schemaname = {} AND tablename = {})"
-        params = [sql.Literal(schema), sql.Literal(table)]
-
-        return self.execute_get_scalar(sql.SQL(query).format(*params))
-        
-
-    def truncate_table(self, table: str|tuple):
-        schema, table = self.split_name(table)
-
-        query = "TRUNCATE {}.{}"
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-
-        self.execute(sql.SQL(query).format(*params))
-    
-
-    def copy_from_csv(self, fp: IOBase, table: str|tuple, columns: list[str] = None, delimiter: str = None, quotechar: str = '"', nullchar: str = '', noheader: bool = False):
-        schema, table = self.split_name(table)
-
-        if delimiter is None:
-            delimiter = get_default_csv_delimiter()
-
-        query = "COPY {}.{}"
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-
-        if columns:
-            query += " ("
-            for i, column in enumerate(columns):
-                if i > 0:
-                    query += ", "
-                query += "{}"
-
-                params.append(sql.Identifier(column))
-            query += ")"
-
-        query += " FROM STDIN WITH CSV"
-
-        query += ' NULL {}'
-        params.append(sql.Literal(nullchar))
-
-        query += ' DELIMITER {}'
-        params.append(sql.Literal(delimiter))
-
-        query += ' QUOTE {}'
-        params.append(sql.Literal(quotechar))
-        
-        query += ' ESCAPE {}'
-        params.append(sql.Literal(quotechar))
-
-        if not noheader:
-            query += " HEADER"
-
-        with self.cursor() as cursor:
-            cursor.copy_expert(sql.SQL(query).format(*params), fp)
-
-
-    def _fix_cursor_column_definition(self, column: Column):
-        # `python_type` is actually a type code.
-        # We will determine the actual python_type from the type_code.
-        # see: https://stackoverflow.com/a/53327695
-
-        type_code: Any = column.python_type
-
-        column.python_type = None
-        column.original_type = None
-
-        try:
-            string_type = string_types[type_code]
-            column.original_type = string_type.name
-
-            if 'BOOL' in column.original_type:
-                value = 'true'
-                column.python_type = type(string_type(value, cursor))
-            elif 'DATE' in column.original_type:
-                value = '2000-01-01'
-                column.python_type = type(string_type(value, cursor))
-            elif 'ARRAY' in column.original_type:
-                value = '{}'
-                column.python_type = type(string_type(value, cursor))
-            elif 'JSON' in column.original_type:
-                column.python_type = None
-            else:
-                value = '100'
-                column.python_type = type(string_type(value, cursor))
-
-        except DataError as err:
-            logger.error(f"cannot determine Python type of column \"{column.name}\" ({column.original_type or f'type_code={type_code}'}): {err}")
-        
-
-class PgLogHandler:
-    """
-    Usage example:
-    ```
-    from django.apps import AppConfig
-    from django.db.backends.signals import connection_created
-    from zut.db.pg import PgLogHandler
-
-    def connection_created_receiver(sender, connection, **kwargs):
-        if connection.alias == "default":
-            connection.connection.notices = PgLogHandler(f"pg:{connection.alias}")
-
-    class MainConfig(AppConfig):
-        default_auto_field = "django.db.models.BigAutoField"
-        name = "main"
-        
-        def ready(self):
-            connection_created.connect(connection_created_receiver)
-    ```
-    """
-    _pg_msg_re = re.compile(r"^(?P<pglevel>[A-Z]+)\:\s(?P<message>.+(?:\r?\n.*)*)$", re.MULTILINE)
-
-    def __init__(self, logger_name: str = 'pg'):
-        self._logger = logging.getLogger(logger_name)
-    
-    def append(self, fullmsg: str):
-        fullmsg = fullmsg.strip()
-        m = self._pg_msg_re.match(fullmsg)
-        if not m:
-            self._logger.warning(fullmsg)
-            return
-
-        message = m.group("message").strip()
-        pglevel = m.group("pglevel")
-        if pglevel == "EXCEPTION":
-            level = logging.ERROR
-        elif pglevel == "WARNING":
-            level = logging.WARNING
-        else:
-            level = logging.INFO
-
-        if level <= logging.INFO and message.endswith("\" does not exist, skipping"):
-            return
-
-        self._logger.log(level, message)
+from __future__ import annotations
+from io import IOBase
+import re
+import logging
+from typing import Any
+from ..csv import get_default_csv_delimiter
+from .commons import Column, DbWrapper
+
+try:
+    from psycopg2 import connect, sql, DataError
+    from psycopg2.extensions import connection, cursor, string_types
+    _available = True
+except ImportError:
+    connection = type(None)
+    cursor = type(None)
+    _available = False
+
+logger = logging.getLogger(__name__)
+
+
+class PgWrapper(DbWrapper[connection, cursor]):
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+    scheme = 'pg'
+    default_schema_name = 'public'
+    compatible_django_engines = ['django.db.backends.postgresql', 'django.contrib.gis.db.backends.postgis']
+
+    def _create_connection(self):
+        connect_kwargs = {'database': self._name}        
+        if self._host:
+            connect_kwargs['host'] = self._host
+        if self._user:
+            connect_kwargs['user'] = self._user
+        if self._password:
+            connect_kwargs['password'] = self._password
+        if self._port:
+            connect_kwargs['port'] = self._port
+
+        if logger.isEnabledFor(logging.DEBUG):
+            loggable_connect_kwargs = dict(connect_kwargs)
+            if 'password' in loggable_connect_kwargs:
+                loggable_connect_kwargs['password'] = re.sub(r'.', '*', loggable_connect_kwargs['password'])
+
+            logger.debug(f"create pg connection: %s", ', '.join(f"{key}={value}" for key, value in loggable_connect_kwargs.items()))
+
+        conn = connect(**connect_kwargs)
+        conn.autocommit = True
+        return conn
+    
+
+    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
+        # Example of positional param: cursor.execute("INSERT INTO foo VALUES (%s)", ["bar"])
+        # Example of named param: cursor.execute("INSERT INTO foo VALUES (%(foo)s)", {"foo": "bar"})
+        if limit is not None:
+            query = self.limit_query(query, limit=limit)
+        
+        if params is None:
+            params = []
+
+        cursor = self.cursor()
+        cursor.execute(query, params)
+        return cursor
+    
+
+    def _limit_parsed_query(self, query: str, limit: int):
+        return f"SELECT * FROM ({query}) s LIMIT {limit}"
+
+
+    def get_select_table_query(self, table: str|tuple, schema_only = False):
+        schema, table = self.split_name(table)
+        
+        query = 'SELECT * FROM {}.{}'
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+        if schema_only:
+            query += ' WHERE false'
+
+        return sql.SQL(query).format(*params)
+    
+
+    def call_procedure(self, name: str|tuple, *args):
+        schema_name, name = self.split_name(name)
+
+        query = "CALL {}.{}("
+        params = [sql.Identifier(schema_name), sql.Identifier(name)]
+
+        first = True
+        for arg in args:
+            if first:
+                first = False
+            else:
+                query += ", "
+            query += "{}"
+            params += [self.get_flexible_param(arg)]
+
+        query += ")"
+
+        logger.info(f"execute {schema_name}.{name}")
+
+        previous_notices_len = len(self.connection.notices)
+        log_handler = PgLogHandler(f"pg:{schema_name}.{name}")
+        try:
+            with self.cursor() as cursor:
+                cursor.execute(sql.SQL(query).format(*params))
+        finally:
+            for notice in self.connection.notices[previous_notices_len:]:
+                log_handler.append(notice, (schema_name, name))
+
+
+    def get_flexible_param(self, value: Any) -> sql.Composable:
+        if value is None:
+            return sql.SQL("null")
+        elif value == '__now__':
+            return sql.SQL("NOW()")
+        elif isinstance(value, sql.Composable):
+            return value
+        else:
+            return sql.Literal(value)
+
+
+    def table_exists(self, table: str|tuple) -> bool:
+        schema, table = self.split_name(table)
+
+        query = "SELECT EXISTS (SELECT FROM pg_tables WHERE schemaname = {} AND tablename = {})"
+        params = [sql.Literal(schema), sql.Literal(table)]
+
+        return self.execute_get_scalar(sql.SQL(query).format(*params))
+        
+
+    def truncate_table(self, table: str|tuple):
+        schema, table = self.split_name(table)
+
+        query = "TRUNCATE {}.{}"
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+
+        self.execute(sql.SQL(query).format(*params))
+    
+
+    def copy_from_csv(self, fp: IOBase, table: str|tuple, columns: list[str] = None, delimiter: str = None, quotechar: str = '"', nullchar: str = '', noheader: bool = False):
+        schema, table = self.split_name(table)
+
+        if delimiter is None:
+            delimiter = get_default_csv_delimiter()
+
+        query = "COPY {}.{}"
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+
+        if columns:
+            query += " ("
+            for i, column in enumerate(columns):
+                if i > 0:
+                    query += ", "
+                query += "{}"
+
+                params.append(sql.Identifier(column))
+            query += ")"
+
+        query += " FROM STDIN WITH CSV"
+
+        query += ' NULL {}'
+        params.append(sql.Literal(nullchar))
+
+        query += ' DELIMITER {}'
+        params.append(sql.Literal(delimiter))
+
+        query += ' QUOTE {}'
+        params.append(sql.Literal(quotechar))
+        
+        query += ' ESCAPE {}'
+        params.append(sql.Literal(quotechar))
+
+        if not noheader:
+            query += " HEADER"
+
+        with self.cursor() as cursor:
+            cursor.copy_expert(sql.SQL(query).format(*params), fp)
+
+
+    def _fix_cursor_column_definition(self, column: Column):
+        # `python_type` is actually a type code.
+        # We will determine the actual python_type from the type_code.
+        # see: https://stackoverflow.com/a/53327695
+
+        type_code: Any = column.python_type
+
+        column.python_type = None
+        column.original_type = None
+
+        try:
+            string_type = string_types[type_code]
+            column.original_type = string_type.name
+
+            if 'BOOL' in column.original_type:
+                value = 'true'
+                column.python_type = type(string_type(value, cursor))
+            elif 'DATE' in column.original_type:
+                value = '2000-01-01'
+                column.python_type = type(string_type(value, cursor))
+            elif 'ARRAY' in column.original_type:
+                value = '{}'
+                column.python_type = type(string_type(value, cursor))
+            elif 'JSON' in column.original_type:
+                column.python_type = None
+            else:
+                value = '100'
+                column.python_type = type(string_type(value, cursor))
+
+        except DataError as err:
+            logger.error(f"cannot determine Python type of column \"{column.name}\" ({column.original_type or f'type_code={type_code}'}): {err}")
+        
+
+class PgLogHandler:
+    """
+    Usage example:
+    ```
+    from django.apps import AppConfig
+    from django.db.backends.signals import connection_created
+    from zut.db.pg import PgLogHandler
+
+    def connection_created_receiver(sender, connection, **kwargs):
+        if connection.alias == "default":
+            connection.connection.notices = PgLogHandler(f"pg:{connection.alias}")
+
+    class MainConfig(AppConfig):
+        default_auto_field = "django.db.models.BigAutoField"
+        name = "main"
+        
+        def ready(self):
+            connection_created.connect(connection_created_receiver)
+    ```
+    """
+    _pg_msg_re = re.compile(r"^(?P<pglevel>[A-Z]+)\:\s(?P<message>.+(?:\r?\n.*)*)$", re.MULTILINE)
+
+    def __init__(self, logger_name: str = 'pg'):
+        self._logger = logging.getLogger(logger_name)
+    
+    def append(self, fullmsg: str):
+        fullmsg = fullmsg.strip()
+        m = self._pg_msg_re.match(fullmsg)
+        if not m:
+            self._logger.warning(fullmsg)
+            return
+
+        message = m.group("message").strip()
+        pglevel = m.group("pglevel")
+        if pglevel == "EXCEPTION":
+            level = logging.ERROR
+        elif pglevel == "WARNING":
+            level = logging.WARNING
+        else:
+            level = logging.INFO
+
+        if level <= logging.INFO and message.endswith("\" does not exist, skipping"):
+            return
+
+        self._logger.log(level, message)
```

## zut/db/sql_pg/010_extensions.sql

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-create extension if not exists unaccent;
+create extension if not exists unaccent;
```

## zut/db/sql_pg/020_slugify.sql

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-create or replace function slugify(input text)
-returns text as $$
-    with "unaccented" as (
-		select unaccent(input) as value
-    ),
-    "clarified" as (
-		select regexp_replace(lower(value), '[^\w\s-]', '', 'g') as value
-	      from "unaccented"
-    ),
-    "hyphenated" as (
-		select trim(both '-_' from regexp_replace(value, '[-\s]+', '-', 'g')) as value
-		from "clarified"
-    )
-    select coalesce(value, 'none') from "hyphenated";
-$$ language sql immutable;
+create or replace function slugify(input text)
+returns text as $$
+    with "unaccented" as (
+		select unaccent(input) as value
+    ),
+    "clarified" as (
+		select regexp_replace(lower(value), '[^\w\s-]', '', 'g') as value
+	      from "unaccented"
+    ),
+    "hyphenated" as (
+		select trim(both '-_' from regexp_replace(value, '[-\s]+', '-', 'g')) as value
+		from "clarified"
+    )
+    select coalesce(value, 'none') from "hyphenated";
+$$ language sql immutable;
```

## zut/django/management/commands/reinit.py

```diff
@@ -1,221 +1,120 @@
-from __future__ import annotations
-import logging, re
-from pathlib import Path
-from psycopg2 import sql
-from django.conf import settings
-from django.db import connection
-from django.core.management import base, call_command, get_commands
-from ....venv import is_in_venv
-from ....db import PgWrapper
-
-logger = logging.getLogger(__name__)
-
-
-class Command(base.BaseCommand):
-    REINIT_SEALED_MIGRATIONS: dict[str,int] = getattr(settings, "REINIT_SEALED_MIGRATIONS", {})
-    REINIT_APPS_ORDER: list[str] = getattr(settings, "REINIT_APPS_ORDER", [])
-    REINIT_POST_COMMANDS: list[str|list[str]] = getattr(settings, "REINIT_POST_COMMANDS", [])
-    BASE_DIR: Path = settings.BASE_DIR
-    _migration_name_re = re.compile(r"^(\d+)_")
-
-
-    def add_arguments(self, parser):
-        parser.add_argument("-d", "--drop", dest="schema", action="store_const", const="drop", help="drop existing objects and data")
-        parser.add_argument("-b", "--bak", dest="schema", action="store_const", const="bak", help="move existing objects and data to schema \"bak\"")
-        parser.add_argument("-t", "--bak-to", dest="schema", help="move existing objects and data to the given schema")
-        parser.add_argument("apps", nargs="*", help="apps for which migrations are remade")
-
-
-    def handle(self, schema: str = None, apps: list[str] = [], **kwargs):
-        if not settings.DEBUG:
-            raise ValueError("reinit may be used only in DEBUG mode")
-        if not schema:
-            raise ValueError("please confirm what to do with current data: --drop, --bak or --bak-to")
-        
-        if not settings.DATABASES['default']['ENGINE'] in PgWrapper.compatible_django_engines:
-            raise ValueError(f"not a postgresql django engine: {settings.DATABASES['ENGINE']}")
-
-        self.apps = apps
-
-        if schema == "drop":
-            self.drop()
-        else:
-            self.move_to_schema(schema)
-
-        self.delete_nonmanual_migrations()
-        renamed = self.rename_manual_migrations()
-
-        logger.info("make migrations")
-        for app in self.REINIT_APPS_ORDER:
-            call_command("makemigrations", app)
-        call_command("makemigrations")
-
-        self.restore_renamed_migrations(renamed)
-        
-        logger.info("migrate")
-        call_command("migrate")
-
-        for post_command in self.REINIT_POST_COMMANDS:
-            if not isinstance(post_command, list):
-                post_command = [post_command]
-            logger.info(' '.join(post_command))
-            call_command(*post_command)
-
-
-    def move_to_schema(self, new_schema, old_schema="public"):
-        query = """do language plpgsql
-    $$declare
-        old_schema name = {};
-        new_schema name = {};
-        sql_query text;
-    begin
-        sql_query = format('create schema %I', new_schema);
-
-        raise notice 'applying %', sql_query;
-        execute sql_query;
-    
-        for sql_query in
-            select
-                format('alter %s %I.%I set schema %I', case when table_type = 'VIEW' then 'view' else 'table' end, table_schema, table_name, new_schema)
-            from information_schema.tables
-            where table_schema = old_schema
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-    end;$$;
-    """
-
-        with connection.cursor() as cursor:
-            cursor.execute(sql.SQL(query).format(sql.Literal(old_schema), sql.Literal(new_schema if new_schema else "public")))
-
-
-    def drop(self, schema="public"):
-        query = """do language plpgsql
-    $$declare
-        old_schema name = {};
-        sql_query text;
-    begin
-        -- First, remove foreign-key constraints
-        for sql_query in
-            select
-                format('alter table %I.%I drop constraint %I', table_schema, table_name, constraint_name)
-            from information_schema.table_constraints
-            where table_schema = old_schema and constraint_type = 'FOREIGN KEY'
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-
-        -- Then, drop tables
-        for sql_query in
-            select
-                format('drop %s if exists %I.%I cascade'
-                    ,case when table_type = 'VIEW' then 'view' else 'table' end
-                    ,table_schema
-                    ,table_name
-                )
-            from information_schema.tables
-            where table_schema = old_schema
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-    end;$$;
-    """
-
-        with connection.cursor() as cursor:
-            cursor.execute(sql.SQL(query).format(sql.Literal(schema)))
-
-
-    def should_remake_migration(self, path: Path, remake_manuals=False):
-        if not remake_manuals and path.name.endswith("_manual.py"):
-            return False
-
-        if is_in_venv(path):
-            # exclude migrations located in venv
-            logger.debug(f"preserve (venv): {path}")
-            return False
-
-        for part in path.parts:
-            if part.startswith('.'):
-                logger.debug(f"preserve (hidden): {path}")
-                return False
-
-            if re.match(r'^\d+\-', part):
-                logger.debug(f"preserve (digit+hyphen): {path}")
-                return False
-
-        app_name = path.parent.parent.name
-        if self.apps:           
-            if app_name in self.apps:
-                if self.is_sealed_migration(app_name, path):
-                    return False
-            else:
-                logger.info(f"preserve (app {app_name}): {path}")
-                return False
-
-        else:
-            if self.is_sealed_migration(app_name, path):
-                return False
-
-        return True
-
-
-    def get_migration_number(self, path: Path):
-        m = self._migration_name_re.match(path.name)
-        if not m:
-            return None
-
-        return int(m.group(1))
-
-
-    def is_sealed_migration(self, app_name: str, path: Path):
-        if not self.REINIT_SEALED_MIGRATIONS:
-            return False
-        if not app_name in self.REINIT_SEALED_MIGRATIONS:
-            return False
-        
-        migration_number = self.get_migration_number(path)
-        if migration_number is None:
-            logger.warning(f"preserve (no migration number): {path}")
-            return True
-        elif migration_number <= self.REINIT_SEALED_MIGRATIONS[app_name]:
-            logger.info(f"preserve (sealed: app {app_name} migration <= {self.REINIT_SEALED_MIGRATIONS[app_name]}): {path}")
-            return True
-        else:
-            return False
-    
-
-    def delete_nonmanual_migrations(self):
-        """ Delete non-manual migrations """
-        for path in self.BASE_DIR.glob("*/migrations/0*.py"):
-            if self.should_remake_migration(path):
-                logger.info(f"delete {path}")
-                path.unlink()
-
-
-    def rename_manual_migrations(self) -> dict[Path,Path]:
-        """ Rename manual migrations to py~ """
-        renamed: dict[Path,Path] = {}
-
-        for path in self.BASE_DIR.glob("*/migrations/*_manual.py"):
-            if self.should_remake_migration(path, remake_manuals=True):
-                target = path.with_name(f"{path.name}~")
-                logger.info(f"rename {path} to {target}")
-                path.rename(target)
-                renamed[path] = target
-            else:
-                logger.info(f"preserve {path}")
-
-        return renamed
-
-    
-    def restore_renamed_migrations(self, renamed: dict[Path,Path]):
-        """ Restore migrations from py~ """
-        for origin, newpath in renamed.items():
-            logger.info(f"rename {newpath} to {origin}")
-            newpath.rename(origin)
+from __future__ import annotations
+from enum import Enum
+import logging, re
+from pathlib import Path
+from psycopg2 import sql
+from django.conf import settings
+from django.db import connection
+from django.core.management import base, call_command, get_commands
+from django.apps import AppConfig, apps as all_apps
+from ....venv import is_in_venv
+from ....db import PgWrapper
+
+logger = logging.getLogger(__name__)
+
+
+class Command(base.BaseCommand):
+    REINIT_POST_COMMANDS: list[str|list[str]] = getattr(settings, "REINIT_POST_COMMANDS", [])
+
+    def add_arguments(self, parser):
+        parser.add_argument("-d", "--drop", dest="schema", action="store_const", const="drop", help="drop existing objects and data")
+        parser.add_argument("-b", "--bak", dest="schema", action="store_const", const="bak", help="move existing objects and data to schema \"bak\"")
+        parser.add_argument("-t", "--bak-to", dest="schema", help="move existing objects and data to the given schema")
+        parser.add_argument("-x", "--exclude", nargs='*', dest="exclude_apps", metavar='apps', help="label of apps to exclude from migrations remaking")
+        parser.add_argument("apps", nargs="*", help="label of apps for which migrations are remade")
+
+
+    def handle(self, schema: str = None, apps: list[str] = [], exclude_apps: list[str] = None, **kwargs):
+        if not settings.DEBUG:
+            raise ValueError("reinit may be used only in DEBUG mode")
+        if not schema:
+            raise ValueError("please confirm what to do with current data: --drop, --bak or --bak-to")
+        
+        if not settings.DATABASES['default']['ENGINE'] in PgWrapper.compatible_django_engines:
+            raise ValueError(f"not a postgresql django engine: {settings.DATABASES['ENGINE']}")
+
+        if schema == "drop":
+            self.drop()
+        else:
+            self.move_to_schema(schema)
+
+        call_command("remakemigrations", *apps, exclude_apps=exclude_apps)
+
+        logger.info("migrate")
+        call_command("migrate")
+
+        for post_command in self.REINIT_POST_COMMANDS:
+            if not isinstance(post_command, list):
+                post_command = [post_command]
+            logger.info(' '.join(post_command))
+            call_command(*post_command)
+
+
+    def move_to_schema(self, new_schema, old_schema="public"):
+        query = """do language plpgsql
+    $$declare
+        old_schema name = {};
+        new_schema name = {};
+        sql_query text;
+    begin
+        sql_query = format('create schema %I', new_schema);
+
+        raise notice 'applying %', sql_query;
+        execute sql_query;
+    
+        for sql_query in
+            select
+                format('alter %s %I.%I set schema %I', case when table_type = 'VIEW' then 'view' else 'table' end, table_schema, table_name, new_schema)
+            from information_schema.tables
+            where table_schema = old_schema
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+    end;$$;
+    """
+
+        with connection.cursor() as cursor:
+            cursor.execute(sql.SQL(query).format(sql.Literal(old_schema), sql.Literal(new_schema if new_schema else "public")))
+
+
+    def drop(self, schema="public"):
+        query = """do language plpgsql
+    $$declare
+        old_schema name = {};
+        sql_query text;
+    begin
+        -- First, remove foreign-key constraints
+        for sql_query in
+            select
+                format('alter table %I.%I drop constraint %I', table_schema, table_name, constraint_name)
+            from information_schema.table_constraints
+            where table_schema = old_schema and constraint_type = 'FOREIGN KEY'
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+
+        -- Then, drop tables
+        for sql_query in
+            select
+                format('drop %s if exists %I.%I cascade'
+                    ,case when table_type = 'VIEW' then 'view' else 'table' end
+                    ,table_schema
+                    ,table_name
+                )
+            from information_schema.tables
+            where table_schema = old_schema
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+    end;$$;
+    """
+
+        with connection.cursor() as cursor:
+            cursor.execute(sql.SQL(query).format(sql.Literal(schema)))
+
```

## zut/django/middleware/__init__.py

```diff
@@ -1,80 +1,96 @@
-import logging
-from django.http import HttpResponseForbidden
-from django.core.exceptions import ImproperlyConfigured
-
-try:
-    from django.contrib.auth.mixins import AccessMixin
-    from django.contrib.auth.views import LoginView, LogoutView, redirect_to_login
-    from django.views.generic.base import RedirectView
-    improperly_configured_error = None
-        
-    try:
-        from rest_framework.views import APIView
-        _with_rest_framework = True
-    except ImportError:
-        _with_rest_framework = False
-except ImproperlyConfigured as improperly_configured_error:
-    # delay exception in class instantiation to avoid failure during unittest discovery
-    pass
-
-
-logger = logging.getLogger(__name__)
-
-class StaffAuthorizationMiddleware:
-    no_default_for = ["/", "/status"]
-
-    def __init__(self, get_response):
-        if improperly_configured_error:
-            raise improperly_configured_error
-        self.get_response = get_response
-
-    def __call__(self, request):
-        return self.get_response(request)
-
-    def process_view(self, request, view_func, view_args, view_kwargs):
-        if request.path in self.no_default_for:
-            return # no default permission
-
-        if view_func.__module__.startswith("django.contrib.admin."):
-            return # no default permission
-
-        if not hasattr(view_func, "view_class"):
-            # function-based view
-            if not self.is_authorized(request.user):
-                logger.debug("function-based view %s: default authorization required", ".".join([view_func.__module__, view_func.__name__]))
-                return self._deny_or_login(request)
-            
-        else:
-            # class-based view
-            view = view_func.view_class
-
-            if issubclass(view, (LoginView, LogoutView, RedirectView)):
-                return # no default permission
-
-            if _with_rest_framework and issubclass(view, APIView):
-                # API view (Django Rest Framework)
-                if not view.permission_classes:
-                    if not self.is_authorized(request.user):
-                        logger.debug("no permission_classes for rest_framework view %s: default authorization required required", ".".join([view.__module__, view.__name__]))
-                        return HttpResponseForbidden() # do not redirect to login page (this is supposed to be accessed by javascript or as an API)
-
-            else:
-                # Standard class-based view
-                if not issubclass(view, AccessMixin):
-                    if not self.is_authorized(request.user):
-                        logger.debug("no AccessMixin for view %s: default authorization required", ".".join([view.__module__, view.__name__]))
-                        return self._deny_or_login(request)
-
-    def is_authorized(self, user):
-        return user.is_staff
-
-    def _deny_or_login(self, request):
-        if request.user.is_authenticated:
-            return HttpResponseForbidden()
-        else:
-            return redirect_to_login(next=request.get_full_path())
-
-
-class AdminAuthorizationMiddleware(StaffAuthorizationMiddleware):
-    def is_authorized(self, user):
-        return user.is_admin
+from __future__ import annotations
+import logging
+from django.http import HttpResponseForbidden
+from django.conf import settings
+from django.contrib.auth.mixins import AccessMixin
+from django.contrib.auth.views import LoginView, LogoutView, redirect_to_login
+from django.contrib.auth.models import AbstractUser
+from django.views.generic.base import RedirectView
+    
+try:
+    from rest_framework.views import APIView
+    _with_rest_framework = True
+except ImportError:
+    _with_rest_framework = False
+
+
+logger = logging.getLogger(__name__)
+
+class StaffAuthorizationMiddleware:
+    """
+    A middleware that restrict access only to staff users by default.
+
+    Ignored for:
+    - Class-based view inheriting from an `AccessMixin`
+    - Django Rest Framework's API views having a non-empty `permission_classes` attribute
+    - admin panel, LoginView, LogoutView, RedirectView
+    - request paths listed in AUTHORIZATION_MIDDLEWARE_BYPASS settings
+    """
+    AUTHORIZATION_MIDDLEWARE_BYPASS: list[str] = getattr(settings, "AUTHORIZATION_MIDDLEWARE_BYPASS", [])
+
+    def __init__(self, get_response):
+        self.get_response = get_response
+
+    def __call__(self, request):
+        return self.get_response(request)
+
+    def process_view(self, request, view_func, view_args, view_kwargs):
+        if request.path in self.AUTHORIZATION_MIDDLEWARE_BYPASS:
+            return # no default permission
+
+        if view_func.__module__.startswith("django.contrib.admin."):
+            return # no default permission
+
+        if not hasattr(view_func, "view_class"):
+            # function-based view
+            if not self.is_authorized(request.user):
+                logger.debug("function-based view %s: %s authorization required", ".".join([view_func.__module__, view_func.__name__]), self.authorization_name)
+                return self._deny_or_login(request)
+            
+        else:
+            # class-based view
+            view = view_func.view_class
+
+            if issubclass(view, (LoginView, LogoutView, RedirectView)):
+                return # no default permission
+
+            if _with_rest_framework and issubclass(view, APIView):
+                # API view (Django Rest Framework)
+                if not view.permission_classes:
+                    if not self.is_authorized(request.user):
+                        logger.debug("no permission_classes for rest_framework view %s: %s authorization required required", ".".join([view.__module__, view.__name__]), self.authorization_name)
+                        return HttpResponseForbidden() # do not redirect to login page (this is supposed to be accessed by javascript or as an API)
+
+            else:
+                # Standard class-based view
+                if not issubclass(view, AccessMixin):
+                    if not self.is_authorized(request.user):
+                        logger.debug("no AccessMixin for view %s: %s authorization required", ".".join([view.__module__, view.__name__]), self.authorization_name)
+                        return self._deny_or_login(request)
+                    
+    authorization_name = 'staff'
+
+    def is_authorized(self, user: AbstractUser):
+        return user.is_staff
+
+    def _deny_or_login(self, request):
+        if request.user.is_authenticated:
+            return HttpResponseForbidden()
+        else:
+            return redirect_to_login(next=request.get_full_path())
+
+
+class SuperuserAuthorizationMiddleware(StaffAuthorizationMiddleware):
+    """
+    A middleware that restrict access only to superusers by default.
+    
+    Ignored for:
+    - Class-based view inheriting from an `AccessMixin`
+    - Django Rest Framework's API views having a non-empty `permission_classes` attribute
+    - admin panel, LoginView, LogoutView, RedirectView
+    - request paths listed in AUTHORIZATION_MIDDLEWARE_BYPASS settings
+    """
+    authorization_name = 'superuser'
+
+    def is_authorized(self, user: AbstractUser):
+        return user.is_superuser
```

## zut/django/templatetags/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-#TODO/ROADMAP: additional tags and filters: timeordate, hostlink, etc
+#TODO/ROADMAP: additional tags and filters: timeordate, hostlink, etc
```

## zut/django/templatetags/static_lib.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-import logging
-from django import template
-from django.utils.safestring import mark_safe
-from django.templatetags.static import static
-from django.conf import settings
-
-logger = logging.getLogger(__name__)
-
-register = template.Library()
-
-# avoid logging warnings for every request
-_missing_version: list[str] = []
-_missing_integrity: list[str] = []
-
-def _get_lib_url(package, file, version=None):
-    LOCAL_STATIC_LIB = getattr(settings, "LOCAL_STATIC_LIB", False)
-    if LOCAL_STATIC_LIB:
-        return f"{static(LOCAL_STATIC_LIB if isinstance(LOCAL_STATIC_LIB, str) else 'lib')}/{package}/{file}"
-    else:
-        if version:
-            return f"https://cdn.jsdelivr.net/npm/{package}@{version}/{file}"
-        else:
-            return f"https://cdn.jsdelivr.net/npm/{package}/{file}"
-
-
-@register.simple_tag
-def style_lib(package, file, version=None, integrity=None):
-    """
-    Usage example in Django base template:
-
-        {% load static %}
-        {% load static_lib %}
-        ...
-        <head>
-        ...
-        {% style_lib  'bootstrap' 'dist/css/bootstrap.min.css' '5.2.0' 'sha256-7ZWbZUAi97rkirk4DcEp4GWDPkWpRMcNaEyXGsNXjLg=' %}
-        ...
-        </head>
-    """
-    url = _get_lib_url(package, file, version)
-
-    if not version and not url in _missing_version:
-        logger.warning(f"missing version for style_lib: {url}")
-        _missing_version.append(url)
-        
-    html = f"<link rel=\"stylesheet\" href=\"{url}\""
-    
-    if integrity:
-        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
-    else:
-        logger.warning(f"missing integrity hash for style_lib: {url}")
-        _missing_integrity.append(url)
-
-    html += f" />"
-    return mark_safe(html)
-
-
-@register.simple_tag
-def script_lib(package, file, version=None, integrity=None, defer=False):
-    """
-    Usage example in Django base template:
-
-        {% load static %}
-        {% load static_lib %}
-        ...
-        <head>
-        ...
-        {% script_lib 'bootstrap' 'dist/js/bootstrap.bundle.min.js' '5.2.0' 'sha256-wMCQIK229gKxbUg3QWa544ypI4OoFlC2qQl8Q8xD8x8=' %}
-        ...
-        </head>
-    """
-    url = _get_lib_url(package, file, version)
-    
-    if not version and not url in _missing_version:
-        logger.warning(f"missing version for script_lib: {url}")
-        _missing_version.append(url)
-
-    html = f"<script"
-    if defer:
-        html=" defer"
-    html += f" src=\"{url}\""
-    
-    if integrity:
-        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
-    elif not url in _missing_integrity:
-        logger.warning(f"missing integrity hash for script_lib: {url}")
-        _missing_integrity.append(url)
-
-    html += f"></script>"
-    return mark_safe(html)
+import logging
+from django import template
+from django.utils.safestring import mark_safe
+from django.templatetags.static import static
+from django.conf import settings
+
+logger = logging.getLogger(__name__)
+
+register = template.Library()
+
+# avoid logging warnings for every request
+_missing_version: list[str] = []
+_missing_integrity: list[str] = []
+
+def _get_lib_url(package, file, version=None):
+    LOCAL_STATIC_LIB = getattr(settings, "LOCAL_STATIC_LIB", False)
+    if LOCAL_STATIC_LIB:
+        return f"{static(LOCAL_STATIC_LIB if isinstance(LOCAL_STATIC_LIB, str) else 'lib')}/{package}/{file}"
+    else:
+        if version:
+            return f"https://cdn.jsdelivr.net/npm/{package}@{version}/{file}"
+        else:
+            return f"https://cdn.jsdelivr.net/npm/{package}/{file}"
+
+
+@register.simple_tag
+def style_lib(package, file, version=None, integrity=None):
+    """
+    Usage example in Django base template:
+
+        {% load static %}
+        {% load static_lib %}
+        ...
+        <head>
+        ...
+        {% style_lib  'bootstrap' 'dist/css/bootstrap.min.css' '5.2.0' 'sha256-7ZWbZUAi97rkirk4DcEp4GWDPkWpRMcNaEyXGsNXjLg=' %}
+        ...
+        </head>
+    """
+    url = _get_lib_url(package, file, version)
+
+    if not version and not url in _missing_version:
+        logger.warning(f"missing version for style_lib: {url}")
+        _missing_version.append(url)
+        
+    html = f"<link rel=\"stylesheet\" href=\"{url}\""
+    
+    if integrity:
+        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
+    else:
+        logger.warning(f"missing integrity hash for style_lib: {url}")
+        _missing_integrity.append(url)
+
+    html += f" />"
+    return mark_safe(html)
+
+
+@register.simple_tag
+def script_lib(package, file, version=None, integrity=None, defer=False):
+    """
+    Usage example in Django base template:
+
+        {% load static %}
+        {% load static_lib %}
+        ...
+        <head>
+        ...
+        {% script_lib 'bootstrap' 'dist/js/bootstrap.bundle.min.js' '5.2.0' 'sha256-wMCQIK229gKxbUg3QWa544ypI4OoFlC2qQl8Q8xD8x8=' %}
+        ...
+        </head>
+    """
+    url = _get_lib_url(package, file, version)
+    
+    if not version and not url in _missing_version:
+        logger.warning(f"missing version for script_lib: {url}")
+        _missing_version.append(url)
+
+    html = f"<script"
+    if defer:
+        html=" defer"
+    html += f" src=\"{url}\""
+    
+    if integrity:
+        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
+    elif not url in _missing_integrity:
+        logger.warning(f"missing integrity hash for script_lib: {url}")
+        _missing_integrity.append(url)
+
+    html += f"></script>"
+    return mark_safe(html)
```

## zut/django/tracking/__init__.py

```diff
@@ -1,258 +1,242 @@
-from __future__ import annotations
-import logging
-from enum import Enum
-from pathlib import Path
-from typing import Any
-from django.http.request import HttpRequest
-from django.utils import timezone
-from django.core.exceptions import FieldDoesNotExist
-from django.db import migrations
-from django.contrib.auth import get_user_model
-from ..migration_tools import sql_file_operations
-from .models import Tracking_Origin, Tracking_EntityType, Tracking_History
-
-logger = logging.getLogger(__name__)
-
-
-class TrackingModelMixin:
-    tracking_ignored_fields = ['created', 'saved', 'changed']
-    """ List of fields to be ignored for tracking. """
-
-    tracking_extra_field = 'extra'
-    """ Name of the JSON field containing remaining extra data considered as top-level data for tracking purpose. """
-
-    tracking_ignore_previously_none = False
-    """ Do not mark a value as changed if it was previously `None`. """
-
-    def set_tracking_origin(self, origin: Tracking_Origin|HttpRequest|str):
-        if origin is None:
-            return
-        
-        if isinstance(origin, (Tracking_Origin, HttpRequest, str)):
-            self._tracking_origin_given = origin
-        else:
-            raise ValueError(f"invalid type for origin: {type(origin)}")
-
-
-    def get_tracking_origin(self) -> Tracking_Origin:
-        if hasattr(self, "_tracking_origin_given"):
-            given = self._tracking_origin_given
-            delattr(self, "_tracking_origin_given")
-
-            if isinstance(given, HttpRequest):
-                self._tracking_origin = Tracking_Origin.for_request(given)
-            elif isinstance(given, str):
-                self._tracking_origin = Tracking_Origin.for_endpoint(given)
-            else:
-                self._tracking_origin = given
-            return self._tracking_origin
-
-        elif hasattr(self, "_tracking_origin"):
-            return self._tracking_origin
-
-        else:
-            return None
-
-
-    def haschanged(self, attr, init_value, new_value):
-        if self.tracking_ignore_previously_none and init_value is None:
-            return False
-        try:
-            method = getattr(self, f"haschanged_{attr}")
-            return method(init_value, new_value)
-        except AttributeError:
-            # default comparison
-            return new_value != init_value
-
-
-    _tracking_entity_type: Tracking_EntityType  # [class attribute] defined in __init__
-    _tracking_saved_attr: str
-    _tracking_changed_attr: str
-    _tracking_snapshot: dict  # defined in _take_tracking_snapshot()
-    _tracking_origin: Tracking_Origin  # defined in get_track_origin()
-    _tracking_origin_given: Tracking_Origin|HttpRequest|str  # defined in set_track_origin()
-
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._take_tracking_snapshot()
-
-        if not hasattr(self.__class__, '_tracking_entity_type'):
-            self.__class__._tracking_entity_type, _ = Tracking_EntityType.objects.get_or_create(
-                app_label=self.__class__._meta.app_label,
-                model_name=self.__class__._meta.model_name
-            )
-
-            self.__class__._meta.get_field('saved')
-            self.__class__. _tracking_saved_attr = 'saved'
-
-            try:
-                self.__class__._meta.get_field('changed')
-                self.__class__._tracking_changed_attr = 'changed'
-            except FieldDoesNotExist:
-                self.__class__._tracking_changed_attr = None
-
-
-    def _current_tracking_values(self) -> dict[str,any]:
-        values = {}
-
-        for attr, value in self.__dict__.items():
-            # remove "_id" suffix
-            if attr.endswith("_id"):
-                nonid_attr = attr[0:-3]
-                if not nonid_attr in self.__dict__:
-                    attr = nonid_attr
-
-            if self.is_field_tracking_ignored(attr):
-                continue
-
-            values[attr] = value
-
-
-        if self.__class__.tracking_extra_field:
-            extra = getattr(self, self.__class__.tracking_extra_field, None)
-            colliding_extra = {}
-
-            if isinstance(extra, dict):
-                for attr, value in extra.items():
-                    if attr in values:
-                        colliding_extra[attr] = value
-                    else:
-                        values[attr] = value
-
-            if colliding_extra:
-                values["_colliding_extra"] = colliding_extra
-
-        return values
-
-
-    def _take_tracking_snapshot(self):
-        if not self.id: # creation
-            return
-        
-        self._tracking_snapshot = self._current_tracking_values()
-
-
-    def is_field_tracking_ignored(self, name: str):
-        if name.startswith('_') or name in self.__class__.tracking_ignored_fields:
-            return True
-
-        if name == self.__class__.tracking_extra_field:
-            value = getattr(self, name, None)
-            if value is None or isinstance(value, dict):
-                return True # handled manually
-
-        return False
-
-
-    def _tracking_updating(self) -> dict[str,Any]:
-        """
-        Return kwargs of the track entry to add if the entity is being updating and has changes.
-        """
-        if not self.id: # creation
-            return None
-
-        changed_data = {}
-
-        new_values = self._current_tracking_values()
-        
-        for attr, init_value in self._tracking_snapshot.items():
-            new_value = new_values.pop(attr, None)
-            if self.haschanged(attr, init_value, new_value):
-                changed_data[attr] = init_value
-
-        # remaining new values: mark that it was not present initially (by using None value)
-        for attr in new_values.keys():
-            if self.is_field_tracking_ignored(attr):
-                continue
-            if not attr in self._tracking_snapshot:
-                changed_data[attr] = None
-
-        if not changed_data:
-            # no change
-            return None
-
-        origin = self.get_tracking_origin()
-
-        return {
-            'entity_type': self.__class__._tracking_entity_type,
-            'entity_id': self.id,
-            'data': changed_data,
-            'data_saved': getattr(self, self._tracking_saved_attr),
-            'origin': origin,
-        }
-        
-
-    def save(self, **kwargs) -> SaveResult:
-        if self.id: # updating
-            trackargs = self._tracking_updating()
-            if trackargs:
-                # updating with changes
-                if self._tracking_changed_attr:
-                    setattr(self, self._tracking_changed_attr, timezone.now())
-                super().save(**kwargs)
-                
-                # NOTE: insertion in TrackHistory is done after super().save()
-                # in order to avoid inserting in case of save failure
-                logger.debug("%s.%s #%s changed: %s", self._meta.app_label, self._meta.model_name, self.id, trackargs['data'])
-                Tracking_History.objects.create(**trackargs)
-                self._take_tracking_snapshot()
-                
-                return SaveResult.CHANGED
-            else:
-                # no changes: save to have 'saved' field updated
-                super().save(**kwargs)
-                return SaveResult.NOCHANGE
-        else:
-            # creating
-            super().save(**kwargs)
-            return SaveResult.CREATED
-
-
-class SaveResult(Enum):
-    NOCHANGE = 0
-    CREATED = 1
-    CHANGED = 2
-
-
-class SaveResultCounter:
-    def __init__(self):
-        self.counts: dict[SaveResult,int] = {}
-
-    def merge(self, result: SaveResult|SaveResultCounter, count: int = 1):
-        if isinstance(result, SaveResultCounter):
-            for sub_result, sub_count in result.counts.items():
-                self.merge(sub_result, count=count*sub_count)
-            return
-
-        if result in self.counts:
-            self.counts[result] += count
-        else:
-            self.counts[result] = count
-
-    @property
-    def total(self):
-        return sum(count for count in self.counts.values())
-    
-    def get_details(self):
-        return ', '.join(f'{result.name.lower()}: {count}' for result, count in self.counts.items())
-
-    @property
-    def parenthesized_details(self):
-        details = self.get_details()
-        if details:
-            return f' ({details})'
-        else:
-            return ''
-
-
-def pg_view_migration_operations() -> list[migrations.RunSQL]:
-    path = Path(__file__).parent.joinpath('sql_pg')
-
-    return sql_file_operations(path,
-        tracking_history_view=Tracking_History._meta.db_table + '_view',
-        tracking_history=Tracking_History._meta.db_table,
-        tracking_entitytype=Tracking_EntityType._meta.db_table,
-        tracking_origin=Tracking_Origin._meta.db_table,
-        user=get_user_model()._meta.db_table,
-    )
+from __future__ import annotations
+import logging
+from enum import Enum
+from typing import Any
+from django.http.request import HttpRequest
+from django.utils import timezone
+from django.core.exceptions import FieldDoesNotExist
+from .models import Tracking_Origin, Tracking_EntityType, Tracking_History
+
+logger = logging.getLogger(__name__)
+
+
+class TrackingModelMixin:
+    tracking_ignored_fields = ['created', 'saved', 'changed']
+    """ List of fields to be ignored for tracking. """
+
+    tracking_extra_field = 'extra'
+    """ Name of the JSON field containing remaining extra data considered as top-level data for tracking purpose. """
+
+    tracking_ignore_previously_none = False
+    """ Do not mark a value as changed if it was previously `None`. """
+
+    def set_tracking_origin(self, origin: Tracking_Origin|HttpRequest|str):
+        if origin is None:
+            return
+        
+        if isinstance(origin, (Tracking_Origin, HttpRequest, str)):
+            self._tracking_origin_given = origin
+        else:
+            raise ValueError(f"invalid type for origin: {type(origin)}")
+
+
+    def get_tracking_origin(self) -> Tracking_Origin:
+        if hasattr(self, "_tracking_origin_given"):
+            given = self._tracking_origin_given
+            delattr(self, "_tracking_origin_given")
+
+            if isinstance(given, HttpRequest):
+                self._tracking_origin = Tracking_Origin.for_request(given)
+            elif isinstance(given, str):
+                self._tracking_origin = Tracking_Origin.for_endpoint(given)
+            else:
+                self._tracking_origin = given
+            return self._tracking_origin
+
+        elif hasattr(self, "_tracking_origin"):
+            return self._tracking_origin
+
+        else:
+            return None
+
+
+    def haschanged(self, attr, init_value, new_value):
+        if self.tracking_ignore_previously_none and init_value is None:
+            return False
+        try:
+            method = getattr(self, f"haschanged_{attr}")
+            return method(init_value, new_value)
+        except AttributeError:
+            # default comparison
+            return new_value != init_value
+
+
+    _tracking_entity_type: Tracking_EntityType  # [class attribute] defined in __init__
+    _tracking_saved_attr: str
+    _tracking_changed_attr: str
+    _tracking_snapshot: dict  # defined in _take_tracking_snapshot()
+    _tracking_origin: Tracking_Origin  # defined in get_track_origin()
+    _tracking_origin_given: Tracking_Origin|HttpRequest|str  # defined in set_track_origin()
+
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._take_tracking_snapshot()
+
+        if not hasattr(self.__class__, '_tracking_entity_type'):
+            self.__class__._tracking_entity_type, _ = Tracking_EntityType.objects.get_or_create(
+                app_label=self.__class__._meta.app_label,
+                model_name=self.__class__._meta.model_name
+            )
+
+            self.__class__._meta.get_field('saved')
+            self.__class__. _tracking_saved_attr = 'saved'
+
+            try:
+                self.__class__._meta.get_field('changed')
+                self.__class__._tracking_changed_attr = 'changed'
+            except FieldDoesNotExist:
+                self.__class__._tracking_changed_attr = None
+
+
+    def _current_tracking_values(self) -> dict[str,any]:
+        values = {}
+
+        for attr, value in self.__dict__.items():
+            # remove "_id" suffix
+            if attr.endswith("_id"):
+                nonid_attr = attr[0:-3]
+                if not nonid_attr in self.__dict__:
+                    attr = nonid_attr
+
+            if self.is_field_tracking_ignored(attr):
+                continue
+
+            values[attr] = value
+
+
+        if self.__class__.tracking_extra_field:
+            extra = getattr(self, self.__class__.tracking_extra_field, None)
+            colliding_extra = {}
+
+            if isinstance(extra, dict):
+                for attr, value in extra.items():
+                    if attr in values:
+                        colliding_extra[attr] = value
+                    else:
+                        values[attr] = value
+
+            if colliding_extra:
+                values["_colliding_extra"] = colliding_extra
+
+        return values
+
+
+    def _take_tracking_snapshot(self):
+        if not self.id: # creation
+            return
+        
+        self._tracking_snapshot = self._current_tracking_values()
+
+
+    def is_field_tracking_ignored(self, name: str):
+        if name.startswith('_') or name in self.__class__.tracking_ignored_fields:
+            return True
+
+        if name == self.__class__.tracking_extra_field:
+            value = getattr(self, name, None)
+            if value is None or isinstance(value, dict):
+                return True # handled manually
+
+        return False
+
+
+    def _tracking_updating(self) -> dict[str,Any]:
+        """
+        Return kwargs of the track entry to add if the entity is being updating and has changes.
+        """
+        if not self.id: # creation
+            return None
+
+        changed_data = {}
+
+        new_values = self._current_tracking_values()
+        
+        for attr, init_value in self._tracking_snapshot.items():
+            new_value = new_values.pop(attr, None)
+            if self.haschanged(attr, init_value, new_value):
+                changed_data[attr] = init_value
+
+        # remaining new values: mark that it was not present initially (by using None value)
+        for attr in new_values.keys():
+            if self.is_field_tracking_ignored(attr):
+                continue
+            if not attr in self._tracking_snapshot:
+                changed_data[attr] = None
+
+        if not changed_data:
+            # no change
+            return None
+
+        origin = self.get_tracking_origin()
+
+        return {
+            'entity_type': self.__class__._tracking_entity_type,
+            'entity_id': self.id,
+            'data': changed_data,
+            'data_saved': getattr(self, self._tracking_saved_attr),
+            'origin': origin,
+        }
+        
+
+    def save(self, **kwargs) -> SaveResult:
+        if self.id: # updating
+            trackargs = self._tracking_updating()
+            if trackargs:
+                # updating with changes
+                if self._tracking_changed_attr:
+                    setattr(self, self._tracking_changed_attr, timezone.now())
+                super().save(**kwargs)
+                
+                # NOTE: insertion in TrackHistory is done after super().save()
+                # in order to avoid inserting in case of save failure
+                logger.debug("%s.%s #%s changed: %s", self._meta.app_label, self._meta.model_name, self.id, trackargs['data'])
+                Tracking_History.objects.create(**trackargs)
+                self._take_tracking_snapshot()
+                
+                return SaveResult.CHANGED
+            else:
+                # no changes: save to have 'saved' field updated
+                super().save(**kwargs)
+                return SaveResult.NOCHANGE
+        else:
+            # creating
+            super().save(**kwargs)
+            return SaveResult.CREATED
+
+
+class SaveResult(Enum):
+    NOCHANGE = 0
+    CREATED = 1
+    CHANGED = 2
+
+
+class SaveResultCounter:
+    def __init__(self):
+        self.counts: dict[SaveResult,int] = {}
+
+    def merge(self, result: SaveResult|SaveResultCounter, count: int = 1):
+        if isinstance(result, SaveResultCounter):
+            for sub_result, sub_count in result.counts.items():
+                self.merge(sub_result, count=count*sub_count)
+            return
+
+        if result in self.counts:
+            self.counts[result] += count
+        else:
+            self.counts[result] = count
+
+    @property
+    def total(self):
+        return sum(count for count in self.counts.values())
+    
+    def get_details(self):
+        return ', '.join(f'{result.name.lower()}: {count}' for result, count in self.counts.items())
+
+    @property
+    def parenthesized_details(self):
+        details = self.get_details()
+        if details:
+            return f' ({details})'
+        else:
+            return ''
```

## zut/django/tracking/models.py

```diff
@@ -1,134 +1,130 @@
-from __future__ import annotations
-from django.db import models
-from django.contrib.auth import get_user_model
-from django.contrib.auth.models import AbstractUser
-from django.http.request import HttpRequest
-from django.utils.safestring import mark_safe
-from ...json import ExtendedJSONDecoder, ExtendedJSONEncoder
-
-APP_LABEL = 'main'
-
-class Tracking_Origin(models.Model):
-    user: AbstractUser = models.ForeignKey(get_user_model(), null=True, blank=True, on_delete=models.RESTRICT)
-    address = models.CharField(max_length=250, blank=True)
-    endpoint = models.CharField(max_length=250) # name of view, test, etc
-    auth = models.CharField(max_length=250, blank=True)
-    details = models.CharField(max_length=250, blank=True)
-    # --------------------
-    created = models.DateTimeField(auto_now_add=True)
-
-
-    class Meta:
-        app_label = APP_LABEL
-        constraints = [
-            models.UniqueConstraint(fields=["user", "address", "endpoint", "auth", "details"], name='main_tracking_origin_uniq'),
-            models.UniqueConstraint(fields=["address", "endpoint", "auth", "details"], condition=models.Q(user=None), name='main_tracking_origin_uniq_without_user'),
-        ]
-
-
-    @property
-    def details_title(self):
-        details = {}
-        
-        if self.user:
-            details["User"] = self.user.username
-        
-        if self.address:
-            details["Adress"] = self.address
-
-        if self.endpoint:
-            details["Endpoint"] = self.endpoint
-
-        if self.auth:
-            details["Auth"] = self.auth
-        
-        if self.details:
-            details["Details"] = self.details
-
-        if details:
-            return mark_safe("\n".join(key.replace('\"', '&quot;') + " : " + val.replace('\"', '&quot;') for key, val in details.items()))
-        else:
-            return mark_safe(f"Origine #{self.id}")
-
-
-    def __str__(self):
-        """ Return main info: user, or address, or endpoint """
-        if self.user:
-            return self.user.username
-
-        if self.address:
-            return self.address
-
-        if self.endpoint:
-            return self.endpoint
-
-        return f"#{self.id}"
-
-
-    @classmethod
-    def for_request(cls, request: HttpRequest):
-        if request.user.is_authenticated:
-            user = request.user
-        else:
-            user = None
-
-        try:
-            auth = request.auth
-            if auth is not None and not isinstance(auth, str):
-                if isinstance(auth, models.Model):
-                    auth = auth._meta.model_name
-                else:
-                    auth = str(auth)
-        except AttributeError:
-            auth = ""
-
-        endpoint = request.path_info
-
-        x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
-        if x_forwarded_for:
-            address = x_forwarded_for.split(',')[-1].strip()
-        else:
-            address = request.META.get('REMOTE_ADDR')
-        
-        details = request.META['HTTP_USER_AGENT']
-
-        origin, _ = cls.objects.get_or_create(user=user, address=address, endpoint=endpoint, auth=auth, details=details)
-        return origin
-
-
-    @classmethod
-    def for_endpoint(cls, endpoint: str, details: str = ""):
-        origin, _ = cls.objects.get_or_create(user=None, address="", endpoint=endpoint, auth="", details=details)
-        return origin
-
-
-class Tracking_EntityType(models.Model):
-    app_label = models.CharField(max_length=1000)
-    model_name = models.CharField(max_length=1000)
-    # --------------------
-    created = models.DateTimeField(auto_now_add=True)
-
-
-    class Meta:
-        app_label = APP_LABEL
-        unique_together = [
-            ('app_label', 'model_name')
-        ]
-
-
-class Tracking_History(models.Model):
-    entity_type = models.ForeignKey(Tracking_EntityType, on_delete=models.CASCADE)
-    entity_id = models.BigIntegerField()
-    # --------------------
-    data = models.JSONField(encoder=ExtendedJSONEncoder, decoder=ExtendedJSONDecoder, help_text="Modified data")
-    data_saved = models.DateTimeField(help_text="Date of last save of historized data")
-    origin = models.ForeignKey(Tracking_Origin, on_delete=models.RESTRICT, null=True, blank=True, related_name="+", help_text="Origin of insertion of history entry")
-    # --------------------
-    created = models.DateTimeField(auto_now_add=True, help_text="Date of insertion of history entry")
-
-    class Meta:
-        app_label = APP_LABEL
-        index_together = [
-            ("entity_type", "entity_id")
-        ]
-        ordering = ["created"]
+from __future__ import annotations
+from django.db import models
+from django.contrib.auth import get_user_model
+from django.contrib.auth.models import AbstractUser
+from django.http.request import HttpRequest
+from django.utils.safestring import mark_safe
+from ...json import ExtendedJSONDecoder, ExtendedJSONEncoder
+
+
+class Tracking_Origin(models.Model):
+    user: AbstractUser = models.ForeignKey(get_user_model(), null=True, blank=True, on_delete=models.RESTRICT)
+    address = models.CharField(max_length=250, blank=True)
+    endpoint = models.CharField(max_length=250) # name of view, test, etc
+    auth = models.CharField(max_length=250, blank=True)
+    details = models.CharField(max_length=250, blank=True)
+    # --------------------
+    created = models.DateTimeField(auto_now_add=True)
+
+
+    class Meta:
+        constraints = [
+            models.UniqueConstraint(fields=["user", "address", "endpoint", "auth", "details"], name='zut_tracking_origin_uniq'),
+            models.UniqueConstraint(fields=["address", "endpoint", "auth", "details"], condition=models.Q(user=None), name='zut_tracking_origin_uniq_without_user'),
+        ]
+
+
+    @property
+    def details_title(self):
+        details = {}
+        
+        if self.user:
+            details["User"] = self.user.username
+        
+        if self.address:
+            details["Adress"] = self.address
+
+        if self.endpoint:
+            details["Endpoint"] = self.endpoint
+
+        if self.auth:
+            details["Auth"] = self.auth
+        
+        if self.details:
+            details["Details"] = self.details
+
+        if details:
+            return mark_safe("\n".join(key.replace('\"', '&quot;') + " : " + val.replace('\"', '&quot;') for key, val in details.items()))
+        else:
+            return mark_safe(f"Origine #{self.id}")
+
+
+    def __str__(self):
+        """ Return main info: user, or address, or endpoint """
+        if self.user:
+            return self.user.username
+
+        if self.address:
+            return self.address
+
+        if self.endpoint:
+            return self.endpoint
+
+        return f"#{self.id}"
+
+
+    @classmethod
+    def for_request(cls, request: HttpRequest):
+        if request.user.is_authenticated:
+            user = request.user
+        else:
+            user = None
+
+        try:
+            auth = request.auth
+            if auth is not None and not isinstance(auth, str):
+                if isinstance(auth, models.Model):
+                    auth = auth._meta.model_name
+                else:
+                    auth = str(auth)
+        except AttributeError:
+            auth = ""
+
+        endpoint = request.path_info
+
+        x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
+        if x_forwarded_for:
+            address = x_forwarded_for.split(',')[-1].strip()
+        else:
+            address = request.META.get('REMOTE_ADDR')
+        
+        details = request.META['HTTP_USER_AGENT']
+
+        origin, _ = cls.objects.get_or_create(user=user, address=address, endpoint=endpoint, auth=auth, details=details)
+        return origin
+
+
+    @classmethod
+    def for_endpoint(cls, endpoint: str, details: str = ""):
+        origin, _ = cls.objects.get_or_create(user=None, address="", endpoint=endpoint, auth="", details=details)
+        return origin
+
+
+class Tracking_EntityType(models.Model):
+    app_label = models.CharField(max_length=1000)
+    model_name = models.CharField(max_length=1000)
+    # --------------------
+    created = models.DateTimeField(auto_now_add=True)
+
+
+    class Meta:
+        unique_together = [
+            ('app_label', 'model_name')
+        ]
+
+
+class Tracking_History(models.Model):
+    entity_type = models.ForeignKey(Tracking_EntityType, on_delete=models.CASCADE)
+    entity_id = models.BigIntegerField()
+    # --------------------
+    data = models.JSONField(encoder=ExtendedJSONEncoder, decoder=ExtendedJSONDecoder, help_text="Modified data")
+    data_saved = models.DateTimeField(help_text="Date of last save of historized data")
+    origin = models.ForeignKey(Tracking_Origin, on_delete=models.RESTRICT, null=True, blank=True, related_name="+", help_text="Origin of insertion of history entry")
+    # --------------------
+    created = models.DateTimeField(auto_now_add=True, help_text="Date of insertion of history entry")
+
+    class Meta:
+        index_together = [
+            ("entity_type", "entity_id")
+        ]
+        ordering = ["created"]
```

## zut/inout/InCsv.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-from __future__ import annotations
-
-import csv
-from io import IOBase
-from pathlib import Path
-from typing import Any
-
-from ..csv import get_default_csv_delimiter
-from ..text import reconfigure_encoding
-from .. import filesh
-from .InTable import InTable
-
-class InCsv(InTable):
-    def __init__(self, src: Path|str|IOBase, *, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):     
-        super().__init__(src, **kwargs)
-
-        self._encoding = encoding
-
-        self._delimiter = delimiter
-        if self._delimiter is None:
-            self._delimiter = get_default_csv_delimiter()
-
-        self._quotechar = quotechar
-        
-        # in _prepare:
-        self.file: IOBase = None
-        self._must_close_file: bool = None
-
-
-    def _prepare(self):
-        if isinstance(self.src, IOBase):
-            self.file = self.src
-            self._must_close_file = False
-        
-        else:
-            self.file = filesh.open_file(self.src, 'r', newline='', encoding=self._encoding)
-            self._must_close_file = True
-
-        self._encoding = reconfigure_encoding(self.file, self._encoding)        
-        self._csv_reader = csv.reader(self.file, delimiter=self._delimiter, quotechar=self._quotechar)
-        
-        self.headers = []        
-        try:
-            for header in next(self._csv_reader):
-                self.headers.append(str(header))
-        except StopIteration:
-            self.file.close()
-            raise ValueError(f"no headers found in {self.name}")
-
-
-    def _get_next_values(self) -> list[Any]:
-        return next(self._csv_reader)
-
-
-    def _end(self):
-        if self.file and self._must_close_file:
-            self.file.close()
+from __future__ import annotations
+
+import csv
+from io import IOBase
+from pathlib import Path
+from typing import Any
+
+from ..csv import get_default_csv_delimiter
+from ..text import reconfigure_encoding
+from .. import filesh
+from .InTable import InTable
+
+class InCsv(InTable):
+    def __init__(self, src: Path|str|IOBase, *, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):     
+        super().__init__(src, **kwargs)
+
+        self._encoding = encoding
+
+        self._delimiter = delimiter
+        if self._delimiter is None:
+            self._delimiter = get_default_csv_delimiter()
+
+        self._quotechar = quotechar
+        
+        # in _prepare:
+        self.file: IOBase = None
+        self._must_close_file: bool = None
+
+
+    def _prepare(self):
+        if isinstance(self.src, IOBase):
+            self.file = self.src
+            self._must_close_file = False
+        
+        else:
+            self.file = filesh.open_file(self.src, 'r', newline='', encoding=self._encoding)
+            self._must_close_file = True
+
+        self._encoding = reconfigure_encoding(self.file, self._encoding)        
+        self._csv_reader = csv.reader(self.file, delimiter=self._delimiter, quotechar=self._quotechar)
+        
+        self.headers = []        
+        try:
+            for header in next(self._csv_reader):
+                self.headers.append(str(header))
+        except StopIteration:
+            self.file.close()
+            raise ValueError(f"no headers found in {self.name}")
+
+
+    def _get_next_values(self) -> list[Any]:
+        return next(self._csv_reader)
+
+
+    def _end(self):
+        if self.file and self._must_close_file:
+            self.file.close()
```

## zut/inout/InDb.py

 * *Ordering differences only*

```diff
@@ -1,62 +1,62 @@
-from __future__ import annotations
-
-import logging
-from hashlib import sha1
-
-from ..db import create_db_wrapper_with_schema_and_table, DbWrapper
-from ..colors import Colors
-from .InTable import InTable
-
-logger = logging.getLogger(__name__)
-
-
-class InDb(InTable):
-    def __init__(self, src: str|DbWrapper, query: str = None, params: list|tuple|dict = None, limit: int = None, **kwargs):
-        if isinstance(src, DbWrapper):
-            self.db = src
-            schema = None
-            table = None
-            super().__init__(self.db.get_uri(with_password=False), **kwargs)
-            self._must_exit_db = False
-        else:
-            super().__init__(src, **kwargs)
-            self.db, schema, table = create_db_wrapper_with_schema_and_table(self.src)
-            self.name = self.db.get_uri(table=(schema, table), with_password=False)
-            self._must_exit_db = True
-        
-
-        self._query = query
-        self._limit = limit
-        self._params = params
-        if self._query:
-            sha1_prefix = sha1(self._query.encode('utf-8')).hexdigest()[0:8]
-            self.name = f"{self.name}#{sha1_prefix}"
-        elif table:
-            if self._params:
-                raise ValueError(f'table query cannot contain params')
-            self._query = self.db.get_select_table_query((schema, table))
-        else:
-            raise ValueError(f'neither a table or a query was given')
-
-        if self._debug:
-            logger.debug(f"execute {self.name} with params {self._params}, limit={self._limit}\n{Colors.CYAN}{self._query}{Colors.RESET}")
-
-        # set in __enter__() -> _prepare():
-        self._cursor = None
-
-
-    def _prepare(self):
-        self._cursor = self.db.execute_get_cursor(self._query, self._params, limit=self._limit)
-
-        self.headers = self.db.get_cursor_column_names(self._cursor)
-
-
-    def _get_next_values(self):
-        cursor_row = next(self._cursor)
-        return list(cursor_row)
-
-
-    def _end(self):
-        self._cursor.close()
-        if self._must_exit_db:
-            self.db.__exit__()
+from __future__ import annotations
+
+import logging
+from hashlib import sha1
+
+from ..db import create_db_wrapper_with_schema_and_table, DbWrapper
+from ..colors import Colors
+from .InTable import InTable
+
+logger = logging.getLogger(__name__)
+
+
+class InDb(InTable):
+    def __init__(self, src: str|DbWrapper, query: str = None, params: list|tuple|dict = None, limit: int = None, **kwargs):
+        if isinstance(src, DbWrapper):
+            self.db = src
+            schema = None
+            table = None
+            super().__init__(self.db.get_uri(with_password=False), **kwargs)
+            self._must_exit_db = False
+        else:
+            super().__init__(src, **kwargs)
+            self.db, schema, table = create_db_wrapper_with_schema_and_table(self.src)
+            self.name = self.db.get_uri(table=(schema, table), with_password=False)
+            self._must_exit_db = True
+        
+
+        self._query = query
+        self._limit = limit
+        self._params = params
+        if self._query:
+            sha1_prefix = sha1(self._query.encode('utf-8')).hexdigest()[0:8]
+            self.name = f"{self.name}#{sha1_prefix}"
+        elif table:
+            if self._params:
+                raise ValueError(f'table query cannot contain params')
+            self._query = self.db.get_select_table_query((schema, table))
+        else:
+            raise ValueError(f'neither a table or a query was given')
+
+        if self._debug:
+            logger.debug(f"execute {self.name} with params {self._params}, limit={self._limit}\n{Colors.CYAN}{self._query}{Colors.RESET}")
+
+        # set in __enter__() -> _prepare():
+        self._cursor = None
+
+
+    def _prepare(self):
+        self._cursor = self.db.execute_get_cursor(self._query, self._params, limit=self._limit)
+
+        self.headers = self.db.get_cursor_column_names(self._cursor)
+
+
+    def _get_next_values(self):
+        cursor_row = next(self._cursor)
+        return list(cursor_row)
+
+
+    def _end(self):
+        self._cursor.close()
+        if self._must_exit_db:
+            self.db.__exit__()
```

## zut/inout/InExcel.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-from __future__ import annotations
-import logging
-from pathlib import Path
-from .InTable import InTable
-from .OutExcel import OutExcel
-from .utils import get_inout_name, split_excel_path
-
-try:
-    from ..excel import ExcelWorkbook
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class InExcel(InTable):
-    @classmethod
-    def is_available(cls):
-        return _available
-
-
-    def __init__(self, src, **kwargs):
-        # Prepare arguments for base classes (InTable)
-        if not isinstance(src, (str,Path)):
-            raise ValueError(f"InExcel's src must be a str or path, got {type(src).__name__}: {src}")
-        
-        src, self.table_name = split_excel_path(src, default_table_name=OutExcel._DEFAULT_TABLE_NAME, **kwargs)
-        
-        # Initialize base classes (InTable)
-        super().__init__(src=src, **kwargs)
-        
-        # Modify attributes set by base classes (InTable)
-        self.name = get_inout_name(self.src) + f'#{self.table_name}'
-
-
-    def _prepare(self):
-        wb = ExcelWorkbook.get_or_create_cached(self.src)
-        self.table = wb.get_table(self.table_name)
-        self.headers = self.table.column_names if self.table.has_headers else None
-        self._iterator = iter(self.table)
-
-
-    def _get_next_values(self):
-        return next(self._iterator)
+from __future__ import annotations
+import logging
+from pathlib import Path
+from .InTable import InTable
+from .OutExcel import OutExcel
+from .utils import get_inout_name, split_excel_path
+
+try:
+    from ..excel import ExcelWorkbook
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class InExcel(InTable):
+    @classmethod
+    def is_available(cls):
+        return _available
+
+
+    def __init__(self, src, **kwargs):
+        # Prepare arguments for base classes (InTable)
+        if not isinstance(src, (str,Path)):
+            raise ValueError(f"InExcel's src must be a str or path, got {type(src).__name__}: {src}")
+        
+        src, self.table_name = split_excel_path(src, default_table_name=OutExcel._DEFAULT_TABLE_NAME, **kwargs)
+        
+        # Initialize base classes (InTable)
+        super().__init__(src=src, **kwargs)
+        
+        # Modify attributes set by base classes (InTable)
+        self.name = get_inout_name(self.src) + f'#{self.table_name}'
+
+
+    def _prepare(self):
+        wb = ExcelWorkbook.get_or_create_cached(self.src)
+        self.table = wb.get_table(self.table_name)
+        self.headers = self.table.column_names if self.table.has_headers else None
+        self._iterator = iter(self.table)
+
+
+    def _get_next_values(self):
+        return next(self._iterator)
```

## zut/inout/InTable.py

 * *Ordering differences only*

```diff
@@ -1,131 +1,131 @@
-from __future__ import annotations
-
-import logging
-from datetime import datetime, timezone
-from pathlib import Path
-from time import time_ns
-from typing import Any, Callable
-from ..tabular import Row
-from ..datetime import is_aware, make_aware
-from .utils import normalize_inout, get_inout_name
-
-logger = logging.getLogger(__name__)
-
-
-class InTable:
-    @classmethod
-    def is_available(cls):
-        return True
-    
-    def __init__(self, src, *, src_dir: str|Path|None = None, title: str|None = None, debug: bool = None, naive_tz: timezone = 'local', formatters: dict[int|str,Callable] = None, **kwargs):
-        if not self.is_available():
-            raise ValueError(f"cannot use {type(self).__name__} (not available)")
-
-        self.src = normalize_inout(src, dir=src_dir, title=title, **kwargs)
-        self.name = get_inout_name(self.src)
-
-        self._debug = debug
-        self._naive_tz = naive_tz
-        self._formatters = formatters
-
-        # set in _prepare():
-        self.headers: list[str] = None
-        self.prepare_duration: int = None
-
-        # set in __iter__():
-        self._extract_t0: int = None
-        self.extract_duration: int = None
-        self.row_count: int = None           
-
-
-    def __enter__(self):        
-        if self._debug:
-            logger.debug(f"prepare {self.name}")
-            t0 = time_ns()
-
-        self._prepare()
-
-        # headers are now set, update formatters to point on index
-        if self._formatters:
-            new_entries = {}
-            for key, formatter in self._formatters.items():
-                if not isinstance(key, str):
-                    continue
-                try:
-                    index = self.headers.index(key)
-                except ValueError:
-                    continue
-                new_entries[index] = formatter
-            
-            for index, formatter in new_entries.items():
-                self._formatters[index] = formatter
-
-
-        if self._debug:
-            self.prepare_duration = time_ns() - t0
-
-        return self
-
-
-    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
-        self._end()
-
-        if self._debug:
-            if self.extract_duration is not None:
-                logger.debug(f"{self.row_count:,d} rows extracted from {self.name} (total duration: {(self.prepare_duration + self.extract_duration)/1e6:,.0f} ms, prepare: {self.prepare_duration/1e6:,.0f} ms, extract: {self.extract_duration/1e6:,.0f} ms)")
-            else:
-                logger.debug(f"prepared {self.name} (duration: {self.prepare_duration/1e6:,.0f} ms)")
-
-
-    def __iter__(self):
-        return self
-
-
-    def __next__(self):
-        if self.row_count is None:
-            if self._debug:
-                self._extract_t0 = time_ns()
-                self.extract_duration = 0
-            self.row_count = 0
-
-        values = self._get_next_values()
-
-        if self._debug:
-            self.extract_duration = time_ns() - self._extract_t0
-
-        self.row_count += 1
-
-        if self._naive_tz is not None or self._formatters:
-            if isinstance(values, Row):
-                values = list(values.values)
-            
-            for i, value in enumerate(values):
-                if self._formatters and i in self._formatters:
-                    formatter = self._formatters[i]
-                    value = formatter(value)
-
-                if self._naive_tz and isinstance(value, datetime) and not is_aware(value):
-                    value = make_aware(value, self._naive_tz)
-
-                values[i] = value
-        
-        if isinstance(values, Row):
-            return values
-        else:
-            return Row(values, headers=self.headers)
-
-
-    # -------------------------------------------------------------------------
-    # For subclasses
-    #
-
-    def _prepare(self):
-        pass
-
-
-    def _get_next_values(self) -> list[Any]:
-        raise StopIteration()
-
-
-    def _end(self):
-        pass
+from __future__ import annotations
+
+import logging
+from datetime import datetime, timezone
+from pathlib import Path
+from time import time_ns
+from typing import Any, Callable
+from ..tabular import Row
+from ..datetime import is_aware, make_aware
+from .utils import normalize_inout, get_inout_name
+
+logger = logging.getLogger(__name__)
+
+
+class InTable:
+    @classmethod
+    def is_available(cls):
+        return True
+    
+    def __init__(self, src, *, src_dir: str|Path|None = None, title: str|None = None, debug: bool = None, naive_tz: timezone = 'local', formatters: dict[int|str,Callable] = None, **kwargs):
+        if not self.is_available():
+            raise ValueError(f"cannot use {type(self).__name__} (not available)")
+
+        self.src = normalize_inout(src, dir=src_dir, title=title, **kwargs)
+        self.name = get_inout_name(self.src)
+
+        self._debug = debug
+        self._naive_tz = naive_tz
+        self._formatters = formatters
+
+        # set in _prepare():
+        self.headers: list[str] = None
+        self.prepare_duration: int = None
+
+        # set in __iter__():
+        self._extract_t0: int = None
+        self.extract_duration: int = None
+        self.row_count: int = None           
+
+
+    def __enter__(self):        
+        if self._debug:
+            logger.debug(f"prepare {self.name}")
+            t0 = time_ns()
+
+        self._prepare()
+
+        # headers are now set, update formatters to point on index
+        if self._formatters:
+            new_entries = {}
+            for key, formatter in self._formatters.items():
+                if not isinstance(key, str):
+                    continue
+                try:
+                    index = self.headers.index(key)
+                except ValueError:
+                    continue
+                new_entries[index] = formatter
+            
+            for index, formatter in new_entries.items():
+                self._formatters[index] = formatter
+
+
+        if self._debug:
+            self.prepare_duration = time_ns() - t0
+
+        return self
+
+
+    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
+        self._end()
+
+        if self._debug:
+            if self.extract_duration is not None:
+                logger.debug(f"{self.row_count:,d} rows extracted from {self.name} (total duration: {(self.prepare_duration + self.extract_duration)/1e6:,.0f} ms, prepare: {self.prepare_duration/1e6:,.0f} ms, extract: {self.extract_duration/1e6:,.0f} ms)")
+            else:
+                logger.debug(f"prepared {self.name} (duration: {self.prepare_duration/1e6:,.0f} ms)")
+
+
+    def __iter__(self):
+        return self
+
+
+    def __next__(self):
+        if self.row_count is None:
+            if self._debug:
+                self._extract_t0 = time_ns()
+                self.extract_duration = 0
+            self.row_count = 0
+
+        values = self._get_next_values()
+
+        if self._debug:
+            self.extract_duration = time_ns() - self._extract_t0
+
+        self.row_count += 1
+
+        if self._naive_tz is not None or self._formatters:
+            if isinstance(values, Row):
+                values = list(values.values)
+            
+            for i, value in enumerate(values):
+                if self._formatters and i in self._formatters:
+                    formatter = self._formatters[i]
+                    value = formatter(value)
+
+                if self._naive_tz and isinstance(value, datetime) and not is_aware(value):
+                    value = make_aware(value, self._naive_tz)
+
+                values[i] = value
+        
+        if isinstance(values, Row):
+            return values
+        else:
+            return Row(values, headers=self.headers)
+
+
+    # -------------------------------------------------------------------------
+    # For subclasses
+    #
+
+    def _prepare(self):
+        pass
+
+
+    def _get_next_values(self) -> list[Any]:
+        raise StopIteration()
+
+
+    def _end(self):
+        pass
```

## zut/inout/OutCsv.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-from __future__ import annotations
-import csv
-import logging
-from datetime import datetime
-from decimal import Decimal
-from io import IOBase
-from pathlib import Path
-from typing import Any, Iterable
-from ..text import ValueString
-from ..numeric import get_default_decimal_separator
-from ..csv import get_csv_headers, get_default_csv_delimiter
-from .. import filesh
-from .OutTable import OutTable
-
-logger = logging.getLogger(__name__)
-
-
-class OutCsv(OutTable):
-    file: IOBase
-    
-    def __init__(self, out: str|Path|IOBase|None = None, *, delimiter: str = None, quotechar: str = '"', decimal_separator: str = None, **kwargs):
-        self._delimiter = delimiter
-        if self._delimiter is None:
-            self._delimiter = get_default_csv_delimiter()
-
-        self._quotechar = quotechar
-
-        self._decimal_separator = decimal_separator
-        if self._decimal_separator is None:
-            self._decimal_separator = get_default_decimal_separator()
-
-        self._writer: csv._writer = None
-
-        super().__init__(out, newline='', **kwargs)
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-    
-    def _get_existing_headers(self) -> list[str]|None:
-        if not self._append:
-            return []  # file will be truncated
-        
-        if not isinstance(self.out, str) or not filesh.exists(self.out):
-            return []  # we consider output as a new file
-        
-        # From now on, we append to an existing file
-        existing_headers = get_csv_headers(self.out, encoding=self._encoding, delimiter=self._delimiter, quotechar=self._quotechar)
-        if existing_headers is None:
-            return []  # existing file is empty
-        
-        return existing_headers
-
-
-    def _export_new_headers(self, headers: list[str]):
-        self._get_writer().writerow(headers)
-        self.file.flush()
-
-
-    def _add_new_header(self, header: str, index: int):
-        # cannot modify existing headers (we don't modify existing file)
-        logger.warning(f"header \"{header}\" not found in existing headers: values will be appended at column {index + 1} without a column title")
-
-
-    def _export_prepared_row(self, row: Iterable):
-        self._get_writer().writerow(row)
-        self.file.flush()
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if isinstance(value, ValueString):
-            # For CSV we want the actual value (we can still apply formatting when opening in Excel)
-            # NOTE: we don't do it on the general case because the formatted ValueString may be interesting (e.g. for tabulate outputs)
-            value = value.value
-                
-        if isinstance(value, bool):
-            return 'true' if value else 'false'
-        elif isinstance(value, datetime):
-            # If output is expected in a given timezone, we make this datetime naive in the target timezone and display it in a format understandable by Excel
-            if value.tzinfo:
-                if self._tz:
-                    value: datetime = value.astimezone(None if self._tz == 'local' else self._tz)
-                    use_tzinfo = False
-                else:
-                    use_tzinfo = True
-            else:
-                use_tzinfo = False
-
-            # Format microseconds
-            if value.microsecond == 0:
-                mspart = ''
-            else:
-                mspart = '.' + value.strftime('%f')
-            
-            # Format tzinfo and microseconds
-            if use_tzinfo:
-                tzpart = value.strftime('%z')
-                if len(tzpart) == 5:
-                    tzpart = tzpart[0:3] + ':' + tzpart[3:]
-            else:
-                tzpart = ''
-
-            return value.strftime("%Y-%m-%d %H:%M:%S") + mspart + tzpart
-        elif isinstance(value, (float,Decimal)) and self._decimal_separator and self._decimal_separator != '.':
-            return str(value).replace('.', self._decimal_separator)
-        else:
-            return super()._format_value(value, index)
-        
-
-    # -------------------------------------------------------------------------
-    # Internal helpers
-    #    
-
-    def _get_writer(self):
-        if not self._writer:
-            # NOTE: setting a value for escapechar (with doublequote=False) fails: value containing escapechar is not quoted (cf. test_table_json)
-            self._writer = csv.writer(self.file, delimiter=self._delimiter, quotechar=self._quotechar)
-        return self._writer
+from __future__ import annotations
+import csv
+import logging
+from datetime import datetime
+from decimal import Decimal
+from io import IOBase
+from pathlib import Path
+from typing import Any, Iterable
+from ..text import ValueString
+from ..numeric import get_default_decimal_separator
+from ..csv import get_csv_headers, get_default_csv_delimiter
+from .. import filesh
+from .OutTable import OutTable
+
+logger = logging.getLogger(__name__)
+
+
+class OutCsv(OutTable):
+    file: IOBase
+    
+    def __init__(self, out: str|Path|IOBase|None = None, *, delimiter: str = None, quotechar: str = '"', decimal_separator: str = None, **kwargs):
+        self._delimiter = delimiter
+        if self._delimiter is None:
+            self._delimiter = get_default_csv_delimiter()
+
+        self._quotechar = quotechar
+
+        self._decimal_separator = decimal_separator
+        if self._decimal_separator is None:
+            self._decimal_separator = get_default_decimal_separator()
+
+        self._writer: csv._writer = None
+
+        super().__init__(out, newline='', **kwargs)
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+    
+    def _get_existing_headers(self) -> list[str]|None:
+        if not self._append:
+            return []  # file will be truncated
+        
+        if not isinstance(self.out, str) or not filesh.exists(self.out):
+            return []  # we consider output as a new file
+        
+        # From now on, we append to an existing file
+        existing_headers = get_csv_headers(self.out, encoding=self._encoding, delimiter=self._delimiter, quotechar=self._quotechar)
+        if existing_headers is None:
+            return []  # existing file is empty
+        
+        return existing_headers
+
+
+    def _export_new_headers(self, headers: list[str]):
+        self._get_writer().writerow(headers)
+        self.file.flush()
+
+
+    def _add_new_header(self, header: str, index: int):
+        # cannot modify existing headers (we don't modify existing file)
+        logger.warning(f"header \"{header}\" not found in existing headers: values will be appended at column {index + 1} without a column title")
+
+
+    def _export_prepared_row(self, row: Iterable):
+        self._get_writer().writerow(row)
+        self.file.flush()
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if isinstance(value, ValueString):
+            # For CSV we want the actual value (we can still apply formatting when opening in Excel)
+            # NOTE: we don't do it on the general case because the formatted ValueString may be interesting (e.g. for tabulate outputs)
+            value = value.value
+                
+        if isinstance(value, bool):
+            return 'true' if value else 'false'
+        elif isinstance(value, datetime):
+            # If output is expected in a given timezone, we make this datetime naive in the target timezone and display it in a format understandable by Excel
+            if value.tzinfo:
+                if self._tz:
+                    value: datetime = value.astimezone(None if self._tz == 'local' else self._tz)
+                    use_tzinfo = False
+                else:
+                    use_tzinfo = True
+            else:
+                use_tzinfo = False
+
+            # Format microseconds
+            if value.microsecond == 0:
+                mspart = ''
+            else:
+                mspart = '.' + value.strftime('%f')
+            
+            # Format tzinfo and microseconds
+            if use_tzinfo:
+                tzpart = value.strftime('%z')
+                if len(tzpart) == 5:
+                    tzpart = tzpart[0:3] + ':' + tzpart[3:]
+            else:
+                tzpart = ''
+
+            return value.strftime("%Y-%m-%d %H:%M:%S") + mspart + tzpart
+        elif isinstance(value, (float,Decimal)) and self._decimal_separator and self._decimal_separator != '.':
+            return str(value).replace('.', self._decimal_separator)
+        else:
+            return super()._format_value(value, index)
+        
+
+    # -------------------------------------------------------------------------
+    # Internal helpers
+    #    
+
+    def _get_writer(self):
+        if not self._writer:
+            # NOTE: setting a value for escapechar (with doublequote=False) fails: value containing escapechar is not quoted (cf. test_table_json)
+            self._writer = csv.writer(self.file, delimiter=self._delimiter, quotechar=self._quotechar)
+        return self._writer
```

## zut/inout/OutDb.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-from __future__ import annotations
-from io import StringIO
-import logging
-
-from ..db import create_db_wrapper_with_schema_and_table
-from .OutCsv import OutCsv
-
-logger = logging.getLogger(__name__)
-
-
-class OutDb(OutCsv):
-    def __init__(self, out: str, decimal_separator: str = None, **kwargs):
-        # Prepare arguments for base classes (OutCsv, OutTable, OutFile)
-        if not isinstance(out, str):
-            raise ValueError(f"OutDb's out must be a str, got {type(out).__name__}: {out}")
-    
-        decimal_separator = '.'
-        
-        # Initialize base classes (OutCsv, OutTable, OutFile)
-        super().__init__(StringIO(), decimal_separator=decimal_separator, **kwargs)
-
-        # Modify attributes set by base classes (OutCsv, OutTable, OutFile)
-        self.db, self.schema_name, self.table_name = create_db_wrapper_with_schema_and_table(out.format(**kwargs))
-        if not self.table_name:
-            raise ValueError(f"invalid db target: table name not provided")
-        
-        self.name = self.db.get_uri(table=(self.schema_name, self.table_name), with_password=False)
-
-    # -------------------------------------------------------------------------
-    # OutFile subclassing
-    #
-
-    def _open_file(self):
-        existing_headers = super()._open_file()
-
-        # Create, drop or truncate table
-        if not self._append:
-            logger.debug(f"truncate table %s.%s", self.schema_name, self.table_name)
-            self.db.truncate_table((self.schema_name, self.table_name))
-
-        return existing_headers
-                
-
-    def _close_file(self):
-        if not self._headers:
-            if self._row_count == 0:
-                return
-            raise ValueError(f"cannot export rows to database: no headers")
-                
-        logger.debug(f"copy data to table %s.%s", self.schema_name, self.table_name)
-        self.out.seek(0)
-        self.db.copy_from_csv(self.out, (self.schema_name, self.table_name), columns=self._headers, delimiter=self._delimiter, quotechar=self._quotechar)
-        self.db.__exit__()
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-    
-    def _get_existing_headers(self) -> list[str]|None:
-        # Only export given headers, but check that they are in the target table
-        column_names = self.db.get_table_column_names((self.schema_name, self.table_name))
-        if not self._headers:
-            raise ValueError(f'headers must be set')
-        
-        missing_columns = []
-        for header in self._headers:
-            if not header in column_names:
-                missing_columns.append(header)
-
-        if missing_columns:
-            raise ValueError(f"column not found in out table: {', '.join(missing_columns)}")
-
-        return []
+from __future__ import annotations
+from io import StringIO
+import logging
+
+from ..db import create_db_wrapper_with_schema_and_table
+from .OutCsv import OutCsv
+
+logger = logging.getLogger(__name__)
+
+
+class OutDb(OutCsv):
+    def __init__(self, out: str, decimal_separator: str = None, **kwargs):
+        # Prepare arguments for base classes (OutCsv, OutTable, OutFile)
+        if not isinstance(out, str):
+            raise ValueError(f"OutDb's out must be a str, got {type(out).__name__}: {out}")
+    
+        decimal_separator = '.'
+        
+        # Initialize base classes (OutCsv, OutTable, OutFile)
+        super().__init__(StringIO(), decimal_separator=decimal_separator, **kwargs)
+
+        # Modify attributes set by base classes (OutCsv, OutTable, OutFile)
+        self.db, self.schema_name, self.table_name = create_db_wrapper_with_schema_and_table(out.format(**kwargs))
+        if not self.table_name:
+            raise ValueError(f"invalid db target: table name not provided")
+        
+        self.name = self.db.get_uri(table=(self.schema_name, self.table_name), with_password=False)
+
+    # -------------------------------------------------------------------------
+    # OutFile subclassing
+    #
+
+    def _open_file(self):
+        existing_headers = super()._open_file()
+
+        # Create, drop or truncate table
+        if not self._append:
+            logger.debug(f"truncate table %s.%s", self.schema_name, self.table_name)
+            self.db.truncate_table((self.schema_name, self.table_name))
+
+        return existing_headers
+                
+
+    def _close_file(self):
+        if not self._headers:
+            if self._row_count == 0:
+                return
+            raise ValueError(f"cannot export rows to database: no headers")
+                
+        logger.debug(f"copy data to table %s.%s", self.schema_name, self.table_name)
+        self.out.seek(0)
+        self.db.copy_from_csv(self.out, (self.schema_name, self.table_name), columns=self._headers, delimiter=self._delimiter, quotechar=self._quotechar)
+        self.db.__exit__()
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+    
+    def _get_existing_headers(self) -> list[str]|None:
+        # Only export given headers, but check that they are in the target table
+        column_names = self.db.get_table_column_names((self.schema_name, self.table_name))
+        if not self._headers:
+            raise ValueError(f'headers must be set')
+        
+        missing_columns = []
+        for header in self._headers:
+            if not header in column_names:
+                missing_columns.append(header)
+
+        if missing_columns:
+            raise ValueError(f"column not found in out table: {', '.join(missing_columns)}")
+
+        return []
```

## zut/inout/OutExcel.py

 * *Ordering differences only*

```diff
@@ -1,102 +1,102 @@
-from __future__ import annotations
-from datetime import datetime
-import logging
-from pathlib import Path
-from typing import Any, Iterable
-from .. import filesh
-from .OutTable import OutTable
-from .utils import get_inout_name, split_excel_path
-
-try:
-    from ..excel import ExcelWorkbook
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class OutExcel(OutTable):
-    file: ExcelWorkbook
-    _DEFAULT_TABLE_NAME = 'Out'
-
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-
-    def __init__(self, out = None, **kwargs):
-        # Prepare arguments for base classes (OutTable, OutFile)
-        if not isinstance(out, (str,Path)):
-            raise ValueError(f"OutExcel's out must be a str or path, got {type(out).__name__}: {out}")
-        
-        out, self.table_name = split_excel_path(out, default_table_name=OutExcel._DEFAULT_TABLE_NAME, **kwargs)
-        
-        # Initialize base classes (OutTable, OutFile)
-        super().__init__(out=out, **kwargs)
-        
-        # Modify attributes set by base classes (OutTable, OutFile)
-        self.name = get_inout_name(self.out) + f'#{self.table_name}'
-
-
-    # -------------------------------------------------------------------------
-    # OutFile subclassing
-    #
-
-    def _open_file(self):
-        self._print_title()
-
-        parent = filesh.dirname(self.out)
-        if parent and not filesh.exists(parent):
-            filesh.makedirs(parent)
-
-        self.file = ExcelWorkbook.get_or_create_cached(self.out)
-        self._must_close_file = True
-
-        self.table = self.file.get_table(self.table_name, default=None)
-        if not self.table:
-            self.table = self.file.create_table(self.table_name, no_headers=True if not self._headers else False)
-        
-        if not self._append:
-            self.table.truncate()
-
-        existing_headers = self._get_existing_headers()
-        return existing_headers
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-
-    def _get_existing_headers(self) -> list[str]|None:
-        return self.table.column_names if self.table.has_headers else None
-    
-
-    def _add_new_header(self, header: str, index: int):
-        self.table.insert_col(header)
-    
-
-    def _export_prepared_row(self, row: Iterable):
-        table_row = self.table.insert_row()
-        
-        for i, value in enumerate(row):
-            if value is None:
-                # keep default formula if any (applied during table.erase_cell(), called from table.insert_row())
-                continue
-            
-            if i < len(table_row):            
-                table_row[i] = value
-            else:
-                logger.warning(f'ignore values from index {i} ({value})')
-                break
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if isinstance(value, datetime) and value.tzinfo:
-            # Excel does not support timezones in datetimes
-            if self._tz:
-                value = value.astimezone(None if self._tz == 'local' else self._tz)
-            return value.replace(tzinfo=None)
-        else:
-            return super()._format_value(value, index)
+from __future__ import annotations
+from datetime import datetime
+import logging
+from pathlib import Path
+from typing import Any, Iterable
+from .. import filesh
+from .OutTable import OutTable
+from .utils import get_inout_name, split_excel_path
+
+try:
+    from ..excel import ExcelWorkbook
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class OutExcel(OutTable):
+    file: ExcelWorkbook
+    _DEFAULT_TABLE_NAME = 'Out'
+
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+
+    def __init__(self, out = None, **kwargs):
+        # Prepare arguments for base classes (OutTable, OutFile)
+        if not isinstance(out, (str,Path)):
+            raise ValueError(f"OutExcel's out must be a str or path, got {type(out).__name__}: {out}")
+        
+        out, self.table_name = split_excel_path(out, default_table_name=OutExcel._DEFAULT_TABLE_NAME, **kwargs)
+        
+        # Initialize base classes (OutTable, OutFile)
+        super().__init__(out=out, **kwargs)
+        
+        # Modify attributes set by base classes (OutTable, OutFile)
+        self.name = get_inout_name(self.out) + f'#{self.table_name}'
+
+
+    # -------------------------------------------------------------------------
+    # OutFile subclassing
+    #
+
+    def _open_file(self):
+        self._print_title()
+
+        parent = filesh.dirname(self.out)
+        if parent and not filesh.exists(parent):
+            filesh.makedirs(parent)
+
+        self.file = ExcelWorkbook.get_or_create_cached(self.out)
+        self._must_close_file = True
+
+        self.table = self.file.get_table(self.table_name, default=None)
+        if not self.table:
+            self.table = self.file.create_table(self.table_name, no_headers=True if not self._headers else False)
+        
+        if not self._append:
+            self.table.truncate()
+
+        existing_headers = self._get_existing_headers()
+        return existing_headers
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+
+    def _get_existing_headers(self) -> list[str]|None:
+        return self.table.column_names if self.table.has_headers else None
+    
+
+    def _add_new_header(self, header: str, index: int):
+        self.table.insert_col(header)
+    
+
+    def _export_prepared_row(self, row: Iterable):
+        table_row = self.table.insert_row()
+        
+        for i, value in enumerate(row):
+            if value is None:
+                # keep default formula if any (applied during table.erase_cell(), called from table.insert_row())
+                continue
+            
+            if i < len(table_row):            
+                table_row[i] = value
+            else:
+                logger.warning(f'ignore values from index {i} ({value})')
+                break
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if isinstance(value, datetime) and value.tzinfo:
+            # Excel does not support timezones in datetimes
+            if self._tz:
+                value = value.astimezone(None if self._tz == 'local' else self._tz)
+            return value.replace(tzinfo=None)
+        else:
+            return super()._format_value(value, index)
```

## zut/inout/OutFile.py

 * *Ordering differences only*

```diff
@@ -1,104 +1,104 @@
-from __future__ import annotations
-from io import IOBase
-from atexit import register as register_atexit
-import logging
-import os
-from pathlib import Path
-import sys
-from typing import Any, Callable
-from .. import filesh
-
-from .utils import get_inout_name, normalize_inout
-
-logger = logging.getLogger(__name__)
-
-
-class OutFile:
-    def __init__(self, out: str|Path|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool|Callable = None, **kwargs):
-        self.out = normalize_inout(out, dir=out_dir, title=title, **kwargs)
-        self.name = get_inout_name(self.out)
-        
-        self._title = title
-        self._append = append
-        self._encoding = encoding
-        self._newline = newline
-
-        self.file: IOBase = None
-        self._must_close_file: bool = True
-
-        self.atexit = atexit
-
-
-    # -------------------------------------------------------------------------
-    # Enter/exit context
-    #
-
-    def __enter__(self) -> IOBase:        
-        self._open_file()
-
-        if self.atexit:
-            if callable(self.atexit):
-                self.atexit(self)
-            else:
-                register_atexit(self.end)
-
-        return self.file
-
-
-    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
-        if not self.atexit:
-            self.end()
-
-            
-    def end(self):
-        """
-        This method is automatically executed when context ends (__exit__), except if `atexit` is truthy.
-        """
-        self._close_file()
-
-
-    # -------------------------------------------------------------------------
-    # Open file
-    #
-
-    def _open_file(self):
-        self._print_title()
-
-        if self.out in [sys.stdout, sys.stderr]:
-            self.file = self.out
-            self._must_close_file = False
-            
-        else:
-            if isinstance(self.out, IOBase):
-                self.file = self.out
-                self._must_close_file = False
-            
-            else:
-                parent = filesh.dirname(self.out)
-                if parent and not filesh.exists(parent):
-                    filesh.makedirs(parent)
-
-                self.file = filesh.open_file(self.out, 'a' if self._append else 'w', newline=self._newline, encoding=self._encoding)
-                self._must_close_file = True
-
-
-    def _print_title(self):
-        if self._title is False:
-            return
-        if self.out == os.devnull:
-            return
-
-        if self.out in [sys.stdout, sys.stderr]:
-            if self._title:
-                print(f"\n########## {self._title} ##########\n", file=self.out)
-        else:
-            logger.info(f"{'append' if self._append else 'export'}{f' {self._title}' if self._title else ''} to {self.name}")
-
-
-    # -------------------------------------------------------------------------
-    # Close file
-    #    
-
-    def _close_file(self):
-        if self.file and self._must_close_file:
-            self.file.close()
+from __future__ import annotations
+from io import IOBase
+from atexit import register as register_atexit
+import logging
+import os
+from pathlib import Path
+import sys
+from typing import Any, Callable
+from .. import filesh
+
+from .utils import get_inout_name, normalize_inout
+
+logger = logging.getLogger(__name__)
+
+
+class OutFile:
+    def __init__(self, out: str|Path|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool|Callable = None, **kwargs):
+        self.out = normalize_inout(out, dir=out_dir, title=title, **kwargs)
+        self.name = get_inout_name(self.out)
+        
+        self._title = title
+        self._append = append
+        self._encoding = encoding
+        self._newline = newline
+
+        self.file: IOBase = None
+        self._must_close_file: bool = True
+
+        self.atexit = atexit
+
+
+    # -------------------------------------------------------------------------
+    # Enter/exit context
+    #
+
+    def __enter__(self) -> IOBase:        
+        self._open_file()
+
+        if self.atexit:
+            if callable(self.atexit):
+                self.atexit(self)
+            else:
+                register_atexit(self.end)
+
+        return self.file
+
+
+    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
+        if not self.atexit:
+            self.end()
+
+            
+    def end(self):
+        """
+        This method is automatically executed when context ends (__exit__), except if `atexit` is truthy.
+        """
+        self._close_file()
+
+
+    # -------------------------------------------------------------------------
+    # Open file
+    #
+
+    def _open_file(self):
+        self._print_title()
+
+        if self.out in [sys.stdout, sys.stderr]:
+            self.file = self.out
+            self._must_close_file = False
+            
+        else:
+            if isinstance(self.out, IOBase):
+                self.file = self.out
+                self._must_close_file = False
+            
+            else:
+                parent = filesh.dirname(self.out)
+                if parent and not filesh.exists(parent):
+                    filesh.makedirs(parent)
+
+                self.file = filesh.open_file(self.out, 'a' if self._append else 'w', newline=self._newline, encoding=self._encoding)
+                self._must_close_file = True
+
+
+    def _print_title(self):
+        if self._title is False:
+            return
+        if self.out == os.devnull:
+            return
+
+        if self.out in [sys.stdout, sys.stderr]:
+            if self._title:
+                print(f"\n########## {self._title} ##########\n", file=self.out)
+        else:
+            logger.info(f"{'append' if self._append else 'export'}{f' {self._title}' if self._title else ''} to {self.name}")
+
+
+    # -------------------------------------------------------------------------
+    # Close file
+    #    
+
+    def _close_file(self):
+        if self.file and self._must_close_file:
+            self.file.close()
```

## zut/inout/OutTable.py

 * *Ordering differences only*

```diff
@@ -1,288 +1,288 @@
-from __future__ import annotations
-from atexit import register as register_atexit
-from datetime import datetime, timezone
-from decimal import Decimal
-from enum import Enum
-from io import IOBase
-import json
-import logging
-from pathlib import Path
-from types import FunctionType
-from typing import Any, Iterable
-from ..lang import is_list_or_tuple_of
-from .OutFile import OutFile
-
-logger = logging.getLogger(__name__)
-
-
-class OutTable(OutFile):
-    """
-    A mixin for appending and exporting tabular data.
-
-    Must be subclassed (to implement read/write of value).
-    """
-    @classmethod
-    def is_available(cls):
-        return True
-    
-    def __init__(self, out: str|Path|IOBase|None = None, append: bool = False, headers: list[str] = None, tz: timezone = None, **kwargs):
-        if not self.is_available():
-            raise ValueError(f"cannot use {type(self).__name__} (not available)")
-    
-        super().__init__(out, **kwargs)
-        
-        self._append = append
-
-        self._start_atonce = True if headers else False # we consider than if headers are given now, they are final, so we don't need to wait for dict rows
-        """ If true, delay rows until end of export """
-
-        self._actually_started = False
-
-        self._headers: list[str]|None = [str(header) for header in headers] if headers else None        
-        self._row_count = 0
-
-        self._delayed_rows: list[Iterable] = []
-
-        self._reordering: list[int]|None = None
-        self._reordering_default: list|None = None
-
-        self._tz = timezone.utc if tz == 'utc' else tz
-
-
-    # -------------------------------------------------------------------------
-    # Actions on start/end (see OutFile)
-    #
-
-    def __enter__(self) -> OutTable:
-        if self._start_atonce:
-            if self._headers:
-                existing_headers = self._open_file()
-                self._export_headers(existing_headers)
-                self._actually_started = True
-
-        if self.atexit:
-            if callable(self.atexit):
-                self.atexit(self)
-            else:
-                register_atexit(self.end)
-
-        return self
-    
-
-    def end(self):
-        """
-        This method is automatically executed when context ends (__exit__), except if `atexit` is truthy.
-        """
-        if not self._actually_started:        
-            existing_headers = self._open_file()
-            self._export_headers(existing_headers, at_end=True)
-            self._actually_started = True
-
-        self._export_delayed_rows()
-
-        self._delayed_rows = []
-        self._close_file()
-    
-
-    # -------------------------------------------------------------------------
-    # Following methods are the core table export mechanism.
-    # They are not supposed to be subclassed.
-    #
-
-    def _export_headers(self, existing_headers: list[str]|None, at_end=False):
-        if not self._headers:
-            if existing_headers:
-                if not (at_end and not self._delayed_rows):
-                    raise ValueError(f"cannot export table without headers: existing table contains headers ({', '.join(existing_headers)})")
-            return
-        
-        if existing_headers is None:
-            if self._headers:
-                if not (at_end and not self._delayed_rows):
-                    raise ValueError(f"cannot export table with headers: existing table cannot contain headers")
-            return
-        
-        if existing_headers == self._headers:
-            return
-
-        if len(existing_headers) == 0:
-            self._export_new_headers(self._headers)
-            return
-
-        # From now own, we must compare existing and target headers
-        self._reordering = []
-        self._reordering_default = [None] * len(existing_headers)
-
-        for header in self._headers:
-            try:
-                index = existing_headers.index(header)
-                self._reordering.append(index)
-            except ValueError:
-                self._reordering_default.append(None)
-                index = len(self._reordering_default) - 1
-                self._reordering.append(index)
-                self._add_new_header(header, index)
-    
-
-    def append(self, row):
-        if isinstance(row, dict):
-            row = self._dict_to_row(row)        
-        else:
-            row = self._iterable_to_row(row)
-
-        if self._start_atonce:
-            self._export_row(row)
-        else:
-            self._delayed_rows.append(row)
-        
-        self._row_count += 1
-
-
-    def _export_row(self, row: Iterable):
-        if not self._actually_started:
-            existing_headers = self._open_file()
-            self._export_headers(existing_headers)
-            self._actually_started = True
-
-        self._export_prepared_row(self._prepare_row(row))
-
-
-    def _prepare_row(self, data: Iterable):
-        if not self._reordering:
-            row = [self._format_value(value, i) for i, value in enumerate(data)]
-            if self._headers:
-                while len(row) < len(self._headers):
-                    row.append(None)
-            return row
-        
-        else:
-            row = list(self._reordering_default)
-            for i, value in enumerate(data):
-                value = self._format_value(value, i)
-                if i < len(self._reordering):
-                    index = self._reordering[i]
-                    row[index] = value
-                else:
-                    row.append(value)
-
-            return row
-
-
-    def _iterable_to_row(self, data: Iterable):
-        if self._headers is not None and len(data) != len(self._headers):
-            logger.warning(f"row {self._row_count+1} length: {len(data)} (expected headers length: {len(self._headers)})")
-        
-        return data
-        
-
-    def _dict_to_row(self, data: dict):
-        row = [None] * (len(self._headers) if self._headers else 0)
-
-        for header, value in data.items():
-            index = self._fetch_header(header)
-            while index >= len(row):
-                row.append(None)
-            row[index] = value
-            
-        return row
-
-
-    def _fetch_header(self, header: str) -> int:
-        if not isinstance(header, str):
-            header = str(header)
-
-        if self._headers is None:
-            self._headers = []
-
-        try:
-            index = self._headers.index(header)
-        except:
-            self._headers.append(header)
-            index = len(self._headers) - 1
-
-            if self._actually_started:
-                logger.warning(f"row {self._row_count + 1} header \"{header}\" not found in exported headers: values will be appended at column {len(self._headers)} with an empty header")
-            
-        return index
-
-
-    # -------------------------------------------------------------------------
-    # These methods are available for subclassing. 
-    #
-    def _open_file(self) -> list[str]|None:
-        """
-        For OutTable and its subclass, `_open_table()` must return existing headers.
-
-        Depending on the situation, `_get_existing_headers()` may be called before opening the underlying file (in order to avoid potential file locking)
-        or after opening the underlying file (for example the Excel workbook for OutExcel).
-        """
-        existing_headers = self._get_existing_headers()
-        super()._open_file()
-        return existing_headers
-    
-
-    def _get_existing_headers(self) -> list[str]|None:
-        """
-        Get headers to consider in the existing target.
-
-        Return a list (including empty list) if headers make sense.
-        Return `None` if headers cannot be used at all.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _export_new_headers(self, headers: list[str]):
-        """
-        Export headers when the list of exiting headers is empty.
-        """        
-        for index, header in enumerate(headers):
-            self._add_new_header(header, index)
-
-
-    def _add_new_header(self, header: str, index: int):
-        """
-        Add a new header to an existing list of headers.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _export_prepared_row(self, row: Iterable):
-        """
-        Export a row that has been reordered (if necessary) to match existing headers + newly added headers.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if value is None:
-            return None
-        elif isinstance(value, str):
-            return value
-        elif isinstance(value, Enum):
-            return value.name
-        elif isinstance(value, (dict,list,tuple)):
-            # Display as a list of elements separated by "|"
-            if is_list_or_tuple_of(value, (str,int,float,Decimal,bool,type(None))):
-                str_elements = []
-                for element in value:
-                    if element is None:
-                        str_element = ''
-                    else:
-                        str_element = self._format_value(element, index)
-                        if not isinstance(str_element, str):
-                            str_element = str(str_element)
-                        if '|' in str_element:
-                            return json.dumps(value) # cancel
-                    str_elements.append(str_element)
-                return '|'.join(str_elements)
-            else:
-                return json.dumps(value)
-        else:
-            # NOTE: we don't transform everything in strings, because the underlying output system may take into account the actual type
-            # (example: tabulate will align numeric values differently)
-            return value
-
-
-    def _export_delayed_rows(self):
-        for row in self._delayed_rows:
-            self._export_row(row)
+from __future__ import annotations
+from atexit import register as register_atexit
+from datetime import datetime, timezone
+from decimal import Decimal
+from enum import Enum
+from io import IOBase
+import json
+import logging
+from pathlib import Path
+from types import FunctionType
+from typing import Any, Iterable
+from ..lang import is_list_or_tuple_of
+from .OutFile import OutFile
+
+logger = logging.getLogger(__name__)
+
+
+class OutTable(OutFile):
+    """
+    A mixin for appending and exporting tabular data.
+
+    Must be subclassed (to implement read/write of value).
+    """
+    @classmethod
+    def is_available(cls):
+        return True
+    
+    def __init__(self, out: str|Path|IOBase|None = None, append: bool = False, headers: list[str] = None, tz: timezone = None, **kwargs):
+        if not self.is_available():
+            raise ValueError(f"cannot use {type(self).__name__} (not available)")
+    
+        super().__init__(out, **kwargs)
+        
+        self._append = append
+
+        self._start_atonce = True if headers else False # we consider than if headers are given now, they are final, so we don't need to wait for dict rows
+        """ If true, delay rows until end of export """
+
+        self._actually_started = False
+
+        self._headers: list[str]|None = [str(header) for header in headers] if headers else None        
+        self._row_count = 0
+
+        self._delayed_rows: list[Iterable] = []
+
+        self._reordering: list[int]|None = None
+        self._reordering_default: list|None = None
+
+        self._tz = timezone.utc if tz == 'utc' else tz
+
+
+    # -------------------------------------------------------------------------
+    # Actions on start/end (see OutFile)
+    #
+
+    def __enter__(self) -> OutTable:
+        if self._start_atonce:
+            if self._headers:
+                existing_headers = self._open_file()
+                self._export_headers(existing_headers)
+                self._actually_started = True
+
+        if self.atexit:
+            if callable(self.atexit):
+                self.atexit(self)
+            else:
+                register_atexit(self.end)
+
+        return self
+    
+
+    def end(self):
+        """
+        This method is automatically executed when context ends (__exit__), except if `atexit` is truthy.
+        """
+        if not self._actually_started:        
+            existing_headers = self._open_file()
+            self._export_headers(existing_headers, at_end=True)
+            self._actually_started = True
+
+        self._export_delayed_rows()
+
+        self._delayed_rows = []
+        self._close_file()
+    
+
+    # -------------------------------------------------------------------------
+    # Following methods are the core table export mechanism.
+    # They are not supposed to be subclassed.
+    #
+
+    def _export_headers(self, existing_headers: list[str]|None, at_end=False):
+        if not self._headers:
+            if existing_headers:
+                if not (at_end and not self._delayed_rows):
+                    raise ValueError(f"cannot export table without headers: existing table contains headers ({', '.join(existing_headers)})")
+            return
+        
+        if existing_headers is None:
+            if self._headers:
+                if not (at_end and not self._delayed_rows):
+                    raise ValueError(f"cannot export table with headers: existing table cannot contain headers")
+            return
+        
+        if existing_headers == self._headers:
+            return
+
+        if len(existing_headers) == 0:
+            self._export_new_headers(self._headers)
+            return
+
+        # From now own, we must compare existing and target headers
+        self._reordering = []
+        self._reordering_default = [None] * len(existing_headers)
+
+        for header in self._headers:
+            try:
+                index = existing_headers.index(header)
+                self._reordering.append(index)
+            except ValueError:
+                self._reordering_default.append(None)
+                index = len(self._reordering_default) - 1
+                self._reordering.append(index)
+                self._add_new_header(header, index)
+    
+
+    def append(self, row):
+        if isinstance(row, dict):
+            row = self._dict_to_row(row)        
+        else:
+            row = self._iterable_to_row(row)
+
+        if self._start_atonce:
+            self._export_row(row)
+        else:
+            self._delayed_rows.append(row)
+        
+        self._row_count += 1
+
+
+    def _export_row(self, row: Iterable):
+        if not self._actually_started:
+            existing_headers = self._open_file()
+            self._export_headers(existing_headers)
+            self._actually_started = True
+
+        self._export_prepared_row(self._prepare_row(row))
+
+
+    def _prepare_row(self, data: Iterable):
+        if not self._reordering:
+            row = [self._format_value(value, i) for i, value in enumerate(data)]
+            if self._headers:
+                while len(row) < len(self._headers):
+                    row.append(None)
+            return row
+        
+        else:
+            row = list(self._reordering_default)
+            for i, value in enumerate(data):
+                value = self._format_value(value, i)
+                if i < len(self._reordering):
+                    index = self._reordering[i]
+                    row[index] = value
+                else:
+                    row.append(value)
+
+            return row
+
+
+    def _iterable_to_row(self, data: Iterable):
+        if self._headers is not None and len(data) != len(self._headers):
+            logger.warning(f"row {self._row_count+1} length: {len(data)} (expected headers length: {len(self._headers)})")
+        
+        return data
+        
+
+    def _dict_to_row(self, data: dict):
+        row = [None] * (len(self._headers) if self._headers else 0)
+
+        for header, value in data.items():
+            index = self._fetch_header(header)
+            while index >= len(row):
+                row.append(None)
+            row[index] = value
+            
+        return row
+
+
+    def _fetch_header(self, header: str) -> int:
+        if not isinstance(header, str):
+            header = str(header)
+
+        if self._headers is None:
+            self._headers = []
+
+        try:
+            index = self._headers.index(header)
+        except:
+            self._headers.append(header)
+            index = len(self._headers) - 1
+
+            if self._actually_started:
+                logger.warning(f"row {self._row_count + 1} header \"{header}\" not found in exported headers: values will be appended at column {len(self._headers)} with an empty header")
+            
+        return index
+
+
+    # -------------------------------------------------------------------------
+    # These methods are available for subclassing. 
+    #
+    def _open_file(self) -> list[str]|None:
+        """
+        For OutTable and its subclass, `_open_table()` must return existing headers.
+
+        Depending on the situation, `_get_existing_headers()` may be called before opening the underlying file (in order to avoid potential file locking)
+        or after opening the underlying file (for example the Excel workbook for OutExcel).
+        """
+        existing_headers = self._get_existing_headers()
+        super()._open_file()
+        return existing_headers
+    
+
+    def _get_existing_headers(self) -> list[str]|None:
+        """
+        Get headers to consider in the existing target.
+
+        Return a list (including empty list) if headers make sense.
+        Return `None` if headers cannot be used at all.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _export_new_headers(self, headers: list[str]):
+        """
+        Export headers when the list of exiting headers is empty.
+        """        
+        for index, header in enumerate(headers):
+            self._add_new_header(header, index)
+
+
+    def _add_new_header(self, header: str, index: int):
+        """
+        Add a new header to an existing list of headers.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _export_prepared_row(self, row: Iterable):
+        """
+        Export a row that has been reordered (if necessary) to match existing headers + newly added headers.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if value is None:
+            return None
+        elif isinstance(value, str):
+            return value
+        elif isinstance(value, Enum):
+            return value.name
+        elif isinstance(value, (dict,list,tuple)):
+            # Display as a list of elements separated by "|"
+            if is_list_or_tuple_of(value, (str,int,float,Decimal,bool,type(None))):
+                str_elements = []
+                for element in value:
+                    if element is None:
+                        str_element = ''
+                    else:
+                        str_element = self._format_value(element, index)
+                        if not isinstance(str_element, str):
+                            str_element = str(str_element)
+                        if '|' in str_element:
+                            return json.dumps(value) # cancel
+                    str_elements.append(str_element)
+                return '|'.join(str_elements)
+            else:
+                return json.dumps(value)
+        else:
+            # NOTE: we don't transform everything in strings, because the underlying output system may take into account the actual type
+            # (example: tabulate will align numeric values differently)
+            return value
+
+
+    def _export_delayed_rows(self):
+        for row in self._delayed_rows:
+            self._export_row(row)
```

## zut/inout/OutTabulate.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-from __future__ import annotations
-import logging
-from .OutTable import OutTable
-
-try:
-    from tabulate import tabulate
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class OutTabulate(OutTable):
-    @classmethod
-    def is_available(cls):
-        return _available
-
-    def __init__(self, out = None, **kwargs):
-        super().__init__(out, **kwargs)
-        self._start_atonce = False
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #    
-    def _export_delayed_rows(self):
-        if not self._delayed_rows and not self._headers:
-            print("no data", file=self.file)
-            return
-        
-        rows = [self._prepare_row(row) for row in self._delayed_rows]
-        if self._headers:
-            result = tabulate(rows, headers=self._headers)
-        else:
-            result = tabulate(rows)
-        print(result, file=self.file)
-
-
-    def _get_existing_headers(self) -> list[str]|None:
-        return []
-
-    def _export_new_headers(self, headers: list[str]):
-        pass
+from __future__ import annotations
+import logging
+from .OutTable import OutTable
+
+try:
+    from tabulate import tabulate
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class OutTabulate(OutTable):
+    @classmethod
+    def is_available(cls):
+        return _available
+
+    def __init__(self, out = None, **kwargs):
+        super().__init__(out, **kwargs)
+        self._start_atonce = False
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #    
+    def _export_delayed_rows(self):
+        if not self._delayed_rows and not self._headers:
+            print("no data", file=self.file)
+            return
+        
+        rows = [self._prepare_row(row) for row in self._delayed_rows]
+        if self._headers:
+            result = tabulate(rows, headers=self._headers)
+        else:
+            result = tabulate(rows)
+        print(result, file=self.file)
+
+
+    def _get_existing_headers(self) -> list[str]|None:
+        return []
+
+    def _export_new_headers(self, headers: list[str]):
+        pass
```

## zut/inout/__init__.py

 * *Ordering differences only*

```diff
@@ -1,148 +1,148 @@
-from __future__ import annotations
-from datetime import timezone
-
-import logging
-from io import IOBase
-from pathlib import Path
-import sys
-from typing import Any, Callable
-from ..text import slugify
-from ..db import DbWrapper
-
-from .OutFile import OutFile
-from .OutTable import OutTable
-from .OutTabulate import OutTabulate
-from .OutCsv import OutCsv
-from .OutExcel import OutExcel
-from .OutDb import OutDb
-from .InTable import InTable
-from .InDb import InDb
-from .InCsv import InCsv
-from .InExcel import InExcel
-from .utils import is_excel_path, split_excel_path, normalize_inout
-
-logger = logging.getLogger(__name__)
-
-
-def out_file(out: str|Path|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool|Callable = None, format: OutFile = None, **kwargs) -> OutFile:
-    if not format:
-        format = OutFile
-    elif not isinstance(format, type) or not issubclass(format, OutFile):
-        raise ValueError(f"invalid format: {format}")
-    
-    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, newline=newline, atexit=atexit, **kwargs)
-
-
-def out_table(out: Path|str|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', headers: list[str] = None, delimiter: str = None, quotechar: str = '"', tz: timezone = None, decimal_separator: str = None, atexit: bool|Callable = None, format: OutTable|str = None, **kwargs) -> OutTable:
-    if not format:
-        if isinstance(out, str) and out.startswith('db:'):
-            format = OutDb
-        elif OutTabulate.is_available() and (out is None or out == 'stdout' or out == sys.stdout or out == 'stderr' or out == sys.stderr):
-            format = OutTabulate
-        elif isinstance(out, (str,Path)) and is_excel_path(out, accept_table_suffix=True):
-            format = OutExcel
-        else:
-            format = OutCsv
-    elif format == 'csv':
-        format = OutCsv
-    elif format == 'excel':
-        format = OutExcel
-    elif format == 'tabulate':
-        format = OutTabulate
-    elif not isinstance(format, type) or not issubclass(format, OutTable):
-        raise ValueError(f"invalid format: {format}")
-
-    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, headers=headers, delimiter=delimiter, tz=tz, decimal_separator=decimal_separator, quotechar=quotechar, atexit=atexit, **kwargs)
-
-
-def in_table(src: Path|str|IOBase|DbWrapper, query: str = None, *args, src_dir: str|Path|None = None, title: str|None = None, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', formatters: dict[int|str,Callable] = None, debug: bool = False, limit: int = None, format: InTable|str = None, **kwargs) -> InTable:
-    if not format:
-        if isinstance(src, DbWrapper) or (isinstance(src, str) and src.startswith('db:')):
-            format = InDb
-        elif isinstance(src, (str,Path)) and is_excel_path(src, accept_table_suffix=True):
-            format = InExcel
-        else:
-            format = InCsv
-    elif format == 'csv':
-        format = InCsv
-    elif format == 'excel':
-        format = InExcel
-    elif not isinstance(format, type) or not issubclass(format, InTable):
-        raise ValueError(f"invalid format: {format}")
-    
-    return format(src, query=query, *args, src_dir=src_dir, title=title, encoding=encoding, delimiter=delimiter, quotechar=quotechar, formatters=formatters, debug=debug, limit=limit, **kwargs)
-
-
-def transfer_table(src: Path|str|IOBase, out: str|Path|IOBase, *, formatters: dict[int|str,Callable] = None, headers: list[str]|dict[str, Any] = None, dir: str|Path|None = None, append: bool = False, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):
-    """
-    - `formatters`: formatters to apply to source values. Keys are source headers.
-    - `headers`: target headers, or mapping (`dict`): keys are target headers, values are source headers (`str`) or value (`str` prefixed with `value:` or other types). Use '*' as key and value to include non-mentionned source headers without modifications.
-    """
-    class RowIndex:
-        def __init__(self, index: int):
-            self.index = index
-
-        def __repr__(self):
-            return f"[{self.index}]"
-        
-
-    with in_table(src, src_dir=dir, formatters=formatters, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _src_table:
-        # Read/write headers and build mapping
-        if headers:
-            out_headers = []
-            row_transform_needed = True
-            out_row_model = []
-
-            default_spec = ('*' if '*' in headers else None) if isinstance(headers, list) else headers.pop('*', None)
-            if default_spec:
-                for src_index, src_header in enumerate(_src_table.headers):
-                    if default_spec == 'slugify':
-                        target_header = slugify(src_header)
-                    elif default_spec.startswith('slugify:'):
-                        target_header = slugify(src_header, separator=default_spec[len('slugify:'):])
-                    else:
-                        target_header = src_header
-
-                    out_headers.append(target_header)
-                    out_row_model.append(RowIndex(src_index))
-        
-
-            for target_header, spec in ([(header, header) for header in headers] if isinstance(headers, list) else headers.items()):
-                if isinstance(spec, str):
-                    if spec.startswith('value:'):
-                        spec = spec[len('value:'):]
-                    else:
-                        try:
-                            src_index = _src_table.headers.index(spec)
-                        except ValueError:
-                            raise ValueError(f"header \"{spec}\" (for mapping to \"{target_header}\") not found in CSV file")
-                        spec = RowIndex(src_index)
-                else:
-                    spec = spec
-
-                try:
-                    out_index = out_headers.index(target_header)
-                    out_row_model[out_index] = spec
-                except ValueError:
-                    out_headers.append(target_header)
-                    out_row_model.append(spec)
-        else:
-            out_headers = _src_table.headers
-            row_transform_needed = False
-
-        # Read/write rows
-        with out_table(out, out_dir=dir, headers=out_headers, append=append, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _out_table:
-            for src_row in _src_table:
-                if row_transform_needed:
-                    out_row = []
-
-                    for spec in out_row_model:
-                        if isinstance(spec, RowIndex):
-                            value = src_row[spec.index]
-                        else:
-                            value = spec
-                        out_row.append(value)
-                else:
-                    out_row = src_row.values
-
-                _out_table.append(out_row)
+from __future__ import annotations
+from datetime import timezone
+
+import logging
+from io import IOBase
+from pathlib import Path
+import sys
+from typing import Any, Callable
+from ..text import slugify
+from ..db import DbWrapper
+
+from .OutFile import OutFile
+from .OutTable import OutTable
+from .OutTabulate import OutTabulate
+from .OutCsv import OutCsv
+from .OutExcel import OutExcel
+from .OutDb import OutDb
+from .InTable import InTable
+from .InDb import InDb
+from .InCsv import InCsv
+from .InExcel import InExcel
+from .utils import is_excel_path, split_excel_path, normalize_inout
+
+logger = logging.getLogger(__name__)
+
+
+def out_file(out: str|Path|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool|Callable = None, format: OutFile = None, **kwargs) -> OutFile:
+    if not format:
+        format = OutFile
+    elif not isinstance(format, type) or not issubclass(format, OutFile):
+        raise ValueError(f"invalid format: {format}")
+    
+    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, newline=newline, atexit=atexit, **kwargs)
+
+
+def out_table(out: Path|str|IOBase|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', headers: list[str] = None, delimiter: str = None, quotechar: str = '"', tz: timezone = None, decimal_separator: str = None, atexit: bool|Callable = None, format: OutTable|str = None, **kwargs) -> OutTable:
+    if not format:
+        if isinstance(out, str) and out.startswith('db:'):
+            format = OutDb
+        elif OutTabulate.is_available() and (out is None or out == 'stdout' or out == sys.stdout or out == 'stderr' or out == sys.stderr):
+            format = OutTabulate
+        elif isinstance(out, (str,Path)) and is_excel_path(out, accept_table_suffix=True):
+            format = OutExcel
+        else:
+            format = OutCsv
+    elif format == 'csv':
+        format = OutCsv
+    elif format == 'excel':
+        format = OutExcel
+    elif format == 'tabulate':
+        format = OutTabulate
+    elif not isinstance(format, type) or not issubclass(format, OutTable):
+        raise ValueError(f"invalid format: {format}")
+
+    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, headers=headers, delimiter=delimiter, tz=tz, decimal_separator=decimal_separator, quotechar=quotechar, atexit=atexit, **kwargs)
+
+
+def in_table(src: Path|str|IOBase|DbWrapper, query: str = None, *args, src_dir: str|Path|None = None, title: str|None = None, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', formatters: dict[int|str,Callable] = None, debug: bool = False, limit: int = None, format: InTable|str = None, **kwargs) -> InTable:
+    if not format:
+        if isinstance(src, DbWrapper) or (isinstance(src, str) and src.startswith('db:')):
+            format = InDb
+        elif isinstance(src, (str,Path)) and is_excel_path(src, accept_table_suffix=True):
+            format = InExcel
+        else:
+            format = InCsv
+    elif format == 'csv':
+        format = InCsv
+    elif format == 'excel':
+        format = InExcel
+    elif not isinstance(format, type) or not issubclass(format, InTable):
+        raise ValueError(f"invalid format: {format}")
+    
+    return format(src, query=query, *args, src_dir=src_dir, title=title, encoding=encoding, delimiter=delimiter, quotechar=quotechar, formatters=formatters, debug=debug, limit=limit, **kwargs)
+
+
+def transfer_table(src: Path|str|IOBase, out: str|Path|IOBase, *, formatters: dict[int|str,Callable] = None, headers: list[str]|dict[str, Any] = None, dir: str|Path|None = None, append: bool = False, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):
+    """
+    - `formatters`: formatters to apply to source values. Keys are source headers.
+    - `headers`: target headers, or mapping (`dict`): keys are target headers, values are source headers (`str`) or value (`str` prefixed with `value:` or other types). Use '*' as key and value to include non-mentionned source headers without modifications.
+    """
+    class RowIndex:
+        def __init__(self, index: int):
+            self.index = index
+
+        def __repr__(self):
+            return f"[{self.index}]"
+        
+
+    with in_table(src, src_dir=dir, formatters=formatters, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _src_table:
+        # Read/write headers and build mapping
+        if headers:
+            out_headers = []
+            row_transform_needed = True
+            out_row_model = []
+
+            default_spec = ('*' if '*' in headers else None) if isinstance(headers, list) else headers.pop('*', None)
+            if default_spec:
+                for src_index, src_header in enumerate(_src_table.headers):
+                    if default_spec == 'slugify':
+                        target_header = slugify(src_header)
+                    elif default_spec.startswith('slugify:'):
+                        target_header = slugify(src_header, separator=default_spec[len('slugify:'):])
+                    else:
+                        target_header = src_header
+
+                    out_headers.append(target_header)
+                    out_row_model.append(RowIndex(src_index))
+        
+
+            for target_header, spec in ([(header, header) for header in headers] if isinstance(headers, list) else headers.items()):
+                if isinstance(spec, str):
+                    if spec.startswith('value:'):
+                        spec = spec[len('value:'):]
+                    else:
+                        try:
+                            src_index = _src_table.headers.index(spec)
+                        except ValueError:
+                            raise ValueError(f"header \"{spec}\" (for mapping to \"{target_header}\") not found in CSV file")
+                        spec = RowIndex(src_index)
+                else:
+                    spec = spec
+
+                try:
+                    out_index = out_headers.index(target_header)
+                    out_row_model[out_index] = spec
+                except ValueError:
+                    out_headers.append(target_header)
+                    out_row_model.append(spec)
+        else:
+            out_headers = _src_table.headers
+            row_transform_needed = False
+
+        # Read/write rows
+        with out_table(out, out_dir=dir, headers=out_headers, append=append, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _out_table:
+            for src_row in _src_table:
+                if row_transform_needed:
+                    out_row = []
+
+                    for spec in out_row_model:
+                        if isinstance(spec, RowIndex):
+                            value = src_row[spec.index]
+                        else:
+                            value = spec
+                        out_row.append(value)
+                else:
+                    out_row = src_row.values
+
+                _out_table.append(out_row)
```

## zut/inout/utils.py

 * *Ordering differences only*

```diff
@@ -1,88 +1,88 @@
-from __future__ import annotations
-
-import os
-import re
-import sys
-from io import IOBase
-from pathlib import Path
-
-
-def normalize_inout(inout: str|Path|IOBase|None, *, dir: str|Path|None, **kwargs) -> str|IOBase:
-    if inout == 'stdout' or inout == sys.stdout or inout is None:
-        return sys.stdout
-    elif inout == 'stderr' or inout == sys.stderr:
-        return sys.stderr
-    elif inout == 'stdin' or inout == sys.stdin:
-        return sys.stdin
-    elif inout == False:
-        return os.devnull
-    elif isinstance(inout, IOBase):
-        return inout
-    elif isinstance(inout, (str,Path)):
-        inout = str(inout).format(**kwargs)
-        if dir and not ':' in inout and not inout.startswith('.'):
-            inout = os.path.join(str(dir), inout)
-        return os.path.expanduser(inout)
-    else:
-        raise ValueError(f'invalid inout type: {type(inout)}')
-
-
-def get_inout_name(inout: str|Path|IOBase|None) -> str:
-    if inout == 'stdout' or inout == sys.stdout or inout is None:
-        return '<stdout>'
-    elif inout == 'stderr' or inout == sys.stderr:
-        return '<stderr>'
-    elif inout == 'stdin' or inout == sys.stdin:
-        return '<stdin>'
-    elif inout == False:
-        return '<devnull>'
-    elif isinstance(inout, IOBase):
-        return get_iobase_name(inout)
-    elif isinstance(inout, (str,Path)):
-        return inout
-    else:
-        raise ValueError(f'invalid inout type: {type(inout)}')
-
-
-def get_iobase_name(out: IOBase) -> str:
-    try:
-        name = out.name
-        if not name or not isinstance(name, str):
-            name = None
-    except AttributeError:
-        name = None
-
-    if name:
-        if name.startswith('<') and name.endswith('>'):
-            return name
-        else:
-            return f'<{name}>'
-
-    else:
-        return f"<{type(out).__name__}>"
-
-
-
-def is_excel_path(path: str|Path, accept_table_suffix = False):
-    if isinstance(path, Path):
-        path = str(path)
-    elif not isinstance(path, str):
-        raise ValueError(f'invalid path type: {type(path)}')
-    
-    return re.search(r'\.xlsx(?:#[^\.]+)?$' if accept_table_suffix else r'\.xlsx$', path, re.IGNORECASE)
-
-
-def split_excel_path(path: str|Path, default_table_name: str = None, **kwargs) -> tuple[Path,str]:
-    """ Return (actual path, table name) """
-    if isinstance(path, Path):
-        path = str(path)
-    elif not isinstance(path, str):
-        raise ValueError(f'invalid path type: {type(path)}')
-        
-    path = path.format(**kwargs)
-
-    m = re.match(r'^(.+\.xlsx)(?:#([^\.]*))?$', path, re.IGNORECASE)
-    if not m:
-        return (Path(path), default_table_name)
-    
-    return (Path(m[1]), m[2] if m[2] else default_table_name)
+from __future__ import annotations
+
+import os
+import re
+import sys
+from io import IOBase
+from pathlib import Path
+
+
+def normalize_inout(inout: str|Path|IOBase|None, *, dir: str|Path|None, **kwargs) -> str|IOBase:
+    if inout == 'stdout' or inout == sys.stdout or inout is None:
+        return sys.stdout
+    elif inout == 'stderr' or inout == sys.stderr:
+        return sys.stderr
+    elif inout == 'stdin' or inout == sys.stdin:
+        return sys.stdin
+    elif inout == False:
+        return os.devnull
+    elif isinstance(inout, IOBase):
+        return inout
+    elif isinstance(inout, (str,Path)):
+        inout = str(inout).format(**kwargs)
+        if dir and not ':' in inout and not inout.startswith('.'):
+            inout = os.path.join(str(dir), inout)
+        return os.path.expanduser(inout)
+    else:
+        raise ValueError(f'invalid inout type: {type(inout)}')
+
+
+def get_inout_name(inout: str|Path|IOBase|None) -> str:
+    if inout == 'stdout' or inout == sys.stdout or inout is None:
+        return '<stdout>'
+    elif inout == 'stderr' or inout == sys.stderr:
+        return '<stderr>'
+    elif inout == 'stdin' or inout == sys.stdin:
+        return '<stdin>'
+    elif inout == False:
+        return '<devnull>'
+    elif isinstance(inout, IOBase):
+        return get_iobase_name(inout)
+    elif isinstance(inout, (str,Path)):
+        return inout
+    else:
+        raise ValueError(f'invalid inout type: {type(inout)}')
+
+
+def get_iobase_name(out: IOBase) -> str:
+    try:
+        name = out.name
+        if not name or not isinstance(name, str):
+            name = None
+    except AttributeError:
+        name = None
+
+    if name:
+        if name.startswith('<') and name.endswith('>'):
+            return name
+        else:
+            return f'<{name}>'
+
+    else:
+        return f"<{type(out).__name__}>"
+
+
+
+def is_excel_path(path: str|Path, accept_table_suffix = False):
+    if isinstance(path, Path):
+        path = str(path)
+    elif not isinstance(path, str):
+        raise ValueError(f'invalid path type: {type(path)}')
+    
+    return re.search(r'\.xlsx(?:#[^\.]+)?$' if accept_table_suffix else r'\.xlsx$', path, re.IGNORECASE)
+
+
+def split_excel_path(path: str|Path, default_table_name: str = None, **kwargs) -> tuple[Path,str]:
+    """ Return (actual path, table name) """
+    if isinstance(path, Path):
+        path = str(path)
+    elif not isinstance(path, str):
+        raise ValueError(f'invalid path type: {type(path)}')
+        
+    path = path.format(**kwargs)
+
+    m = re.match(r'^(.+\.xlsx)(?:#([^\.]*))?$', path, re.IGNORECASE)
+    if not m:
+        return (Path(path), default_table_name)
+    
+    return (Path(m[1]), m[2] if m[2] else default_table_name)
```

## zut/network/__init__.py

 * *Ordering differences only*

```diff
@@ -1,241 +1,241 @@
-from __future__ import annotations
-import logging, os, re, urllib.request, socket, base64
-from types import FunctionType
-from urllib.parse import unquote, urlparse
-from contextlib import closing
-from unittest import TestCase
-from ..colors import Colors
-from .commons import ProxyConfig, _proxyconfig, report_failure
-
-try:
-    from .winhttp import check_winhttp_connectivity
-    _with_winhttp = True
-except ImportError:
-    _with_winhttp = False
-
-try:
-    from ..credentials import get_password
-    _with_credentials = True
-except ImportError:
-    _with_credentials = False
-
-
-logger = logging.getLogger(__name__)
-
-IP_ADDRESS_PATTERN = re.compile(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
-
-
-def check_socket(host, port, timeout=1):
-    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
-        sock.settimeout(timeout)
-        try:
-            returncode = sock.connect_ex((host, port))
-            if returncode == 0:
-                return True
-            else:
-                logger.debug("socket connnect_ex returned %d", returncode)
-                return False
-        except Exception as e:
-            logger.debug("socket connnect_ex: %s", e)
-            return False
-
-
-class SimpleProxyHandler(urllib.request.BaseHandler):
-    # Inspired from urllib.request.ProxyHandler   
-    handler_order = 100 # Proxies must be in front
-
-    def __init__(self, config: ProxyConfig):
-        self.config = config
-
-    def finalize(self) -> str:
-        # Get proxy url (for logs)
-        proxy_logurl = self.config.get_proxy_url(include_password="*")
-        logger.debug(f"use proxy for urllib: {proxy_logurl}")
-
-        # Check proxy existency
-        if not check_socket(self.config.host, self.config.port):
-            logger.warning(f"cannot connect to proxy for urllib: {proxy_logurl}")
-
-        # Prepare usefull variables for add_header and set_proxy
-        self.hostport = self.config.hostport
-        self.scheme = self.config.scheme
-
-        if self.config.username:
-            if not self.config.password:
-                if self.config.password_func:
-                    raise ValueError(f"password function did not return any value for username: {self.config.username}")
-                else:
-                    raise ValueError(f"missing password for urllib proxy: {proxy_logurl}")
-            userpass = '%s:%s' % (self.config.username, self.config.password)
-            self.authorization = "Basic " + base64.b64encode(userpass.encode()).decode("ascii")
-        else:
-            self.authorization = None
-
-    def http_open(self, req):
-        if not req.host:
-            return None
-        
-        if urllib.request.proxy_bypass(req.host):
-            # NOTE: because Proxy-Authorization header is not encrypted, we must add it ONLY when we're actually talking to the proxy
-            return None
-
-        if not hasattr(self, "authorization"):
-            self.finalize()
-
-        if self.authorization:
-            req.add_header("Proxy-Authorization", self.authorization)
-        req.set_proxy(self.hostport, self.scheme)
-        return None
-    
-    def https_open(self, req):
-        return self.http_open(req)
-
-
-def configure_proxy(url: str = None, exclusions: str = None, password_func: FunctionType = None):
-    """
-    Configure proxy for urllib requests (and winhttp requests created with `zut.network.winhttp.create_winhttp_request`).
-    """
-    # Detect proxy URL
-    if url is None:
-        if "HTTP_PROXY" in os.environ:
-            url = os.environ["HTTP_PROXY"]
-        elif "http_proxy" in os.environ:
-            url = os.environ["http_proxy"]
-        elif "HTTPS_PROXY" in os.environ:
-            url = os.environ["HTTPS_PROXY"]
-        elif "https_proxy" in os.environ:
-            url = os.environ["https_proxy"]
-
-    # Detect proxy exclusions
-    if exclusions is None:
-        if "NO_PROXY" in os.environ:
-            exclusions = os.environ["NO_PROXY"]
-        elif "no_proxy" in os.environ:
-            exclusions = os.environ["no_proxy"]
-
-    # Update proxy configuration
-    _proxyconfig.update(url, exclusions=exclusions)
-    
-    # Detect and register proxy func
-    if _proxyconfig.username and not _proxyconfig._password:
-        if password_func is None:
-            service = os.environ.get("PROXY_PASSWORD_SERVICE", None)
-            if service:
-                if not _with_credentials:
-                    logger.error(f"cannot register PROXY_PASSWORD_SERVICE \"{service}\": zut[credentials] optional dependencies are not installed")
-                else:
-                    password_func = lambda username: get_password(service, username)
-                
-        if password_func:
-            _proxyconfig.set_password_func(password_func)
-
-    def _del_if_exists(data: dict, attr: str):
-        if attr in data:
-            del data[attr]
-
-    # Remove environment variables (would take precedence in some cases, which might cause issues: e.g. should contain password or not?)
-    _del_if_exists(os.environ, "http_proxy")
-    _del_if_exists(os.environ, "https_proxy")
-    _del_if_exists(os.environ, "HTTP_PROXY")
-    _del_if_exists(os.environ, "HTTPS_PROXY")
-
-    # Ensure no_proxy is specified (must be passed as environment variable for urllib)
-    env_exclusions = _proxyconfig.env_exclusions_str
-    if env_exclusions:
-        os.environ["no_proxy"] = env_exclusions
-        os.environ["NO_PROXY"] = env_exclusions
-    else:
-        _del_if_exists(os.environ, "no_proxy")
-        _del_if_exists(os.environ, "NO_PROXY")
-
-    # Stop if no proxy specified/detected
-    if not _proxyconfig.host:
-        return
-    
-    # Register proxy for urllib
-    try:
-        handler = SimpleProxyHandler(_proxyconfig)
-        opener = urllib.request.build_opener(handler)
-        urllib.request.install_opener(opener)
-    except Exception as e:
-        logger.error("cannot register proxy for urllib: %s", e)
-
-
-def get_configured_proxy_url(for_url: str = None, include_password: bool = False) -> str:
-    """
-    Return currently configured proxy URL.
-    If `for_url` is given, return None if the proxy is excluded for this url.
-    """
-    if for_url:
-        o = urlparse(for_url)
-        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
-            return None
-    return _proxyconfig.get_proxy_url(include_password=include_password)
-
-
-def get_configured_proxy_hostport(for_url: str = None, include_password: bool = False) -> tuple[str,int]:
-    """
-    Return currently configured proxy host and port.
-    If `for_url` is given, return None if the proxy is excluded for this url.
-    """
-    if for_url:
-        o = urlparse(for_url)
-        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
-            return None
-    return _proxyconfig.host, _proxyconfig.port
-
-
-def get_configured_proxy_exclusions() -> list[str]:
-    """
-    Return currently configured proxy exclusions.
-    """
-    return _proxyconfig.exclusions
-
-
-def check_urllib_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity using urllib library.
-    - `timeout`: in seconds (defaults to 3 seconds).
-
-    Return `True` on success, `False` on failure.
-    """        
-    if not isinstance(expected_regex, re.Pattern):
-        expected_regex = re.compile(expected_regex)
-
-    if label is None:
-        label = url
-
-    if not timeout:
-        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
-
-    msg=f"{label} urllib"
-
-    try:
-        res = urllib.request.urlopen(url, timeout=timeout)
-        text = res.read().decode('utf-8')
-        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
-        
-        if not expected_regex.match(text):
-            report_failure(case, f"{msg}: response ({Colors.RED}{text_startup}{Colors.RESET}) does not match expected regex ({Colors.GRAY}{expected_regex.pattern}{Colors.RESET}")
-            return False
-
-        logger.info(f"{msg}: {Colors.GREEN}success{Colors.RESET} (response: {Colors.GRAY}{text_startup}{Colors.RESET})")
-        return True
-    except urllib.error.URLError as e:
-        report_failure(case, f"{msg}: {Colors.RED}{e.reason}{Colors.RESET}")
-        return False
-
-
-def check_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity with urllib library, and winhttp library on Windows (if [winhttp] extra dependencies are installed).
-    - `timeout`: in seconds (defaults to 3 seconds).
-
-    Return `True` on success, `False` on failure.
-    """ 
-    ok = check_urllib_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
-
-    if _with_winhttp:
-        ok = ok and check_winhttp_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
-
-    return ok
+from __future__ import annotations
+import logging, os, re, urllib.request, socket, base64
+from types import FunctionType
+from urllib.parse import unquote, urlparse
+from contextlib import closing
+from unittest import TestCase
+from ..colors import Colors
+from .commons import ProxyConfig, _proxyconfig, report_failure
+
+try:
+    from .winhttp import check_winhttp_connectivity
+    _with_winhttp = True
+except ImportError:
+    _with_winhttp = False
+
+try:
+    from ..credentials import get_password
+    _with_credentials = True
+except ImportError:
+    _with_credentials = False
+
+
+logger = logging.getLogger(__name__)
+
+IP_ADDRESS_PATTERN = re.compile(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
+
+
+def check_socket(host, port, timeout=1):
+    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
+        sock.settimeout(timeout)
+        try:
+            returncode = sock.connect_ex((host, port))
+            if returncode == 0:
+                return True
+            else:
+                logger.debug("socket connnect_ex returned %d", returncode)
+                return False
+        except Exception as e:
+            logger.debug("socket connnect_ex: %s", e)
+            return False
+
+
+class SimpleProxyHandler(urllib.request.BaseHandler):
+    # Inspired from urllib.request.ProxyHandler   
+    handler_order = 100 # Proxies must be in front
+
+    def __init__(self, config: ProxyConfig):
+        self.config = config
+
+    def finalize(self) -> str:
+        # Get proxy url (for logs)
+        proxy_logurl = self.config.get_proxy_url(include_password="*")
+        logger.debug(f"use proxy for urllib: {proxy_logurl}")
+
+        # Check proxy existency
+        if not check_socket(self.config.host, self.config.port):
+            logger.warning(f"cannot connect to proxy for urllib: {proxy_logurl}")
+
+        # Prepare usefull variables for add_header and set_proxy
+        self.hostport = self.config.hostport
+        self.scheme = self.config.scheme
+
+        if self.config.username:
+            if not self.config.password:
+                if self.config.password_func:
+                    raise ValueError(f"password function did not return any value for username: {self.config.username}")
+                else:
+                    raise ValueError(f"missing password for urllib proxy: {proxy_logurl}")
+            userpass = '%s:%s' % (self.config.username, self.config.password)
+            self.authorization = "Basic " + base64.b64encode(userpass.encode()).decode("ascii")
+        else:
+            self.authorization = None
+
+    def http_open(self, req):
+        if not req.host:
+            return None
+        
+        if urllib.request.proxy_bypass(req.host):
+            # NOTE: because Proxy-Authorization header is not encrypted, we must add it ONLY when we're actually talking to the proxy
+            return None
+
+        if not hasattr(self, "authorization"):
+            self.finalize()
+
+        if self.authorization:
+            req.add_header("Proxy-Authorization", self.authorization)
+        req.set_proxy(self.hostport, self.scheme)
+        return None
+    
+    def https_open(self, req):
+        return self.http_open(req)
+
+
+def configure_proxy(url: str = None, exclusions: str = None, password_func: FunctionType = None):
+    """
+    Configure proxy for urllib requests (and winhttp requests created with `zut.network.winhttp.create_winhttp_request`).
+    """
+    # Detect proxy URL
+    if url is None:
+        if "HTTP_PROXY" in os.environ:
+            url = os.environ["HTTP_PROXY"]
+        elif "http_proxy" in os.environ:
+            url = os.environ["http_proxy"]
+        elif "HTTPS_PROXY" in os.environ:
+            url = os.environ["HTTPS_PROXY"]
+        elif "https_proxy" in os.environ:
+            url = os.environ["https_proxy"]
+
+    # Detect proxy exclusions
+    if exclusions is None:
+        if "NO_PROXY" in os.environ:
+            exclusions = os.environ["NO_PROXY"]
+        elif "no_proxy" in os.environ:
+            exclusions = os.environ["no_proxy"]
+
+    # Update proxy configuration
+    _proxyconfig.update(url, exclusions=exclusions)
+    
+    # Detect and register proxy func
+    if _proxyconfig.username and not _proxyconfig._password:
+        if password_func is None:
+            service = os.environ.get("PROXY_PASSWORD_SERVICE", None)
+            if service:
+                if not _with_credentials:
+                    logger.error(f"cannot register PROXY_PASSWORD_SERVICE \"{service}\": zut[credentials] optional dependencies are not installed")
+                else:
+                    password_func = lambda username: get_password(service, username)
+                
+        if password_func:
+            _proxyconfig.set_password_func(password_func)
+
+    def _del_if_exists(data: dict, attr: str):
+        if attr in data:
+            del data[attr]
+
+    # Remove environment variables (would take precedence in some cases, which might cause issues: e.g. should contain password or not?)
+    _del_if_exists(os.environ, "http_proxy")
+    _del_if_exists(os.environ, "https_proxy")
+    _del_if_exists(os.environ, "HTTP_PROXY")
+    _del_if_exists(os.environ, "HTTPS_PROXY")
+
+    # Ensure no_proxy is specified (must be passed as environment variable for urllib)
+    env_exclusions = _proxyconfig.env_exclusions_str
+    if env_exclusions:
+        os.environ["no_proxy"] = env_exclusions
+        os.environ["NO_PROXY"] = env_exclusions
+    else:
+        _del_if_exists(os.environ, "no_proxy")
+        _del_if_exists(os.environ, "NO_PROXY")
+
+    # Stop if no proxy specified/detected
+    if not _proxyconfig.host:
+        return
+    
+    # Register proxy for urllib
+    try:
+        handler = SimpleProxyHandler(_proxyconfig)
+        opener = urllib.request.build_opener(handler)
+        urllib.request.install_opener(opener)
+    except Exception as e:
+        logger.error("cannot register proxy for urllib: %s", e)
+
+
+def get_configured_proxy_url(for_url: str = None, include_password: bool = False) -> str:
+    """
+    Return currently configured proxy URL.
+    If `for_url` is given, return None if the proxy is excluded for this url.
+    """
+    if for_url:
+        o = urlparse(for_url)
+        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
+            return None
+    return _proxyconfig.get_proxy_url(include_password=include_password)
+
+
+def get_configured_proxy_hostport(for_url: str = None, include_password: bool = False) -> tuple[str,int]:
+    """
+    Return currently configured proxy host and port.
+    If `for_url` is given, return None if the proxy is excluded for this url.
+    """
+    if for_url:
+        o = urlparse(for_url)
+        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
+            return None
+    return _proxyconfig.host, _proxyconfig.port
+
+
+def get_configured_proxy_exclusions() -> list[str]:
+    """
+    Return currently configured proxy exclusions.
+    """
+    return _proxyconfig.exclusions
+
+
+def check_urllib_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity using urllib library.
+    - `timeout`: in seconds (defaults to 3 seconds).
+
+    Return `True` on success, `False` on failure.
+    """        
+    if not isinstance(expected_regex, re.Pattern):
+        expected_regex = re.compile(expected_regex)
+
+    if label is None:
+        label = url
+
+    if not timeout:
+        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
+
+    msg=f"{label} urllib"
+
+    try:
+        res = urllib.request.urlopen(url, timeout=timeout)
+        text = res.read().decode('utf-8')
+        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
+        
+        if not expected_regex.match(text):
+            report_failure(case, f"{msg}: response ({Colors.RED}{text_startup}{Colors.RESET}) does not match expected regex ({Colors.GRAY}{expected_regex.pattern}{Colors.RESET}")
+            return False
+
+        logger.info(f"{msg}: {Colors.GREEN}success{Colors.RESET} (response: {Colors.GRAY}{text_startup}{Colors.RESET})")
+        return True
+    except urllib.error.URLError as e:
+        report_failure(case, f"{msg}: {Colors.RED}{e.reason}{Colors.RESET}")
+        return False
+
+
+def check_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity with urllib library, and winhttp library on Windows (if [winhttp] extra dependencies are installed).
+    - `timeout`: in seconds (defaults to 3 seconds).
+
+    Return `True` on success, `False` on failure.
+    """ 
+    ok = check_urllib_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
+
+    if _with_winhttp:
+        ok = ok and check_winhttp_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
+
+    return ok
```

## zut/network/commons.py

 * *Ordering differences only*

```diff
@@ -1,121 +1,121 @@
-from __future__ import annotations
-import re, logging
-from types import FunctionType
-from urllib.parse import quote, unquote, urlparse
-from unittest import TestCase
-
-logger = logging.getLogger(__name__)
-
-
-def report_failure(case: TestCase, msg: str):
-    if case:
-        case.fail(msg)
-    else:
-        logger.error(msg)
-
-
-class ProxyConfig:
-    def __init__(self):
-        self.clear()
-
-    def clear(self):
-        self.host: str = None
-        self.port: int = None
-        self.username: str = None
-        self.password_func: FunctionType = None
-        self.scheme: str = "http"
-        self.exclusions: list[str] = None
-
-        # caches
-        self._non_cidr_exclusions_str = '__undefined__'
-        self._password: str = None
-
-    def update(self, url: str, exclusions: str|list[str] = None):
-        self.clear()
-        if url is None:
-            return
-
-        o = urlparse(url)
-        m = re.match(r"^(?:(?P<username>[^\:]+)(?:\:(?P<password>[^\:]+))?@)?(?P<host>[^@\:]+)\:(?P<port>\d+)$", o.netloc)
-        if not m:
-            raise ValueError("invalid url netloc \"%s\"" % o.netloc)
-
-        self.host = unquote(m.group("host")) if m.group("host") else None
-        self.port = int(m.group("port"))
-        self.username = unquote(m.group("username")) if m.group("username") else None
-        self._password = unquote(m.group("password")) if m.group("password") else None
-        self.scheme = o.scheme
-
-        if exclusions is not None:
-            if isinstance(exclusions, str):
-                self.exclusions = exclusions.split(',')
-            else:
-                self.exclusions = exclusions
-
-    def set_password_func(self, func: FunctionType):
-        self.password_func = func
-
-    @property
-    def is_rtm_host(self):
-        return self.host.endswith(('.rtm.fr','.rtm.lan'))
-
-    @property
-    def hostport(self):
-        if not self.host:
-            return None
-        return f"{self.host}:{self.port}"
-
-    @property
-    def env_exclusions_str(self) -> str:
-        """
-        Return a comma-separated string of exclusions, for use as an environment variable.
-        """
-        if not self.exclusions:
-            return None
-        return ",".join(self.exclusions)
-
-    @property
-    def non_cidr_exclusions_str(self) -> str:
-        """
-        Return a comma-separated string of exclusions, excluding CIDR blocks (not supported by WinHTTP).
-        """
-        if self._non_cidr_exclusions_str == '__undefined__':
-            if self.exclusions:
-                self._non_cidr_exclusions_str = ",".join([exclusion for exclusion in self.exclusions if not "/" in exclusion])
-            else:
-                logger.warning("no exclusions for proxy: missing NO_PROXY environment variable?")
-                self._non_cidr_exclusions_str = "localhost"
-
-        return self._non_cidr_exclusions_str
-
-    @property
-    def password(self):
-        if self._password is None:
-            if self.username and self.password_func:
-                self._password = self.password_func(self.username)
-
-        return self._password
-
-    def get_proxy_url(self, include_password=False):
-        if self.host is None:
-            return None
-        
-        proxy_url = self.scheme + "://"
-
-        if self.username:
-            proxy_url += quote(self.username)
-
-            if include_password:
-                if self.password:
-                    if include_password == "*":
-                        proxy_url += ":" + ("*" * len(self.password))
-                    else:
-                        proxy_url += ":" + quote(self.password)
-            
-            proxy_url += "@"
-
-        proxy_url += f"{quote(self.host)}:{self.port}"
-        return proxy_url
-
-
-_proxyconfig = ProxyConfig()
+from __future__ import annotations
+import re, logging
+from types import FunctionType
+from urllib.parse import quote, unquote, urlparse
+from unittest import TestCase
+
+logger = logging.getLogger(__name__)
+
+
+def report_failure(case: TestCase, msg: str):
+    if case:
+        case.fail(msg)
+    else:
+        logger.error(msg)
+
+
+class ProxyConfig:
+    def __init__(self):
+        self.clear()
+
+    def clear(self):
+        self.host: str = None
+        self.port: int = None
+        self.username: str = None
+        self.password_func: FunctionType = None
+        self.scheme: str = "http"
+        self.exclusions: list[str] = None
+
+        # caches
+        self._non_cidr_exclusions_str = '__undefined__'
+        self._password: str = None
+
+    def update(self, url: str, exclusions: str|list[str] = None):
+        self.clear()
+        if url is None:
+            return
+
+        o = urlparse(url)
+        m = re.match(r"^(?:(?P<username>[^\:]+)(?:\:(?P<password>[^\:]+))?@)?(?P<host>[^@\:]+)\:(?P<port>\d+)$", o.netloc)
+        if not m:
+            raise ValueError("invalid url netloc \"%s\"" % o.netloc)
+
+        self.host = unquote(m.group("host")) if m.group("host") else None
+        self.port = int(m.group("port"))
+        self.username = unquote(m.group("username")) if m.group("username") else None
+        self._password = unquote(m.group("password")) if m.group("password") else None
+        self.scheme = o.scheme
+
+        if exclusions is not None:
+            if isinstance(exclusions, str):
+                self.exclusions = exclusions.split(',')
+            else:
+                self.exclusions = exclusions
+
+    def set_password_func(self, func: FunctionType):
+        self.password_func = func
+
+    @property
+    def is_rtm_host(self):
+        return self.host.endswith(('.rtm.fr','.rtm.lan'))
+
+    @property
+    def hostport(self):
+        if not self.host:
+            return None
+        return f"{self.host}:{self.port}"
+
+    @property
+    def env_exclusions_str(self) -> str:
+        """
+        Return a comma-separated string of exclusions, for use as an environment variable.
+        """
+        if not self.exclusions:
+            return None
+        return ",".join(self.exclusions)
+
+    @property
+    def non_cidr_exclusions_str(self) -> str:
+        """
+        Return a comma-separated string of exclusions, excluding CIDR blocks (not supported by WinHTTP).
+        """
+        if self._non_cidr_exclusions_str == '__undefined__':
+            if self.exclusions:
+                self._non_cidr_exclusions_str = ",".join([exclusion for exclusion in self.exclusions if not "/" in exclusion])
+            else:
+                logger.warning("no exclusions for proxy: missing NO_PROXY environment variable?")
+                self._non_cidr_exclusions_str = "localhost"
+
+        return self._non_cidr_exclusions_str
+
+    @property
+    def password(self):
+        if self._password is None:
+            if self.username and self.password_func:
+                self._password = self.password_func(self.username)
+
+        return self._password
+
+    def get_proxy_url(self, include_password=False):
+        if self.host is None:
+            return None
+        
+        proxy_url = self.scheme + "://"
+
+        if self.username:
+            proxy_url += quote(self.username)
+
+            if include_password:
+                if self.password:
+                    if include_password == "*":
+                        proxy_url += ":" + ("*" * len(self.password))
+                    else:
+                        proxy_url += ":" + quote(self.password)
+            
+            proxy_url += "@"
+
+        proxy_url += f"{quote(self.host)}:{self.port}"
+        return proxy_url
+
+
+_proxyconfig = ProxyConfig()
```

## zut/network/winhttp.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-from __future__ import annotations
-from unittest import TestCase
-import re, os, logging, win32com.client, win32inetcon, pywintypes
-from ..format import RED, GREEN, GRAY
-from .commons import _proxyconfig, report_failure
-
-logger = logging.getLogger(__name__)
-
-
-def create_winhttp_request(timeout: float = None):
-    """
-    Create a winhttp request.
-    - `timeout`: in seconds.
-    """
-    winhttp_req = win32com.client.Dispatch('WinHTTP.WinHTTPRequest.5.1')
-
-    if timeout:
-        winhttp_req.SetTimeouts(int(timeout*1000), int(timeout*1000), int(timeout*1000), int(timeout*1000))
-
-    if _proxyconfig.hostport:
-        # See: https://docs.microsoft.com/en-us/windows/win32/winhttp/iwinhttprequest-setproxy
-        # NOTE: no need to pass credentials, this is handled directly by Windows
-        HTTPREQUEST_PROXYSETTING_DEFAULT   = 0
-        HTTPREQUEST_PROXYSETTING_PRECONFIG = 0
-        HTTPREQUEST_PROXYSETTING_DIRECT    = 1
-        HTTPREQUEST_PROXYSETTING_PROXY     = 2
-        winhttp_req.SetProxy(HTTPREQUEST_PROXYSETTING_PROXY, _proxyconfig.hostport, _proxyconfig.non_cidr_exclusions_str)
-        winhttp_req.SetAutoLogonPolicy(HTTPREQUEST_PROXYSETTING_DEFAULT)
-
-    return winhttp_req
-
-
-def check_winhttp_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity using winhttp library.
-    - `timeout`: in seconds (defaults to 3 seconds).
-    
-    Return `True` on success, `False` on failure.
-    """        
-    if not isinstance(expected_regex, re.Pattern):
-        expected_regex = re.compile(expected_regex)
-
-    if label is None:
-        label = url
-
-    if not timeout:
-        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
-
-    msg=f"{label} winhttp"
-
-    winhttp_req = create_winhttp_request(timeout=timeout)
-    winhttp_req.Open('GET', url, False)
-    try:
-        winhttp_req.Send()
-        winhttp_req.WaitForResponse()
-        if winhttp_req.Status != 200:
-            report_failure(case, f"{msg}: {winhttp_req.Status} {winhttp_req.StatusText}")
-            return False
-
-        text = winhttp_req.ResponseText
-        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
-
-        if not expected_regex.match(text):
-            report_failure(case, f"{msg}: response ({RED % text_startup}) does not match expected regex ({GRAY % expected_regex.pattern})")
-            return False
-        
-        logger.info(f"{msg}: {GREEN % 'success'} (response: {GRAY % text_startup})")
-        return True
-    except pywintypes.com_error as e:
-        if e.excepinfo[5] + 2**32 & 0xffff == win32inetcon.ERROR_INTERNET_TIMEOUT:
-            details = "timed out"
-        else:
-            details = e.excepinfo[2].strip()
-        report_failure(case, f"{msg}: {RED % details}")
-        return False
+from __future__ import annotations
+from unittest import TestCase
+import re, os, logging, win32com.client, win32inetcon, pywintypes
+from ..format import RED, GREEN, GRAY
+from .commons import _proxyconfig, report_failure
+
+logger = logging.getLogger(__name__)
+
+
+def create_winhttp_request(timeout: float = None):
+    """
+    Create a winhttp request.
+    - `timeout`: in seconds.
+    """
+    winhttp_req = win32com.client.Dispatch('WinHTTP.WinHTTPRequest.5.1')
+
+    if timeout:
+        winhttp_req.SetTimeouts(int(timeout*1000), int(timeout*1000), int(timeout*1000), int(timeout*1000))
+
+    if _proxyconfig.hostport:
+        # See: https://docs.microsoft.com/en-us/windows/win32/winhttp/iwinhttprequest-setproxy
+        # NOTE: no need to pass credentials, this is handled directly by Windows
+        HTTPREQUEST_PROXYSETTING_DEFAULT   = 0
+        HTTPREQUEST_PROXYSETTING_PRECONFIG = 0
+        HTTPREQUEST_PROXYSETTING_DIRECT    = 1
+        HTTPREQUEST_PROXYSETTING_PROXY     = 2
+        winhttp_req.SetProxy(HTTPREQUEST_PROXYSETTING_PROXY, _proxyconfig.hostport, _proxyconfig.non_cidr_exclusions_str)
+        winhttp_req.SetAutoLogonPolicy(HTTPREQUEST_PROXYSETTING_DEFAULT)
+
+    return winhttp_req
+
+
+def check_winhttp_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity using winhttp library.
+    - `timeout`: in seconds (defaults to 3 seconds).
+    
+    Return `True` on success, `False` on failure.
+    """        
+    if not isinstance(expected_regex, re.Pattern):
+        expected_regex = re.compile(expected_regex)
+
+    if label is None:
+        label = url
+
+    if not timeout:
+        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
+
+    msg=f"{label} winhttp"
+
+    winhttp_req = create_winhttp_request(timeout=timeout)
+    winhttp_req.Open('GET', url, False)
+    try:
+        winhttp_req.Send()
+        winhttp_req.WaitForResponse()
+        if winhttp_req.Status != 200:
+            report_failure(case, f"{msg}: {winhttp_req.Status} {winhttp_req.StatusText}")
+            return False
+
+        text = winhttp_req.ResponseText
+        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
+
+        if not expected_regex.match(text):
+            report_failure(case, f"{msg}: response ({RED % text_startup}) does not match expected regex ({GRAY % expected_regex.pattern})")
+            return False
+        
+        logger.info(f"{msg}: {GREEN % 'success'} (response: {GRAY % text_startup})")
+        return True
+    except pywintypes.com_error as e:
+        if e.excepinfo[5] + 2**32 & 0xffff == win32inetcon.ERROR_INTERNET_TIMEOUT:
+            details = "timed out"
+        else:
+            details = e.excepinfo[2].strip()
+        report_failure(case, f"{msg}: {RED % details}")
+        return False
```

## Comparing `zut-0.6.4.dist-info/LICENSE.txt` & `zut-0.6.5.dist-info/LICENSE.txt`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-Copyright (C) 2022 Ipamo
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+Copyright (C) 2022-2023 Ipamo
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

## Comparing `zut-0.6.4.dist-info/METADATA` & `zut-0.6.5.dist-info/METADATA`

 * *Files 22% similar despite different names*

```diff
@@ -1,149 +1,145 @@
-Metadata-Version: 2.1
-Name: zut
-Version: 0.6.4
-Summary: Reusable Python, Django and PostgreSql utilities.
-Author-email: Ipamo <dev@ipamo.net>
-Project-URL: Homepage, https://github.com/ipamo/zut
-Project-URL: Bug Tracker, https://github.com/ipamo/zut/issues
-Keywords: reusable,util,utils,common,commons,flexout,csv,excel,tabulate,smb,samba,share
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.7.3
-Description-Content-Type: text/markdown
-License-File: LICENSE.txt
-Provides-Extra: all
-Requires-Dist: cmarkgfm ; extra == 'all'
-Requires-Dist: django ; extra == 'all'
-Requires-Dist: djangorestframework ; extra == 'all'
-Requires-Dist: openpyxl ; extra == 'all'
-Requires-Dist: defusedxml ; extra == 'all'
-Requires-Dist: pyodbc ; extra == 'all'
-Requires-Dist: sqlparams ; extra == 'all'
-Requires-Dist: sqlparse ; extra == 'all'
-Requires-Dist: psycopg2-binary ; extra == 'all'
-Requires-Dist: smbprotocol ; extra == 'all'
-Requires-Dist: tabulate ; extra == 'all'
-Requires-Dist: typing-extensions ; (python_version < "3.11") and extra == 'all'
-Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'all'
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'all'
-Provides-Extra: credentials
-Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'credentials'
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'credentials'
-Provides-Extra: django
-Requires-Dist: cmarkgfm ; extra == 'django'
-Requires-Dist: django ; extra == 'django'
-Requires-Dist: djangorestframework ; extra == 'django'
-Provides-Extra: excel
-Requires-Dist: openpyxl ; extra == 'excel'
-Requires-Dist: defusedxml ; extra == 'excel'
-Provides-Extra: mssql
-Requires-Dist: pyodbc ; extra == 'mssql'
-Requires-Dist: sqlparams ; extra == 'mssql'
-Requires-Dist: sqlparse ; extra == 'mssql'
-Provides-Extra: pg
-Requires-Dist: psycopg2-binary ; extra == 'pg'
-Requires-Dist: sqlparse ; extra == 'pg'
-Requires-Dist: sqlparams ; extra == 'pg'
-Provides-Extra: smb
-Requires-Dist: smbprotocol ; extra == 'smb'
-Provides-Extra: tabulate
-Requires-Dist: tabulate ; extra == 'tabulate'
-Provides-Extra: typing
-Requires-Dist: typing-extensions ; (python_version < "3.11") and extra == 'typing'
-Provides-Extra: winhttp
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'winhttp'
-
-Zut
-===
-
-Reusable Python, Django and PostgreSql utilities.
-
-## Installation
-
-From PyPI, with all optional dependencies:
-
-    pip install zut[all]
-
-From a Git branch or tag (using https or ssh):
-
-    pip install git+https://gitlab.com/ipamo/zut.git@main#egg=zut[all]
-    pip install git+ssh://git@gitlab.com/ipamo/zut.git@main#egg=zut[all]
-
-Note: optionaly dependencies may also be chosen individually. For example, use `zut[excel,pg]` instead of `zut[all]`.
-
-
-## Usage examples
-
-### Configure logging
-
-```py
-from zut import configure_logging
-configure_logging()
-```
-
-### Flexible in/out
-
-Write text or tabular data to a flexible, easily configurable output: CSV or Excel file, or tabulated stdout/stderr.
-
-The output file may be on the local file system or on a Windows/Samba share (including when the library is used on Linux).
-
-Export text to stdout or to a file:
-
-```py
-import sys
-from zut import out_file
-
-with out_file(filename or sys.stdout) as f:
-    f.write("Content")
-```
-    
-Export tabular data to stdout or to a file:
-
-```py
-import sys
-from zut import out_table
-
-with out_table(filename or sys.stdout, headers=["Id", "Word"]) as t:
-    t.append([1, "Hello"])
-    t.append([2, "World"])
-```
-
-Tabular data can also be exported using dictionnaries (in this case, headers will be detected automatically by the library):
-
-```py
-import sys
-from zut import out_table
-
-with out_table(filename or sys.stdout) as t:
-    t.append({'id': 1, 'name': "Hello"})
-    t.append({'id': 2, 'col3': True})
-```
-
-If `filename` has extension with `.xlsx`, output will be in Excel 2010 format.
-Otherwise it will be in CSV format.
-
-If `filename` starts with `\\`, output will be done on the corresponding Windows/Samba share.
-To indicate Samba credentials, call `configure_smb_credentials` before using function `out_table`.
-Example:
-
-```py
-from zut import out_table, configure_smb_credentials
-
-configure_smb_credentials(user=..., password=...)
-
-with out_table(r"\\server\share\path\to\file") as o:
-    ...
-```
-
-
-## Resources
-
-- [Maintenance of the library and repository](doc/maintain.md)
+Metadata-Version: 2.1
+Name: zut
+Version: 0.6.5
+Summary: Reusable Python, Django and PostgreSql utilities.
+Author-email: Ipamo <dev@ipamo.net>
+Project-URL: Homepage, https://github.com/ipamo/zut
+Project-URL: Bug Tracker, https://github.com/ipamo/zut/issues
+Keywords: reusable,util,utils,common,commons,flexout,csv,excel,tabulate,smb,samba,share
+Classifier: Development Status :: 4 - Beta
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Topic :: Software Development :: Libraries
+Requires-Python: >=3.7.3
+Description-Content-Type: text/markdown
+License-File: LICENSE.txt
+Provides-Extra: all
+Requires-Dist: django ; extra == 'all'
+Requires-Dist: openpyxl ; extra == 'all'
+Requires-Dist: defusedxml ; extra == 'all'
+Requires-Dist: pyodbc ; extra == 'all'
+Requires-Dist: sqlparams ; extra == 'all'
+Requires-Dist: sqlparse ; extra == 'all'
+Requires-Dist: psycopg2-binary ; extra == 'all'
+Requires-Dist: smbprotocol ; extra == 'all'
+Requires-Dist: tabulate ; extra == 'all'
+Requires-Dist: typing-extensions ; (python_version < "3.11") and extra == 'all'
+Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'all'
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'all'
+Provides-Extra: credentials
+Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'credentials'
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'credentials'
+Provides-Extra: django
+Requires-Dist: django ; extra == 'django'
+Provides-Extra: excel
+Requires-Dist: openpyxl ; extra == 'excel'
+Requires-Dist: defusedxml ; extra == 'excel'
+Provides-Extra: mssql
+Requires-Dist: pyodbc ; extra == 'mssql'
+Requires-Dist: sqlparams ; extra == 'mssql'
+Requires-Dist: sqlparse ; extra == 'mssql'
+Provides-Extra: pg
+Requires-Dist: psycopg2-binary ; extra == 'pg'
+Requires-Dist: sqlparse ; extra == 'pg'
+Requires-Dist: sqlparams ; extra == 'pg'
+Provides-Extra: smb
+Requires-Dist: smbprotocol ; extra == 'smb'
+Provides-Extra: tabulate
+Requires-Dist: tabulate ; extra == 'tabulate'
+Provides-Extra: typing
+Requires-Dist: typing-extensions ; (python_version < "3.11") and extra == 'typing'
+Provides-Extra: winhttp
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'winhttp'
+
+Zut
+===
+
+Reusable Python, Django and PostgreSql utilities.
+
+## Installation
+
+From PyPI, with all optional dependencies:
+
+    pip install zut[all]
+
+From a Git branch or tag (using https or ssh):
+
+    pip install git+https://gitlab.com/ipamo/zut.git@main#egg=zut[all]
+    pip install git+ssh://git@gitlab.com/ipamo/zut.git@main#egg=zut[all]
+
+Note: optionaly dependencies may also be chosen individually. For example, use `zut[excel,pg]` instead of `zut[all]`.
+
+
+## Usage examples
+
+### Configure logging
+
+```py
+from zut import configure_logging
+configure_logging()
+```
+
+### Flexible in/out
+
+Write text or tabular data to a flexible, easily configurable output: CSV or Excel file, or tabulated stdout/stderr.
+
+The output file may be on the local file system or on a Windows/Samba share (including when the library is used on Linux).
+
+Export text to stdout or to a file:
+
+```py
+import sys
+from zut import out_file
+
+with out_file(filename or sys.stdout) as f:
+    f.write("Content")
+```
+    
+Export tabular data to stdout or to a file:
+
+```py
+import sys
+from zut import out_table
+
+with out_table(filename or sys.stdout, headers=["Id", "Word"]) as t:
+    t.append([1, "Hello"])
+    t.append([2, "World"])
+```
+
+Tabular data can also be exported using dictionnaries (in this case, headers will be detected automatically by the library):
+
+```py
+import sys
+from zut import out_table
+
+with out_table(filename or sys.stdout) as t:
+    t.append({'id': 1, 'name': "Hello"})
+    t.append({'id': 2, 'col3': True})
+```
+
+If `filename` has extension with `.xlsx`, output will be in Excel 2010 format.
+Otherwise it will be in CSV format.
+
+If `filename` starts with `\\`, output will be done on the corresponding Windows/Samba share.
+To indicate Samba credentials, call `configure_smb_credentials` before using function `out_table`.
+Example:
+
+```py
+from zut import out_table, configure_smb_credentials
+
+configure_smb_credentials(user=..., password=...)
+
+with out_table(r"\\server\share\path\to\file") as o:
+    ...
+```
+
+
+## Resources
+
+- [Maintenance of the library and repository](doc/maintain.md)
```

## Comparing `zut-0.6.4.dist-info/RECORD` & `zut-0.6.5.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,65 @@
-zut/__init__.py,sha256=oaxRzTW6q6yJkXot7O0sb5zGsDXjvyofnPmDdASP2IA,2172
-zut/_version.py,sha256=-1I2Rc1r60PPGi3Q_MAqpzf3LwxAChD5fuG04BNLNZo,164
-zut/apps.py,sha256=jQ5XEeKzi7EBmtPjeZb8sAph8XWTT98g8zqGwVWIK2U,276
-zut/colors.py,sha256=T_NrD_MBlHoL5-muusPNmMlDcP3fGZnsVnd8jICGZIQ,949
-zut/commands.py,sha256=WDCPxtGnKIqv31gBOvVoP7qYJHs9ozjlzyB_QOJ0ynU,6914
-zut/config.py,sha256=jJ2Yd18JtvglQZK-qIdqCWNH83ZHFvWdrfuVC_Eeiy8,1979
-zut/credentials.py,sha256=VeC0Tdium0F0X1UfMieqWog-W4SGC6RC1QNO_kd4-xA,9861
-zut/csv.py,sha256=PqmJdxqkhHdW6pteVjiuZTV1fUpXK7mtynWayGHZKEY,1220
-zut/datetime.py,sha256=jyb_zTv-CgoEOwT3zXmBU8WfET7gOjsGPLAI2YnqEls,1662
-zut/excel.py,sha256=dRiDYfTAb_39H_FEnAh85T3nBhX6wMHiWod0wWyA0RE,19433
+zut/__init__.py,sha256=uM1R_P9memusOBo5OD1dU5BeTcDpR9uRtXMrbTtWyzE,2064
+zut/_version.py,sha256=HOOOlWSq_oq1xqjxIkNY5w_ixt3hUn8UuBP_F6k04ac,160
+zut/apps.py,sha256=5NSs2IEsOPg2lB7EtpkCq3zj7QN78b9FS59NPvkGX_U,281
+zut/colors.py,sha256=EdBHFTMoYO7b-oGJo9ViJTYJfWt3iI1EkLS4SALKNO8,915
+zut/commands.py,sha256=ylkDm3sxzBxnsqvyOTQ7rmzlnR6kNghHL147GAO2zRk,6876
+zut/config.py,sha256=zVP3YDCe0j_tbniPFwk1qvxnRF6ljRguTxB7q8z2qCU,1915
+zut/credentials.py,sha256=xF19LBdmVJj41uldo3p4fv3dMsdbR5ZxaunHZwh_3O8,9602
+zut/csv.py,sha256=-VXyRZ8KW_kCzsO-0RQzjBp_SqE-qetHQnVaCwx4eQU,1176
+zut/datetime.py,sha256=kABa-12v6Ho9p7EcvzWEjLqn1ZSGCUQeiTGBWahBX-8,1605
+zut/excel.py,sha256=XBTNKWQB4ctIh6SSf9Ty_wB8kKFQJ_JKCSkkxmhY6BU,18969
 zut/filesh.py,sha256=VlDtbcQrPHlAg0I5IOlgqE75R4tptcFixyH3DOVWCfQ,12996
-zut/git.py,sha256=A0oJiGkMOTleHyD6guaz2q507iGW6FBdpUyoAmPUXyU,2590
-zut/gpg.py,sha256=ax0CO0I9hq5yAtzOfxvwTp8jJR0PzzQ-oOQRbNtjzfk,1537
-zut/json.py,sha256=9UrErYUQyEYz11fQUtVovYv26XKPlcdDuKynupNzD38,4943
-zut/lang.py,sha256=MfCa-2W-4zoF9UQpx9b5BQTWfMsM0d0glq_auaPupbY,429
-zut/logging.py,sha256=5reFTldB4OXVLr6dbCY9z_yGv9vW6qbBp-4HOrV9HCo,4578
-zut/numeric.py,sha256=3aQ0mEaOR-QNr32N5oKfmUGiQvgLjN1xCnFoOp8XGKE,3252
-zut/process.py,sha256=P1XtNqm13hwEvFYr9lQjPzirpRXe2ZBCQU6fcU5DVGE,2710
-zut/tabular.py,sha256=XQUEFiYW6uUbqLxWrzhLs_4iPuUuQI7rIPDZMPhhZEw,2054
-zut/text.py,sha256=ePkvz5CkB2J9enwZwV1H-EMfgVsmLjRsp-0m-sw9aZw,2019
-zut/typing.py,sha256=yFyfrxO3JZcvZwNAkNDmy-Lz7F6IPH5W61KXBkc6A_s,309
-zut/venv.py,sha256=0D8WhnKZeIb-kIlXO03bW9zibUaumMrH8sBVw2rscqI,632
-zut/db/__init__.py,sha256=kIIvke3nvHIRCmwrkwu72mLOHLnBMBpFMBQ5Hh1zhNA,1665
-zut/db/commons.py,sha256=unXTHZoMe3AhQK97MZ_HYhu1QpMmhZ3PmwrI-OsIei0,14970
-zut/db/mssql.py,sha256=wTYz_w13PxOkptesuoqlC8bI2MGZwm1RmyEuD764NQ4,2650
-zut/db/pg.py,sha256=9sXyh8qvRxnCP43MvQ1kYJ3VMSJLK_1PRbh9Fh9U-1Y,8672
-zut/db/sql_pg/010_extensions.sql,sha256=_uPdGwRw1eMwgO57xs-WSftqSbG62dmGJbdCl9mh7Z0,42
-zut/db/sql_pg/020_slugify.sql,sha256=bQy0KSOD3pYQnEHonP2zAENxINWrxIWtPEvdBfnl4Po,479
-zut/django/migration_tools.py,sha256=fzpZsDhGPRJj7lsWKPzuMiwqGrMGWpFiY7UHsS0Li3w,1157
+zut/git.py,sha256=U5CGH_ridbSh607ijx4DXwGA6llIbo5LE2Vunktr9gI,2508
+zut/gpg.py,sha256=6Ljl6WzncNeEAuvTYV8MfXagUpuGGwchPgsNUmtDkyw,1497
+zut/json.py,sha256=ayq4byVpHtxaWMbGwYxtnMiH9wF8pJwoIUZc9WSmb5o,4770
+zut/lang.py,sha256=5UN0DXNDRdTapxe27HgT9Pggq2cEPJc0Cpsu4Ilskq8,412
+zut/logging.py,sha256=DBinUB1mNteoLgx1h61u_s7gdti9Gt1oVgzTZdKSlZE,4448
+zut/numeric.py,sha256=YyKIg2XPO0cQJM-TvuPVMZReNkgBLyU_m99sHhtUVEs,3157
+zut/process.py,sha256=y95bIeYSJaob1pE3yVkICPuLQn7eBtz9EHqR3ajsdf0,2626
+zut/tabular.py,sha256=4BhVXNuGwlBh8wpRFffnEPrdsTx0fBc8lebMSHjtx1g,1987
+zut/text.py,sha256=MaXP-0cdanQx8vXqbw8dvWxb1K9kcm2vyEyIDB4AQkA,1949
+zut/typing.py,sha256=6CSo9fVCkB5mVezNy13e2AiGRxobgDYc6w_RWywKp60,296
+zut/unittest.py,sha256=9wLfEK5KVzRy7EDIOEikW3-nFFPll6LTVYgk5Yz6wl0,803
+zut/venv.py,sha256=BAIvpYSF9A5OByYyRThjU9mIVBFHvcQTahni-TGDBsk,610
+zut/db/__init__.py,sha256=PhRTylrgUgD5EZab7gnoH-4RUgCYAcHwMSI-yCDjeWE,1637
+zut/db/commons.py,sha256=qoV9QsV88EoQ9fvmq-oRT25OH_To0iipW-uGTbRFYng,15330
+zut/db/mssql.py,sha256=16uwcNcizPxB_hoT0EwqrZGETRBI1Lo9xCYjuL2Gr1A,2576
+zut/db/pg.py,sha256=8jZuyysY-t28SWHhsQF0WbdXSdmmy2ux8WqJnn9Ap6Q,8415
+zut/db/sql_pg/010_extensions.sql,sha256=O_cAxgoDfBa3HbwklHapV8kpwvCAvEUDEBDZ3LCPALM,41
+zut/db/sql_pg/020_slugify.sql,sha256=U3n-sPePz8htN4Zdqfc4O-SEsyAlAVbfiZw9to8wBHc,464
+zut/django/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+zut/django/mixins.py,sha256=XqJVbcG6hckzwwgFmrEyREKOVSj1_11VBqVLDi17zXM,886
+zut/django/models.py,sha256=gNUp-uXnbGytc-E5MmPJJ0Sw_z-_7ReBHVRWlFFwQp4,84
 zut/django/management/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 zut/django/management/commands/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-zut/django/management/commands/reinit.py,sha256=KRpGoQrcNXGQh5dI7WeFADePWa2pi1EocZcZXXCK6A4,8265
-zut/django/middleware/__init__.py,sha256=feBoOfNooaUB3e4PgsGr3w_2-2fWZBBhD__-YmZ6rDo,3271
-zut/django/templatetags/__init__.py,sha256=ilO4n2YF-8Nmcg4gB1sH9hB5Sa_lDzFu-gjoDuBMJAk,71
-zut/django/templatetags/static_lib.py,sha256=p3jbZGCNlqOWpDZFnGXonKlOOHZKQuNwR6ziF0wY0io,2808
-zut/django/tracking/__init__.py,sha256=KtHO-ri7ThbrYeu3Wvjpo73Izk7Jp7Dqouu9lwooDIc,9036
-zut/django/tracking/models.py,sha256=gB1wQG-JyYq3cXPKBGXMJB2AG2AlAcBzT7eXNyKsKBA,4666
-zut/django/tracking/sql_pg/tracking_history_view.sql,sha256=o79jhgTnQ8nvx_nlILKQDnFjccC1M3WgAEYUAjMd5Gw,621
-zut/django/views/MarkdownTemplateView.py,sha256=fNNDD5Z71fZucbnUKhcyCCBmDFFLzY6DVEgdn3pcYkA,1652
-zut/django/views/__init__.py,sha256=1ueMo8cvT4-sr9csWSxwWvlGot7__HWqAwT38gIVVZU,56
-zut/inout/InCsv.py,sha256=oOI-H-WutdfGk2pZhOG7NMKO_f65WkgHT2o1oaIKCVM,1767
-zut/inout/InDb.py,sha256=xLffAgM1snpU9hMzgJFiWsn9ufEH0kL5I_aHMqB_9eA,2124
-zut/inout/InExcel.py,sha256=3MHjziiiv2DKfvwS_UZgBm5AJV5CxyyjpnHUXor8C5I,1403
-zut/inout/InTable.py,sha256=IZXiHiWau4uWehlCXvgFq-i9keOcyJTGeQg_fiIF_OY,4082
-zut/inout/OutCsv.py,sha256=_uzvUwDCmX9zwz4wRIsTHdDs7ADJpcxrrrZHEQsu2sE,4626
-zut/inout/OutDb.py,sha256=ujN8Pzx_nDjUp7fW80EDQrJLt3UEC5y7I6NRLC3SG0g,2876
-zut/inout/OutExcel.py,sha256=-OgeKWhDu2weRwGxEVED0RDa6-zt3-Znow71_mHHqeQ,3340
-zut/inout/OutFile.py,sha256=QOX5Yk_dXEIpHj_QsX2epykd-c9jEab8cQhJIfu15hs,3151
-zut/inout/OutTable.py,sha256=mLejmHHFlSTRfaf3gKmq7NlNcSBvRC5rBES6Km3OCmo,10204
-zut/inout/OutTabulate.py,sha256=u3zd8QrbdofEdnvN1r4jI4QVwIqBd34WRFtwfyUcl4Q,1163
-zut/inout/__init__.py,sha256=kr7cs3k3EmWGxtwRMOWoJ7Yj2ATVS1QRiEJsXoeCLa8,7261
-zut/inout/utils.py,sha256=SkgU1TyoWGVF7VTqifsoO9hhZt79gvvyD7hwFg4N0rg,2786
-zut/network/__init__.py,sha256=DaidKvr4BcJ3xL7fT1jUixRsXM1E8WQKgdbk0xCGvH8,9147
-zut/network/commons.py,sha256=ZeVqQ5I1neDMx3-GEUJFgCTOIlBAVSXmgPvhc7ETh4A,3850
-zut/network/winhttp.py,sha256=sJQQc0a7-BLI5NEI82F4x4qEYBNtx31kAgNZkOGvoyM,2962
-zut-0.6.4.dist-info/LICENSE.txt,sha256=g3B0wycmR6XLn-gWLyJ8IwvBHSAs3pzqmiwME45tzJk,1066
-zut-0.6.4.dist-info/METADATA,sha256=HlaR3YUZEuSuaSaZxeu0Nnfnl_PeAwBjTCtG7iTYkUQ,4962
-zut-0.6.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-zut-0.6.4.dist-info/top_level.txt,sha256=0b5rfbMHLPzpcwBifEaAzzfyv1gZPGT7RXa2myDnXXs,4
-zut-0.6.4.dist-info/RECORD,,
+zut/django/management/commands/reinit.py,sha256=cr8ph7mvZ0Ei2UI3oL8pqjItIVk1_KOWjfSIfYa-YGU,4750
+zut/django/management/commands/remakemigrations.py,sha256=tzCXH_gm9mEX9k0GBfNs3ERLlJ9vBbkYcM-pt5TIG-k,4964
+zut/django/middleware/__init__.py,sha256=Sz22dtfvUpuKuG3qlY_wIJ7Mc9qJAJN2kUbkED2HZ7Y,3959
+zut/django/migrations/0001_initial.py,sha256=ghTSXRsFls4Vk-O6zsUZe6-0fveNbj6TxN7DEX44CHc,3345
+zut/django/migrations/0002_manual.py,sha256=DiCPXhlNDg_OVZ13ZNLvf-Fe44DGwTltey3zPOIRH00,1515
+zut/django/migrations/__init__.py,sha256=_VLBlNQ84NKsJ6MRWg-5_ly7YHPrwTscbgwjpcIoz90,763
+zut/django/migrations/0002_manual/sql_pg/tracking_history_view.sql,sha256=74ytYIa6iO7vJevi47xxsVlMBle_qPnDdtAAfAQOc0s,611
+zut/django/templatetags/__init__.py,sha256=UJDTUjhOgjwbo5q3oEkVhUXVPdXFbYz-uLCqmAZjsYU,70
+zut/django/templatetags/static_lib.py,sha256=dDwQsLIJNTxcxXwpyXeTr8iu6D1c1kuUrYQv54Mu-A4,2718
+zut/django/tracking/__init__.py,sha256=aPWMmrjIwCKgD9BEMjVYy27hmEgYEW8RJVaXIT9dltk,8165
+zut/django/tracking/models.py,sha256=eoiuPpeLDiNN04sQSZ_FcIgsmA0DF1_k1BJ8wpYPOTo,4421
+zut/inout/InCsv.py,sha256=P-mghYKM4_eQ1JDk3MYsu5hK2Ujdw6xMLLwbuoaOQv0,1710
+zut/inout/InDb.py,sha256=OP6kpOIc1t5w7w_XU86x91JFmSZalx0WiWZCNsxhgWU,2062
+zut/inout/InExcel.py,sha256=8s15ypgIVmDupqjz0F5DLB7mSRf2BNQflJdA9FMtDBU,1357
+zut/inout/InTable.py,sha256=VyDMg0QyIx4Hw4DbrF2bkD4Dbc5a_2fAFMhMe5lxRzQ,3951
+zut/inout/OutCsv.py,sha256=DmNQHM6qpANK2aeu6VCB59r8WvgkwBM28URKonBv7pE,4507
+zut/inout/OutDb.py,sha256=Yu4ZzplERZiRZD3B-Dhiny-Y3nDhAT8kYroo0YJqPMM,2802
+zut/inout/OutExcel.py,sha256=W_nWD6sdMecK_fEHzGpwXP34USnAgEbIQnXr5MegxTE,3238
+zut/inout/OutFile.py,sha256=fUaAE_d7Epoiyvye_BFVRgA6h5s3tf8uRnqupzyF2uM,3047
+zut/inout/OutTable.py,sha256=14Nvna-BQ8EG8XvzbLvWpoUe0J7-xYB1lQlFVe40WTc,9916
+zut/inout/OutTabulate.py,sha256=zTQ5WQWiENLE386x9MNGjqBM8dNr0LBE4ytXiYA6KQg,1119
+zut/inout/__init__.py,sha256=4O0Z_Ijcpp3RWcrwIO8CfVfvWMnLKVwnp-fafogq0kU,7113
+zut/inout/utils.py,sha256=YuUalmXqBRnrxuuPSzqXp4Vk7H345rXwLJOFFYJlarw,2698
+zut/network/__init__.py,sha256=Or_-LjSzJEcUWrpw56pyVRgB1Ba9aGgQgL-hqZWIioE,8906
+zut/network/commons.py,sha256=rLDIIOhyT3FnidK1bva53QEXA7WJB8PiHJj9_Eol2FA,3729
+zut/network/winhttp.py,sha256=1ZcK2jFl9uJ5mDoo5ZDOx_27hvIHQ6Ytwda-GakhcJg,2887
+zut-0.6.5.dist-info/LICENSE.txt,sha256=o3Iqne7TjWF-I5p9iynn3Qcnn6F7KRQA7R-cZgZ7PPk,1054
+zut-0.6.5.dist-info/METADATA,sha256=ah5V1Qsmd8FGri5ayqOzsE3s4AalFePm6HStPDq-iUg,4621
+zut-0.6.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+zut-0.6.5.dist-info/top_level.txt,sha256=0b5rfbMHLPzpcwBifEaAzzfyv1gZPGT7RXa2myDnXXs,4
+zut-0.6.5.dist-info/RECORD,,
```

