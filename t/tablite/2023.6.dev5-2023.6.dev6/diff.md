# Comparing `tmp/tablite-2023.6.dev5-py3-none-any.whl.zip` & `tmp/tablite-2023.6.dev6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,34 @@
-Zip file size: 80590 bytes, number of entries: 31
--rw-r--r--  2.0 unx      244 b- defN 23-Jun-13 19:40 tablite/__init__.py
--rw-r--r--  2.0 unx    59024 b- defN 23-Jun-13 19:40 tablite/base.py
--rw-r--r--  2.0 unx     2319 b- defN 23-Jun-13 19:40 tablite/config.py
--rw-r--r--  2.0 unx    28666 b- defN 23-Jun-13 19:40 tablite/core.py
--rw-r--r--  2.0 unx     4504 b- defN 23-Jun-13 19:40 tablite/datasets.py
--rw-r--r--  2.0 unx    28790 b- defN 23-Jun-13 19:40 tablite/datatypes.py
--rw-r--r--  2.0 unx     2872 b- defN 23-Jun-13 19:40 tablite/diff.py
--rw-r--r--  2.0 unx     7759 b- defN 23-Jun-13 19:40 tablite/export_utils.py
--rw-r--r--  2.0 unx    10112 b- defN 23-Jun-13 19:40 tablite/file_reader_utils.py
--rw-r--r--  2.0 unx     4889 b- defN 23-Jun-13 19:40 tablite/groupby_utils.py
--rw-r--r--  2.0 unx     5621 b- defN 23-Jun-13 19:40 tablite/groupbys.py
--rw-r--r--  2.0 unx    22204 b- defN 23-Jun-13 19:40 tablite/import_utils.py
--rw-r--r--  2.0 unx     7589 b- defN 23-Jun-13 19:40 tablite/imputation.py
--rw-r--r--  2.0 unx    12181 b- defN 23-Jun-13 19:40 tablite/joins.py
--rw-r--r--  2.0 unx     6737 b- defN 23-Jun-13 19:40 tablite/lookup.py
--rw-r--r--  2.0 unx     3199 b- defN 23-Jun-13 19:40 tablite/mp_utils.py
--rw-r--r--  2.0 unx     8863 b- defN 23-Jun-13 19:40 tablite/pivots.py
--rw-r--r--  2.0 unx     9171 b- defN 23-Jun-13 19:40 tablite/redux.py
--rw-r--r--  2.0 unx     6884 b- defN 23-Jun-13 19:40 tablite/sort_utils.py
--rw-r--r--  2.0 unx     5459 b- defN 23-Jun-13 19:40 tablite/sortation.py
--rw-r--r--  2.0 unx     1125 b- defN 23-Jun-13 19:40 tablite/tools.py
--rw-r--r--  2.0 unx    11195 b- defN 23-Jun-13 19:40 tablite/utils.py
--rw-r--r--  2.0 unx      139 b- defN 23-Jun-13 19:40 tablite/version.py
--rw-r--r--  2.0 unx     1069 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.data/data/LICENSE
--rw-r--r--  2.0 unx     6960 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.data/data/README.md
--rw-r--r--  2.0 unx      246 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.data/data/requirements.txt
--rw-r--r--  2.0 unx     1069 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.dist-info/LICENSE
--rw-r--r--  2.0 unx     8677 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2457 b- defN 23-Jun-13 19:40 tablite-2023.6.dev5.dist-info/RECORD
-31 files, 270124 bytes uncompressed, 76722 bytes compressed:  71.6%
+Zip file size: 81949 bytes, number of entries: 32
+-rw-r--r--  2.0 unx      244 b- defN 23-Jun-16 11:05 tablite/__init__.py
+-rw-r--r--  2.0 unx    59472 b- defN 23-Jun-16 11:05 tablite/base.py
+-rw-r--r--  2.0 unx     2781 b- defN 23-Jun-16 11:05 tablite/config.py
+-rw-r--r--  2.0 unx    28614 b- defN 23-Jun-16 11:05 tablite/core.py
+-rw-r--r--  2.0 unx     4781 b- defN 23-Jun-16 11:05 tablite/datasets.py
+-rw-r--r--  2.0 unx    28790 b- defN 23-Jun-16 11:05 tablite/datatypes.py
+-rw-r--r--  2.0 unx     2872 b- defN 23-Jun-16 11:05 tablite/diff.py
+-rw-r--r--  2.0 unx     7822 b- defN 23-Jun-16 11:05 tablite/export_utils.py
+-rw-r--r--  2.0 unx    10112 b- defN 23-Jun-16 11:05 tablite/file_reader_utils.py
+-rw-r--r--  2.0 unx     4889 b- defN 23-Jun-16 11:05 tablite/groupby_utils.py
+-rw-r--r--  2.0 unx     5627 b- defN 23-Jun-16 11:05 tablite/groupbys.py
+-rw-r--r--  2.0 unx    22262 b- defN 23-Jun-16 11:05 tablite/import_utils.py
+-rw-r--r--  2.0 unx     7666 b- defN 23-Jun-16 11:05 tablite/imputation.py
+-rw-r--r--  2.0 unx    12059 b- defN 23-Jun-16 11:05 tablite/joins.py
+-rw-r--r--  2.0 unx     6276 b- defN 23-Jun-16 11:05 tablite/lookup.py
+-rw-r--r--  2.0 unx     3199 b- defN 23-Jun-16 11:05 tablite/mp_utils.py
+-rw-r--r--  2.0 unx     8892 b- defN 23-Jun-16 11:05 tablite/pivots.py
+-rw-r--r--  2.0 unx    10170 b- defN 23-Jun-16 11:05 tablite/redux.py
+-rw-r--r--  2.0 unx     1521 b- defN 23-Jun-16 11:05 tablite/reindex.py
+-rw-r--r--  2.0 unx     6884 b- defN 23-Jun-16 11:05 tablite/sort_utils.py
+-rw-r--r--  2.0 unx     5411 b- defN 23-Jun-16 11:05 tablite/sortation.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jun-16 11:05 tablite/tools.py
+-rw-r--r--  2.0 unx    11195 b- defN 23-Jun-16 11:05 tablite/utils.py
+-rw-r--r--  2.0 unx      139 b- defN 23-Jun-16 11:05 tablite/version.py
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.data/data/LICENSE
+-rw-r--r--  2.0 unx     6960 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.data/data/README.md
+-rw-r--r--  2.0 unx      246 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.data/data/requirements.txt
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8677 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2533 b- defN 23-Jun-16 11:05 tablite-2023.6.dev6.dist-info/RECORD
+32 files, 273457 bytes uncompressed, 77969 bytes compressed:  71.5%
```

## zipnote {}

```diff
@@ -48,14 +48,17 @@
 
 Filename: tablite/pivots.py
 Comment: 
 
 Filename: tablite/redux.py
 Comment: 
 
+Filename: tablite/reindex.py
+Comment: 
+
 Filename: tablite/sort_utils.py
 Comment: 
 
 Filename: tablite/sortation.py
 Comment: 
 
 Filename: tablite/tools.py
@@ -63,32 +66,32 @@
 
 Filename: tablite/utils.py
 Comment: 
 
 Filename: tablite/version.py
 Comment: 
 
-Filename: tablite-2023.6.dev5.data/data/LICENSE
+Filename: tablite-2023.6.dev6.data/data/LICENSE
 Comment: 
 
-Filename: tablite-2023.6.dev5.data/data/README.md
+Filename: tablite-2023.6.dev6.data/data/README.md
 Comment: 
 
-Filename: tablite-2023.6.dev5.data/data/requirements.txt
+Filename: tablite-2023.6.dev6.data/data/requirements.txt
 Comment: 
 
-Filename: tablite-2023.6.dev5.dist-info/LICENSE
+Filename: tablite-2023.6.dev6.dist-info/LICENSE
 Comment: 
 
-Filename: tablite-2023.6.dev5.dist-info/METADATA
+Filename: tablite-2023.6.dev6.dist-info/METADATA
 Comment: 
 
-Filename: tablite-2023.6.dev5.dist-info/WHEEL
+Filename: tablite-2023.6.dev6.dist-info/WHEEL
 Comment: 
 
-Filename: tablite-2023.6.dev5.dist-info/top_level.txt
+Filename: tablite-2023.6.dev6.dist-info/top_level.txt
 Comment: 
 
-Filename: tablite-2023.6.dev5.dist-info/RECORD
+Filename: tablite-2023.6.dev6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tablite/base.py

```diff
@@ -276,14 +276,25 @@
                     data = page.get()
                     pages[ix] = np.flip(data)
                 else:
                     pages[ix] = np.flip(page)
 
         return pages
 
+    def iter_by_page(self):
+        """iterates over the column, page by page
+
+        Yields:
+            tuple: start, end, data
+        """
+        start, end = 0, 0
+        for page in self.pages:
+            start, end = end, end + page.len
+            yield start, end, page.get()
+
     def __getitem__(self, item):  # USER FUNCTION.
         """gets numpy array.
 
         Args:
             item (int OR slice): slice of column
 
         Returns:
@@ -554,14 +565,40 @@
         new = np_type_unify(changed_pages)
         # create mask for np.delete.
         filter = [i - starts_on for i in seq]
         pruned = np.delete(new, filter)
         new_arrays = self._paginate(pruned)
         self.pages = head + [Page(self.path, arr) for arr in new_arrays] + tail
 
+    def get_by_indices(self, indices):
+        """retrieves values from column given a set of indices.
+
+        A useful alternative to iterating over rows.
+
+        >>> indices = np.array(list(range(3,700_700, 426)))
+        >>> arr = np.array(list(range(2_000_000)))
+        Slow:
+        >>> [v for i,v in enumerate(arr) if i in indices]
+
+        Fast:
+        >>> np.take(arr, indices)
+
+        Args:
+            indices (np.array): targets
+        """
+        type_check(indices, np.ndarray)
+        arrays = []
+        for start, end, data in self.iter_by_page():
+            range_match = np.where(((indices >= start) & (indices < end)) | (indices == -1))[0]
+            if len(range_match):
+                sub_index = np.take(indices, range_match)
+                arr = np.take(data, sub_index - start)
+                arrays.append(arr)
+        return np_type_unify(arrays)
+
     def __iter__(self):  # USER FUNCTION.
         for page in self.pages:
             data = page.get()
             for value in data:
                 yield value
 
     def __eq__(self, other):  # USER FUNCTION.
@@ -767,15 +804,15 @@
         dtypes = set()
         for page in set(self.pages):
             dtypes.add(page.dtype if page.dtype is object else page.dtype.__name__)
             if len(dtypes) > 1:
                 return "mixed"
         dtype = dtypes.pop()
         return "mixed" if dtype is object else dtype
-        
+
     def index(self):
         """
         returns dict with { unique entry : list of indices }
 
         example:
         >>> c = Column(data=['a','b','a','c','b'])
         >>> c.index()
@@ -1118,81 +1155,68 @@
 
     def clear(self):  # USER FUNCTION.
         """
         clears the table. Like dict().clear()
         """
         self.columns.clear()
 
-    def save(self, path, compression_method=zipfile.ZIP_STORED, compression_level=None):  # USER FUNCTION.
+    def save(self, path, compression_method=zipfile.ZIP_DEFLATED, compression_level=1):  # USER FUNCTION.
         """saves table to compressed tpz file.
 
         Args:
-            path (Path): workdir / PID / tables / <int>.yml
-            compression_method: See zipfile compression methods. Default no compression.
-            compression_level: See zipfile compression levels. Defaults to None.
-            The defaults are the fastest mode of operation.
+            path (Path): file destination.
+            compression_method: See zipfile compression methods. Defaults to ZIP_DEFLATED.
+            compression_level: See zipfile compression levels. Defaults to 1.
+            The default settings produce 80% compression at 10% slowdown.
 
         .tpz is a gzip archive with table metadata captured as table.yml
         and the necessary set of pages saved as .npy files.
 
         The zip contains table.yml which provides an overview of the data:
         --------------------------------------
         %YAML 1.2                              yaml version
-        temp = false                           temp identifier.
         columns:                               start of columns section.
             name: “列 1”                       name of column 1.
                 pages: [p1b1, p1b2]            list of pages in column 1.
-                length: [1_000_000, 834_312]   list of page-lengths
-                types: [0,0]                   list of zeroes, so column 1 is a C-level data format.
             name: “列 2”                       name of column 2
                 pages: [p2b1, p2b2]            list of pages in column 2.
-                length: [1_000_000, 834_312]   list of page-lengths
-                types: [p3b1, p3b2]            list of nonzero type codes, so column 2 is not a C
-                                                 -level data format. The type codes are available
-                                                 in datatypes.Datatypes as _type_codes
         ----------------------------------------
 
         """
         type_check(path, Path)
         if path.is_dir():
             raise TypeError(f"filename needed: {path}")
         if path.suffix != ".tpz":
             path += ".tpz"
 
+        # create yaml document
         _page_counter = 0
-        d = {"temp": False}
+        d = {}
         cols = {}
         for name, col in self.columns.items():
             type_check(col, Column)
-            cols[name] = {
-                "pages": [p.path.name for p in col.pages],
-                "length": [p.len for p in col.pages],
-                "types": [0 for _ in col.pages],
-            }
+            cols[name] = {"pages": [p.path.name for p in col.pages]}
             _page_counter += len(col.pages)
         d["columns"] = cols
-
         yml = yaml.safe_dump(d, sort_keys=False, allow_unicode=True, default_flow_style=None)
 
         _file_counter = 0
-        with zipfile.ZipFile(
-            path, "w", compression=compression_method, compresslevel=compression_level
-        ) as f:  # raise if exists.
+        with zipfile.ZipFile(path, "w", compression=compression_method, compresslevel=compression_level) as f:
             log.debug(f"writing .tpz to {path} with\n{yml}")
             f.writestr("table.yml", yml)
             for name, col in self.columns.items():
                 for page in set(col.pages):  # set of pages! remember t *= 1000 repeats t 1000x
                     with open(page.path, "rb", buffering=0) as raw_io:
                         f.writestr(page.path.name, raw_io.read())
                     _file_counter += 1
                     log.debug(f"adding Page {page.path}")
 
             _fields = len(self) * len(self.columns)
             _avg = _fields // _page_counter
-            log.debug(f"Wrote {_fields} on {_page_counter} pages in {_file_counter} files: {_avg} fields/page")
+            log.debug(f"Wrote {_fields:,} on {_page_counter:,} pages in {_file_counter} files: {_avg} fields/page")
 
     @classmethod
     def load(cls, path, tqdm=_tqdm):  # USER FUNCTION.
         """loads a table from .tpz file.
         See also Table.save for details on the file format.
 
         Args:
@@ -1206,15 +1230,15 @@
         with zipfile.ZipFile(path, "r") as f:
             yml = f.read("table.yml")
             metadata = yaml.safe_load(yml)
             t = cls()
 
             page_count = sum([len(c["pages"]) for c in metadata["columns"].values()])
 
-            with tqdm(total=page_count, desc=f"importing '{path.name}' file") as pbar:
+            with tqdm(total=page_count, desc=f"loading '{path.name}' file", disable=Config.TQDM_DISABLE) as pbar:
                 for name, d in metadata["columns"].items():
                     column = Column(t.path)
                     for page in d["pages"]:
                         bytestream = io.BytesIO(f.read(page))
                         data = np.load(bytestream, allow_pickle=True, fix_imports=False)
                         column.extend(data)
                         pbar.update(1)
```

## tablite/config.py

```diff
@@ -55,15 +55,31 @@
     #             pbar.update(dump_size)
     #     else:
     #         with TaskManager(cpu_count - 1) as tm:
     #             errors = tm.execute(tasks, pbar=PatchTqdm())  # I expects a list of None's if everything is ok.
     #             if any(errors):
     #                 raise Exception("\n".join(e for e in errors if e))
 
+    TQDM_DISABLE = False  # set to True to disable tqdm
+
     @classmethod
     def reset(cls):
         """Resets the config class to original values."""
         for k, v in _default_values.items():
             setattr(Config, k, v)
 
+    @classmethod
+    def page_steps(cls, length):
+        """an iterator that yield start and end in page sizes
+
+        Yields:
+            tuple: start:int, end:int
+        """
+        start, end = 0, 0
+        for _ in range(0, length + 1, cls.PAGE_SIZE):
+            start, end = end, min(end + cls.PAGE_SIZE, length)
+            yield start, end
+            if end == length:
+                return
+
 
 _default_values = {k: v for k, v in Config.__dict__.items() if not k.startswith("__") or callable(v)}
```

## tablite/core.py

```diff
@@ -387,15 +387,15 @@
         if no args, all columns are used.
         """
         if not args:
             args = self.columns
         index = [min(v) for v in self.index(*args).values()]
         return self.reindex(index)
 
-    def sort(self, mapping, sort_mode="excel", tqdm=_tqdm, pbar:_tqdm=None):
+    def sort(self, mapping, sort_mode="excel", tqdm=_tqdm, pbar: _tqdm = None):
         """Perform multi-pass sorting with precedence given order of column names.
 
         Args:
             mapping (dict): keys as columns,
                             values as boolean for 'reverse'
             sort_mode: str: "alphanumeric", "unix", or, "excel"
 
@@ -406,24 +406,24 @@
         Table.sort(mappinp={A':False}) means sort by 'A' in ascending order.
         Table.sort(mapping={'A':True, 'B':False}) means sort 'A' in descending order, then (2nd priority)
         sort B in ascending order.
         """
         new = sortation.sort(self, mapping, sort_mode, tqdm=tqdm, pbar=pbar)
         self.columns = new.columns
 
-    def sorted(self, mapping, sort_mode="excel", tqdm=_tqdm, pbar:_tqdm=None):
+    def sorted(self, mapping, sort_mode="excel", tqdm=_tqdm, pbar: _tqdm = None):
         """See sort.
         Sorted returns a new table in contrast to "sort", which is in-place.
 
         Returns:
             Table.
         """
         return sortation.sort(self, mapping, sort_mode, tqdm=tqdm, pbar=pbar)
 
-    def is_sorted(self, mapping, sort_mode='excel'):
+    def is_sorted(self, mapping, sort_mode="excel"):
         """Performs multi-pass sorting check with precedence given order of column names.
         **kwargs: optional: sort criteria. See Table.sort()
         :return bool
         """
         return sortation.is_sorted(self, mapping, sort_mode)
 
     def any(self, **kwargs):
@@ -479,16 +479,15 @@
         >>> t2 = t.drop(None)
         >>> t2['A'][:], t2['B'][:]
         ([2,3], [2,3])
 
         """
         if not args:
             raise ValueError("What to drop? None? np.nan? ")
-        d = {n: lambda x: x not in set(args) for n in self.columns}
-        return self.all(**d)
+        return redux.drop(self, *args)
 
     def replace(self, mapping, columns=None):
         """replaces all mapped keys with values from named columns
 
         Args:
             mapping (dict): keys are targets for replacement,
                             values are replacements.
```

## tablite/datasets.py

```diff
@@ -1,11 +1,13 @@
+import math
 import random
 from datetime import datetime
 from string import ascii_uppercase
 from tablite import Table
+from tablite.config import Config
 
 
 def synthetic_order_data(rows=100_000):
     """Creates a synthetic dataset for testing that looks like this:
     (depending on number of rows)
 
     +=========+=======+=============+===================+=====+===+=====+====+===+=====+=====+===================+==================+
@@ -38,32 +40,43 @@
     rows = int(rows)
 
     L1 = ["None", "0°", "6°", "21°"]
     L2 = ["ABC", "XYZ", ""]
 
     t = Table()
     assert isinstance(t, Table)
-    t["#"] = list(range(1, rows + 1))
-    # 1 - mock orderid
-    t["1"] = [random.randint(18_778_628_504, 2277_772_117_504) for i in range(1, rows + 1)]
-    # 2 - mock delivery date.
-    t["2"] = [datetime.fromordinal(random.randint(738000, 738150)).isoformat() for i in range(1, rows + 1)]
-    # 3 - mock store id.
-    t["3"] = [random.randint(50000, 51000) for _ in range(1, rows + 1)]
-    # 4 - random bit.
-    t["4"] = [random.randint(0, 1) for _ in range(1, rows + 1)]
-    # 5 - mock product id
-    t["5"] = [random.randint(3000, 30000) for _ in range(1, rows + 1)]
-    # 6 - random weird string
-    t["6"] = [f"C{random.randint(1, 5)}-{random.randint(1, 5)}" for _ in range(1, rows + 1)]
-    # 7 - # random category
-    t["7"] = ["".join(random.choice(ascii_uppercase) for _ in range(3)) for _ in range(1, rows + 1)]
-    # 8 -random temperature group.
-    t["8"] = [random.choice(L1) for _ in range(1, rows + 1)]
-    # 9 - random choice of category
-    t["9"] = [random.choice(L2) for _ in range(1, rows + 1)]
-    # 10 - volume?
-    t["10"] = [random.uniform(0.01, 2.5) for _ in range(1, rows + 1)]
-    # 11 - units?
-    t["11"] = [f"{random.uniform(0.1, 25)}" for _ in range(1, rows + 1)]
+    for page_n in range(math.ceil(rows / Config.PAGE_SIZE)):  # n pages
+        start = (page_n * Config.PAGE_SIZE)
+        end = min(start + Config.PAGE_SIZE, rows)
+        ro = range(start, end)
+
+        t2 = Table()
+        t2["#"] = [v+1 for v in ro]
+        # 1 - mock orderid
+        t2["1"] = [random.randint(18_778_628_504, 2277_772_117_504) for i in ro]
+        # 2 - mock delivery date.
+        t2["2"] = [datetime.fromordinal(random.randint(738000, 738150)).isoformat() for i in ro]
+        # 3 - mock store id.
+        t2["3"] = [random.randint(50000, 51000) for _ in ro]
+        # 4 - random bit.
+        t2["4"] = [random.randint(0, 1) for _ in ro]
+        # 5 - mock product id
+        t2["5"] = [random.randint(3000, 30000) for _ in ro]
+        # 6 - random weird string
+        t2["6"] = [f"C{random.randint(1, 5)}-{random.randint(1, 5)}" for _ in ro]
+        # 7 - # random category
+        t2["7"] = ["".join(random.choice(ascii_uppercase) for _ in range(3)) for _ in ro]
+        # 8 -random temperature group.
+        t2["8"] = [random.choice(L1) for _ in ro]
+        # 9 - random choice of category
+        t2["9"] = [random.choice(L2) for _ in ro]
+        # 10 - volume?
+        t2["10"] = [random.uniform(0.01, 2.5) for _ in ro]
+        # 11 - units?
+        t2["11"] = [f"{random.uniform(0.1, 25)}" for _ in ro]
+
+        if len(t) == 0:
+            t = t2
+        else:
+            t += t2
 
     return t
```

## tablite/export_utils.py

```diff
@@ -1,9 +1,10 @@
 from tablite.utils import sub_cls_check, type_check
 from tablite.base import Table
+from tablite.config import Config
 from tablite.datatypes import DataTypes
 from pathlib import Path
 
 
 from tqdm import tqdm as _tqdm
 
 
@@ -176,15 +177,15 @@
             return str(DataTypes.to_json(value))  # this handles datetimes, timedelta, etc.
 
     delimiters = {".csv": ",", ".tsv": "\t", ".txt": "|"}
     delimiter = delimiters.get(path.suffix)
 
     with path.open("w", encoding="utf-8") as fo:
         fo.write(delimiter.join(c for c in table.columns) + "\n")
-        for row in tqdm(table.rows, total=len(table)):
+        for row in tqdm(table.rows, total=len(table), disable=Config.TQDM_DISABLE):
             fo.write(delimiter.join(txt(c) for c in row) + "\n")
 
 
 def sql_writer(table, path):
     type_check(table, Table)
     type_check(path, Path)
     with path.open("w", encoding="utf-8") as fo:
```

## tablite/groupbys.py

```diff
@@ -116,15 +116,15 @@
         else:
             plural = f"the functions {L} are not in GroupBy.functions"
             raise ValueError(plural)
 
     # only keys will produce unique values for each key group.
     if keys and not functions:
         cols = list(zip(*T.index(*keys)))
-        result = Table()
+        result = T.__class__()
 
         pbar = tqdm(total=len(keys), desc="groupby") if pbar is None else pbar
 
         for col_name, col in zip(keys, cols):
             result[col_name] = col
 
             pbar.update(1)
```

## tablite/import_utils.py

```diff
@@ -86,15 +86,15 @@
     type_check(path, Path)
     t = T()
     with h5py.File(path, "r") as h5:
         for col_name in h5.keys():
             dset = h5[col_name]
             arr = np.array(dset[:])
             if arr.dtype == object:
-                arr = np.array(DataTypes.guess([v.decode('utf-8') for v in arr]))
+                arr = np.array(DataTypes.guess([v.decode("utf-8") for v in arr]))
             t[col_name] = arr
     return t
 
 
 def from_json(T, jsn):
     """
     Imports tables exported using .to_json
@@ -111,15 +111,15 @@
 def from_html(T, path, tqdm=_tqdm, pbar=None):
     if not issubclass(T, Table):
         raise TypeError("Expected subclass of Table")
     type_check(path, Path)
 
     if pbar is None:
         total = path.stat().st_size
-        pbar = tqdm(total=total, desc="from_html")
+        pbar = tqdm(total=total, desc="from_html", disable=Config.TQDM_DISABLE)
 
     row_start, row_end = "<tr>", "</tr>"
     value_start, value_end = "<th>", "</th>"
     chunk = ""
     t = None  # will be T()
     start, end = 0, 0
     data = {}
@@ -425,15 +425,15 @@
     if not (isinstance(start, int) and start >= 0):
         raise ValueError("expected start as an integer >= 0")
     if not (isinstance(limit, int) and limit > 0):
         raise ValueError("expected limit as an integer > 0")
 
     # fmt:off
     with tqdm(total=100, desc=f"importing: reading '{pbar_fname}' bytes", unit="%",
-              bar_format="{desc}: {percentage:3.2f}%|{bar}| [{elapsed}<{remaining}]") as pbar:
+              bar_format="{desc}: {percentage:3.2f}%|{bar}| [{elapsed}<{remaining}]", disable=Config.TQDM_DISABLE) as pbar:
         # fmt:on
         with path.open("r", encoding=encoding, errors="ignore") as fi:
             # task: find chunk ...
             # Here is the problem in a nutshell:
             # --------------------------------------------------------
             # text = "this is my \n text".encode('utf-16')
             # >>> text
```

## tablite/imputation.py

```diff
@@ -1,11 +1,12 @@
 import math
 
 from tablite.base import Table, Column
 from tablite.utils import sub_cls_check
+from tablite.config import Config
 from tablite import sort_utils
 
 from tqdm import tqdm as _tqdm
 
 
 def imputation(T, targets, missing=None, method="carry forward", sources=None, tqdm=_tqdm):
     """
@@ -150,15 +151,17 @@
     }  # strip out all that do not have missings.
     ranks = set()
     for k, v in missing_value_index.items():
         ranks.update(set(k))
     item_order = sort_utils.unix_sort(list(ranks))
     new_order = {tuple(item_order[i] for i in k): k for k in missing_value_index.keys()}
 
-    with tqdm(unit="missing values", total=sum(len(v) for v in missing_value_index.values())) as pbar:
+    with tqdm(
+        unit="missing values", total=sum(len(v) for v in missing_value_index.values()), disable=Config.TQDM_DISABLE
+    ) as pbar:
         for _, key in sorted(new_order.items(), reverse=True):  # Fewest None's are at the front of the list.
             for row_id in missing_value_index[key]:
                 err_map = [0.0 for _ in range(len(T))]
                 for n, v in T.to_dict(columns=sources, slice_=slice(row_id, row_id + 1, 1)).items():
                     # ^--- T.to_dict doesn't go to disk as hence saves an IOP.
                     v = v[0]
                     norm_value = norm_index[n][v]
```

## tablite/joins.py

```diff
@@ -1,11 +1,12 @@
 import math
 import numpy as np
-from tablite.base import Table
 from itertools import product
+from tablite.base import Table
+from tablite.reindex import reindex
 from tablite.utils import sub_cls_check, unique_name
 from tablite.mp_utils import share_mem, map_task, select_processing_method
 from mplite import TaskManager, Task
 import psutil
 
 from tqdm import tqdm as _tqdm
 
@@ -64,29 +65,24 @@
 
 
 def _sp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=_tqdm, pbar=None):
     """
     private helper for single processing join
     """
     assert len(LEFT) == len(RIGHT)
-    result = type(T)()
-
     if pbar is None:
         total = len(left_columns) + len(right_columns)
         pbar = tqdm(total=total, desc="join")
 
-    for col_name in left_columns:
-        col_data = T[col_name][:]
-        result[col_name] = [col_data[k] if k != -1 else None for k in LEFT]
-        pbar.update(1)
-    for col_name in right_columns:
-        col_data = other[col_name][:]
-        revised_name = unique_name(col_name, result.columns)
-        result[revised_name] = [col_data[k] if k != -1 else None for k in RIGHT]
-        pbar.update(1)
+    result = reindex(T, LEFT, left_columns, tqdm=tqdm, pbar=pbar)
+    second = reindex(other, RIGHT, right_columns, tqdm=tqdm, pbar=pbar)
+    for name in right_columns:
+        revised_name = unique_name(name, result.columns)
+        result[revised_name] = second[name]
+
     return result
 
 
 def _mp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=_tqdm, pbar=None):
     return _sp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     assert len(LEFT) == len(RIGHT)
```

## tablite/lookup.py

```diff
@@ -1,11 +1,11 @@
-import psutil
 import math
 import numpy as np
 from tablite.base import Table, Column
+from tablite.reindex import reindex as _reindex
 from tablite.utils import sub_cls_check, unique_name
 from tablite.mp_utils import lookup_ops, share_mem, map_task, select_processing_method
 from mplite import Task, TaskManager
 
 
 from tqdm import tqdm as _tqdm
 
@@ -96,33 +96,26 @@
 
     f = select_processing_method(2 * max(len(T), len(other)), _sp_lookup, _mp_lookup)
     return f(T, other, result_index)
 
 
 def _sp_lookup(T, other, index):
     result = T.copy()
-    for col_name in other.columns:
-        col_data = other[col_name][:]
-        revised_name = unique_name(col_name, result.columns)
-        # 1/3 reindex but well knowing that -1 in the index will be wrong.
-        reindexed = np.take(col_data, index)
-        # 2/3 prepare an array of nones
-        nones = np.empty(shape=index.shape, dtype=object)
-        # 3/3 merge reindexed array with None, whenever the original index is -1.
-        result[revised_name] = np.where(index == -1, nones, reindexed)
-        # the result of the three steps above are the same as in python below:
-        # result[revised_name] = [col_data[k] if k != -1 else None for k in index]
+    second = _reindex(other, index)
+    for name in other.columns:
+        revised_name = unique_name(name, result.columns)
+        result[revised_name] = second[name]
     return result
 
 
 def _mp_lookup(T, other, index):
-    return _sp_lookup(T,other,index)
+    return _sp_lookup(T, other, index)
 
     result = T.copy()
-    cpus = 1# max(psutil.cpu_count(logical=False), 1)
+    cpus = 1  # max(psutil.cpu_count(logical=False), 1)
     step_size = math.ceil(len(T) / cpus)
 
     with TaskManager(cpu_count=cpus) as tm:  # keeps the CPU pool alive during the whole join.
         # for table, columns, side in ([T, left_columns, LEFT], [other, right_columns, RIGHT]):
 
         index, index_shm = share_mem(index, np.int64)  # <-- this is index
         # As all indices in `index` are positive, -1 is used as replacement for None.
```

## tablite/pivots.py

```diff
@@ -254,15 +254,15 @@
 
     new = type(T)()
     new.add_columns(*keep + [column_name, value_name])
     news = {name: [] for name in new.columns}
 
     n = len(keep)
 
-    with tqdm(total=len(T), desc="transpose") as pbar:
+    with tqdm(total=len(T), desc="transpose", disable=Config.TQDM_DISABLE) as pbar:
         for ix, row in enumerate(T[keep + columns].rows, start=1):
             keeps = row[:n]
             transposes = row[n:]
 
             for name, value in zip(keep, keeps):
                 news[name].extend([value] * len(transposes))
             for name, value in zip(columns, transposes):
```

## tablite/redux.py

```diff
@@ -1,12 +1,13 @@
 from tablite.base import Table
 import numpy as np
 from tablite.utils import sub_cls_check, type_check, expression_interpreter
 from tablite.mp_utils import filter_ops
 from tablite.datatypes import list_to_np_array
+from tablite.config import Config
 from tqdm import tqdm as _tqdm
 
 
 def _filter_using_expression(T, expression):
     """
     filters based on an expression, such as:
 
@@ -25,15 +26,15 @@
     except Exception as e:
         raise ValueError(f"Expression could not be compiled: {expression}:\n{e}")
 
     req_columns = [i for i in T.columns if i in expression]
     return np.array([bool(_f(*r)) for r in T.__getitem__(*req_columns).rows], dtype=bool)
 
 
-def filter_using_list_of_dicts(T, expressions, filter_type, tqdm=_tqdm):
+def _filter_using_list_of_dicts(T, expressions, filter_type, tqdm=_tqdm):
     """
     enables filtering across columns for multiple criteria.
 
     expressions:
 
         str: Expression that can be compiled and executed row by row.
             exampLe: "all((A==B and C!=4 and 200<D))"
@@ -93,41 +94,47 @@
         c1 = expression.get("column1", None)
         c2 = expression.get("column2", None)
         expr = expression.get("criteria", None)
         assert expr in filter_ops
         v1 = expression.get("value1", None)
         v2 = expression.get("value2", None)
 
-        if c1 is not None:
-            dset_A = T[c1]
-        else:  # v1 is active:
-            dset_A = np.array([v1] * len(T))
-
-        if c2 is not None:
-            dset_B = T[c2]
-        else:  # v2 is active:
-            dset_B = np.array([v2] * len(T))
-        # Evaluate
-        if expr == ">":
-            result = dset_A[:] > dset_B[:]
-        elif expr == ">=":
-            result = dset_A[:] >= dset_B[:]
-        elif expr == "==":
-            result = dset_A[:] == dset_B[:]
-        elif expr == "<":
-            result = dset_A[:] < dset_B[:]
-        elif expr == "<=":
-            result = dset_A[:] <= dset_B[:]
-        elif expr == "!=":
-            result = dset_A[:] != dset_B[:]
-        else:  # it's a python evaluations (slow)
-            f = filter_ops.get(expr)
-            assert callable(f)
-            result = list_to_np_array([f(a, b) for a, b in zip(dset_A, dset_B)])
-        bitmap[bit_index] = result
+        for start, end in Config.page_steps(len(T)):
+            if c1 is not None:
+                dset_A = T[c1][start:end]
+            else:  # v1 is active:
+                dset_A = np.array([v1] * (end - start))
+
+            if c2 is not None:
+                dset_B = T[c2][start:end]
+            else:  # v2 is active:
+                dset_B = np.array([v2] * (end - start))
+
+            if len(dset_A) != len(dset_B):
+                raise ValueError(
+                    f"Assymmetric dataset: {c1} has {len(dset_A)} values, whilst {c2} has {len(dset_B)} values."
+                )
+            # Evaluate
+            if expr == ">":
+                result = dset_A > dset_B
+            elif expr == ">=":
+                result = dset_A >= dset_B
+            elif expr == "==":
+                result = dset_A == dset_B
+            elif expr == "<":
+                result = dset_A < dset_B
+            elif expr == "<=":
+                result = dset_A <= dset_B
+            elif expr == "!=":
+                result = dset_A != dset_B
+            else:  # it's a python evaluations (slow)
+                f = filter_ops.get(expr)
+                assert callable(f)
+                result = list_to_np_array([f(a, b) for a, b in zip(dset_A, dset_B)])
+            bitmap[bit_index, start:end] = result
 
     f = np.all if filter_type == "all" else np.any
     mask = f(bitmap, axis=0)
     # 4. The mask is now created and is no longer needed.
     return mask
 
 
@@ -167,80 +174,96 @@
     sub_cls_check(T, Table)
 
     if not isinstance(kwargs, dict):
         raise TypeError("did you forget to add the ** in front of your dict?")
     if not all([k in T.columns for k in kwargs]):
         raise ValueError(f"Unknown column(s): {[k for k in kwargs if k not in T.columns]}")
 
-    ixs = None
+    mask = np.full((len(T),), True)
     for k, v in kwargs.items():
-        col = T[k][:]
-        if ixs is None:  # first header generates base set.
+        col = T[k]
+        for start, end, data in col.iter_by_page():
             if callable(v):
-                ix2 = {ix for ix, i in enumerate(col) if v(i)}
+                mask[start:end] = mask[start:end] & v(data)
             else:
-                ix2 = {ix for ix, i in enumerate(col) if v == i}
+                mask[start:end] = mask[start:end] & (data == v)
+
+    return _compress_one(T, mask)
 
-        else:  # remaining headers reduce the base set.
-            if callable(v):
-                ix2 = {ix for ix in ixs if v(col[ix])}
-            else:
-                ix2 = {ix for ix in ixs if v == col[ix]}
 
-        if not isinstance(ixs, set):
-            ixs = ix2
-        else:
-            ixs = ixs.intersection(ix2)
-
-        if not ixs:  # There are no matches.
-            break
-
-    mask = np.array([True if i in ixs else False for i in range(len(T))], dtype=bool)
-    ixs.clear()
-    new = type(T)()
+def drop(T, *args):
+    """drops all rows that contain args
+
+    Args:
+        T (Table):
+    """
+    sub_cls_check(T, Table)
+    mask = np.full((len(T),), False)
     for name in T.columns:
-        new[name] = np.compress(mask, T[name][:])
-    return new
+        col = T[name]
+        for start, end, data in col.iter_by_page():
+            for arg in args:
+                mask[start:end] = mask[start:end] | (data == arg)
+
+    mask = np.invert(mask)
+    return _compress_one(T, mask)
 
 
 def filter_any(T, **kwargs):
     """
     returns Table for rows where ANY kwargs match
     :param kwargs: dictionary with headers and values / boolean callable
     """
     sub_cls_check(T, Table)
     if not isinstance(kwargs, dict):
         raise TypeError("did you forget to add the ** in front of your dict?")
 
-    ixs = set()
+    mask = np.full((len(T),), False)
     for k, v in kwargs.items():
-        col = T[k][:]
-        if callable(v):
-            ix2 = {ix for ix, r in enumerate(col) if v(r)}
-        else:
-            ix2 = {ix for ix, r in enumerate(col) if v == r}
-        ixs.update(ix2)
-
-    mask = np.array([True if i in ixs else False for i in range(len(T))], dtype=bool)
-    ixs.clear()
-    new = type(T)()
+        col = T[k]
+        for start, end, data in col.iter_by_page():
+            if callable(v):
+                mask[start:end] = mask[start:end] | v(data)
+            else:
+                mask[start:end] = mask[start:end] | (v == data)
+
+    return _compress_one(T, mask)
+
+
+def _compress_one(T, mask):
+    # NOTE FOR DEVELOPERS:
+    # np.compress is so fast that the overhead of multiprocessing doesn't pay off.
+    cls = type(T)
+    new = cls()
     for name in T.columns:
-        new[name] = np.compress(mask, T[name][:])
+        new.add_columns(name)
+        col = new[name]  # fetch the col to avoid doing it in the loop below
+
+        # prevent OOMError by slicing the getitem ops
+        for start, end in Config.page_steps(len(T)):
+            col.extend(np.compress(mask[start:end], T[name][start:end]))  # <-- getitem ops
     return new
 
 
-def compress(T, mask):
+def _compress_both(T, mask):
     # NOTE FOR DEVELOPERS:
-    # _sp_compress is so fast that the overhead of multiprocessing doesn't pay off.
+    # np.compress is so fast that the overhead of multiprocessing doesn't pay off.
     cls = type(T)
     true, false = cls(), cls()
-    for col_name in T.columns:
-        data = T[col_name][:]
-        true[col_name] = np.compress(mask, data)
-        false[col_name] = np.compress(np.invert(mask), data)
+
+    for name in T.columns:
+        true.add_column(name)
+        false.add_column(name)
+        true_col = true[name]  # fetch the col to avoid doing it in the loop below
+        false_col = false[name]
+        # prevent OOMError by slicing the getitem ops
+        for start, end in Config.page_steps(len(T)):
+            data = T[name][start:end]
+            true_col.extend(np.compress(mask[start:end], data))
+            false_col.extend(np.compress(np.invert(mask)[start:end], data))
     return true, false
 
 
 def filter(T, expressions, filter_type="all", tqdm=_tqdm):
     """filters table
 
 
@@ -276,12 +299,12 @@
     sub_cls_check(T, Table)
     if len(T) == 0:
         return T.copy(), T.copy()
 
     if isinstance(expressions, str):
         mask = _filter_using_expression(T, expressions)
     elif isinstance(expressions, list):
-        mask = filter_using_list_of_dicts(T, expressions, filter_type, tqdm)
+        mask = _filter_using_list_of_dicts(T, expressions, filter_type, tqdm)
     else:
         raise TypeError
     # create new tables
-    return compress(T, mask)
+    return _compress_both(T, mask)
```

## tablite/sortation.py

```diff
@@ -1,13 +1,14 @@
 import os
 import numpy as np
 import psutil
 from mplite import Task, TaskManager
 from tablite.mp_utils import share_mem, reindex_task, select_processing_method
 from tablite.datatypes import multitype_set, numpy_to_python
+from tablite.reindex import reindex as _reindex
 from tablite.sort_utils import modes as sort_modes
 from tablite.sort_utils import rank as sort_rank
 from tablite.base import Table, Column, Page
 from tablite.utils import sub_cls_check, type_check
 
 from tqdm import tqdm as _tqdm
 
@@ -80,24 +81,20 @@
     type_check(index, np.ndarray)
     if max(index) >= len(T):
         raise IndexError("index out of range: max(index) > len(self)")
     if min(index) < -len(T):
         raise IndexError("index out of range: min(index) < -len(self)")
 
     fields = len(T) * len(T.columns)
-    m = select_processing_method(fields, _sp_reindex, _mp_reindex)
+    m = select_processing_method(fields, _reindex, _mp_reindex)
     return m(T, index)
 
 
 def _sp_reindex(T, index):
-    t = type(T)()
-    for name in T.columns:
-        data = T[name][:]
-        t[name] = np.take(data, index)
-    return t
+    return _reindex(T, index)
 
 
 def _mp_reindex(T, index):
     assert isinstance(index, np.ndarray)
     return _sp_reindex(T, index)
 
     index, shm = share_mem(index, dtype=index.dtype)
```

## tablite/version.py

```diff
@@ -1,3 +1,3 @@
-major, minor, patch = 2023, 6, "dev5"
+major, minor, patch = 2023, 6, "dev6"
 __version_info__ = (major, minor, patch)
 __version__ = ".".join(str(i) for i in __version_info__)
```

## Comparing `tablite-2023.6.dev5.data/data/LICENSE` & `tablite-2023.6.dev6.data/data/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev5.data/data/README.md` & `tablite-2023.6.dev6.data/data/README.md`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev5.dist-info/LICENSE` & `tablite-2023.6.dev6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev5.dist-info/METADATA` & `tablite-2023.6.dev6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tablite
-Version: 2023.6.dev5
+Version: 2023.6.dev6
 Summary: multiprocessing enabled out-of-memory data analysis library for tabular data.
 Home-page: https://github.com/root-11/tablite
 Author: https://github.com/root-11
 License: MIT
 Keywords: all,any,average,column,columns,count,csv,data imputation,date range,dict,excel,filter,first,from,from_pandas,groupby,guess,imputation,in-memory,index,indexing,inner join,is sorted,json,last,left join,list,list on disk,log,max,median,min,mode,numpy,ods,out-of-memory,outer join,pandas,pivot,pivot table,product,read csv,remove duplicates,replace,replace missing values,rows,show,sort,standard deviation,stored list,sum,table,tables,tablite,to,to_pandas,tools,transpose,txt,unique,use disk,xlsx,xround,zip
 Platform: any
 Classifier: Development Status :: 5 - Production/Stable
```

## Comparing `tablite-2023.6.dev5.dist-info/RECORD` & `tablite-2023.6.dev6.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 tablite/__init__.py,sha256=SH-lADLJJHz9XwBebXZMYNBsBneAGwSCXY5rJhYgrYg,244
-tablite/base.py,sha256=RTC8kpZR62-2NJQmBWSITkcRxb2ezfTO6mrBTtmyF2U,59024
-tablite/config.py,sha256=IWwf7uue50vgydkj1s62twnJDmcWGDFm31t45YqUZww,2319
-tablite/core.py,sha256=noEOohGL1qbNAKFjbUGxZ-3fStnVSAorKWOOe_IqTQ4,28666
-tablite/datasets.py,sha256=WCCXy6nbLFokBGt3S9nd3IdYfeEqvvmHS90ZZVguV84,4504
+tablite/base.py,sha256=WnZuu-jfTV0tXwXY3cExB0kfu7LkKSqP0ynbtXJuc_Q,59472
+tablite/config.py,sha256=cgd9W89kJyud6nIKCUn-JSeL1z1diM4iVB6c3uTQbgs,2781
+tablite/core.py,sha256=NSX07t4zsTe7kVx8Pq1X72gXdSWBAG3d6pl3Ejhlgoo,28614
+tablite/datasets.py,sha256=pRlucE6XUvRFkuEm9ITtgtCMjdzxcDnHjxXNc1Xc5Vc,4781
 tablite/datatypes.py,sha256=Ek5YQErLJhPDdHAsAtRxCMRMzD8uQ1KfA4JY3vIjhBI,28790
 tablite/diff.py,sha256=NuxQGJLtXEat4utd1gyRDZBucV33nsMZeP9T02Mmofc,2872
-tablite/export_utils.py,sha256=cP_daDl-qXYqfVt2JVeFcNWYxMxTWiaA9IliYGRZ870,7759
+tablite/export_utils.py,sha256=ylIgplaTJzWcSHhcOIQY1xV-C3ZniHZ607NK9ZinBwI,7822
 tablite/file_reader_utils.py,sha256=3a0XGUrmfM84BueFrP-wbTfL8_xpXmHp3RvknsYUf8k,10112
 tablite/groupby_utils.py,sha256=-GhV2q_4gFd7c1QKz0Oz6aiW5YLSMXXlIa_iNsJasBw,4889
-tablite/groupbys.py,sha256=ifu_EWHle0PkYiuPnNy11fRJmU4vQcBvZFfVW9b9_BY,5621
-tablite/import_utils.py,sha256=zGzrialo7V9ivJs7ebpec3eA7MALqpLeYBXSZJkGWjI,22204
-tablite/imputation.py,sha256=cBQ3k6_5LOFWyPC0-cum7IwIGk32PKcHb8kJ-WBK7a0,7589
-tablite/joins.py,sha256=fD19iAX2Czu0mlheEd8oCrg7UryncbDMWX2git-eRO8,12181
-tablite/lookup.py,sha256=rRgwhWSDc1wTBHzoc7iRSWR0Ma0d7lWxfVHRoKE5dGU,6737
+tablite/groupbys.py,sha256=wFnhcdjvzMkCAxFca1oVFk_adWzAnsVysQdu9MwZQHc,5627
+tablite/import_utils.py,sha256=3z2xvXMjsnTVVf4cGdqN2KjsQGYXjoIojiqJwK5hTB0,22262
+tablite/imputation.py,sha256=ay-z1rxMe8OhM5Cnb1fpKWU4345AtdwZ0V6xb7ka01I,7666
+tablite/joins.py,sha256=ON7JLwpPe2RrdX6ms02zJXSq9LUYFk6BSbBF18YbxYs,12059
+tablite/lookup.py,sha256=niSfZIqNgXPqu_nEPXlOu8sIgVyRNjeQcQs0_5MarD0,6276
 tablite/mp_utils.py,sha256=hY1X3PodD4jEMJ04AahrnLbQztJrXEntDm_ftPFeYuw,3199
-tablite/pivots.py,sha256=Yg5-ZAHTNlndESK2k3NwNJ1YkbxYLlX9KS64hys8pcE,8863
-tablite/redux.py,sha256=yh_i7my5aBQRP95Lkt5El8lDLTV0y2aRvtcHoRfXHpQ,9171
+tablite/pivots.py,sha256=qJLVsXpw0Nw8p83NJvxl7-kVSZp_0HUlqWIa2IamQcU,8892
+tablite/redux.py,sha256=rVQcUD_vg5fIH14DYT7rhNELWkI4Wwq6sWILzxm8_rI,10170
+tablite/reindex.py,sha256=lsSfk84XYQhmMxmUNovPWRmxtiaCC6Ue2CiEiju4qek,1521
 tablite/sort_utils.py,sha256=KevYkZlRq3nsSRDElG_chUm-t7dya2KOOzh0p-he0_4,6884
-tablite/sortation.py,sha256=r5q2-2g_zQ7SwE6KJwxuCQ8O6QB2PpGnoGY6DJopOwc,5459
+tablite/sortation.py,sha256=CafBHU3emzG4m2azu_mny0-IHC6o_JkZoMx2ETmaMs4,5411
 tablite/tools.py,sha256=hEpqij7_2yc8yptUetk9Qjq1b7NxGoMamcCWOX3X010,1125
 tablite/utils.py,sha256=GXSuj7UfJzKa9a0t3FW5TnYacnvPD2_g1P-hDFP0ZWQ,11195
-tablite/version.py,sha256=-U5dvHpcKPHkwWUPPb48wmD5kI7L9maXwMiiWw7oZtY,139
-tablite-2023.6.dev5.data/data/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
-tablite-2023.6.dev5.data/data/README.md,sha256=TlGniuVuNAT76KFExrlEPDOiUDnrRb6Qa7aahNmxZ3Y,6960
-tablite-2023.6.dev5.data/data/requirements.txt,sha256=AGNyHRtmn33IBZb4LVH8kqvOsoQCbzZTM1F9-VBUxVI,246
-tablite-2023.6.dev5.dist-info/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
-tablite-2023.6.dev5.dist-info/METADATA,sha256=Fm8csVFl7_fu_V_e6HQzXmxhOA8YDU1dZRwqeYCWCqs,8677
-tablite-2023.6.dev5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-tablite-2023.6.dev5.dist-info/top_level.txt,sha256=wLPhlgUC10JlueE5ZhtCVSrs2VnvMQgNlhdCNsxPXPg,8
-tablite-2023.6.dev5.dist-info/RECORD,,
+tablite/version.py,sha256=BF3NIHhEOHqQIJ4QWNepABgxeI67SO6IyDHez5RFR8o,139
+tablite-2023.6.dev6.data/data/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
+tablite-2023.6.dev6.data/data/README.md,sha256=TlGniuVuNAT76KFExrlEPDOiUDnrRb6Qa7aahNmxZ3Y,6960
+tablite-2023.6.dev6.data/data/requirements.txt,sha256=AGNyHRtmn33IBZb4LVH8kqvOsoQCbzZTM1F9-VBUxVI,246
+tablite-2023.6.dev6.dist-info/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
+tablite-2023.6.dev6.dist-info/METADATA,sha256=JGztMraYLY9zVTUv-8GsDLfqXcrmZd4OHWHsmkp-r5A,8677
+tablite-2023.6.dev6.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+tablite-2023.6.dev6.dist-info/top_level.txt,sha256=wLPhlgUC10JlueE5ZhtCVSrs2VnvMQgNlhdCNsxPXPg,8
+tablite-2023.6.dev6.dist-info/RECORD,,
```

